/*
THIS IS A GENERATED/BUNDLED FILE BY ESBUILD
if you want to view the source, please visit the github repository of this plugin
*/

var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/main.ts
var main_exports = {};
__export(main_exports, {
  DEFAULT_SETTINGS: () => DEFAULT_SETTINGS,
  default: () => LLMPlugin2
});
module.exports = __toCommonJS(main_exports);
var import_obsidian13 = require("obsidian");

// src/Assistants/AssistantHandler.ts
var Assistants = class {
  constructor(plugin) {
    this.plugin = plugin;
  }
  push(assistant2) {
    try {
      let assistants = this.plugin.settings.assistants;
      assistants.push(assistant2);
      this.plugin.settings.assistants = assistants;
      this.plugin.saveSettings();
      return true;
    } catch (exception) {
      return false;
    }
  }
  reset() {
    this.plugin.settings.assistants = [];
    this.plugin.saveSettings();
  }
};

// src/History/HistoryHandler.ts
var History = class {
  constructor(plugin) {
    this.plugin = plugin;
  }
  push(message_context) {
    try {
      let history = this.plugin.settings.promptHistory;
      history.push(message_context);
      if (history.length > 10) {
        history.remove(history[0]);
      }
      this.plugin.settings.promptHistory = history;
      this.plugin.saveSettings();
      return true;
    } catch (exception) {
      return false;
    }
  }
  reset() {
    this.plugin.settings.promptHistory = [];
    this.plugin.saveSettings();
  }
  //take in an index from the selected chat history
  //overwrite history with new prompt/additional prompt
  overwriteHistory(messages2, index) {
    const historyItem = this.plugin.settings.promptHistory[index];
    historyItem.messages = messages2;
    this.plugin.settings.promptHistory[index] = historyItem;
    this.plugin.saveSettings();
  }
};

// src/Plugin/Components/AssistantsContainer.ts
var import_obsidian3 = require("obsidian");

// src/utils/constants.ts
var claude = "claude";
var openAI = "openAI";
var gemini = "gemini";
var GPT4All = "GPT4All";
var claudeSonnetJuneModel = "claude-3-5-sonnet-20240620";
var geminiModel = "gemini-1.5-flash";
var openAIModel = "gpt-3.5-turbo";
var messages = "messages";
var assistant = "assistant";
var chat = "chat";

// src/utils/models.ts
var openAIModels = {
  "ChatGPT-3.5 turbo": {
    model: "gpt-3.5-turbo",
    type: "openAI",
    endpoint: chat,
    url: "/chat/completions"
  },
  "GPT-4o": {
    model: "gpt-4o",
    type: "openAI",
    endpoint: chat,
    url: "/chat/completions"
  }
};
var models = {
  ...openAIModels,
  "ChatGPT-3.5 turbo GPT4All": {
    model: "gpt4all-gpt-3.5-turbo.rmodel",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Mistral OpenOrca": {
    model: "mistral-7b-openorca.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Mistral Instruct": {
    model: "mistral-7b-instruct-v0.1.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "GPT4All Falcon": {
    model: "gpt4all-falcon-newbpe-q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Orca 2 (Medium)": {
    model: "orca-2-7b.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Orca 2 (Full)": {
    model: "orca-2-13b.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Mini Orca (Small)": {
    model: "orca-mini-3b-gguf2-q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "MPT Chat": {
    model: "mpt-7b-chat-newbpe-q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Wizard v1.2": {
    model: "wizardlm-13b-v1.2.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Hermes 13B": {
    model: "nous-hermes-llama2-13b.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "Hermes 7B": {
    model: "Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  Snoozy: {
    model: "gpt4all-13b-snoozy-q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  "EM German Mistral": {
    model: "em_german_mistral_v01.Q4_0.gguf",
    type: GPT4All,
    endpoint: chat,
    url: "/v1/chat/completions"
  },
  // Claude Models
  "Claude-3-5-Sonnet-20240620": {
    model: "claude-3-5-sonnet-20240620",
    type: claude,
    endpoint: messages,
    // We will not consume these so can we remove them?
    url: "/v1/messages"
  },
  // Gemini Models
  "Gemini-1.5-flash": {
    model: geminiModel,
    type: gemini,
    endpoint: "gemini",
    url: "gemini"
  },
  "DALL\xB7E 3": {
    model: "dall-e-3",
    type: "openAI",
    endpoint: "images",
    url: "/images/generations"
  },
  "DALL\xB7E 2": {
    model: "dall-e-2",
    type: "openAI",
    endpoint: "images",
    url: "/images/generations"
  }
};
var modelNames = {
  "mistral-7b-openorca.Q4_0.gguf": "Mistral OpenOrca",
  "mistral-7b-instruct-v0.1.Q4_0.gguf": "Mistral Instruct",
  "gpt4all-falcon-newbpe-q4_0.gguf": "GPT4All Falcon",
  "orca-2-7b.Q4_0.gguf": "Orca 2 (Medium)",
  "orca-2-13b.Q4_0.gguf": "Orca 2 (Full)",
  "orca-mini-3b-gguf2-q4_0.gguf": "Mini Orca (Small)",
  "mpt-7b-chat-newbpe-q4_0.gguf": "MPT Chat",
  "wizardlm-13b-v1.2.Q4_0.gguf": "Wizard v1.2",
  "nous-hermes-llama2-13b.Q4_0.gguf": "Hermes 13B",
  "Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf": "Hermes 7B",
  "gpt4all-gpt-3.5-turbo.rmodel": "ChatGPT-3.5 turbo GPT4All",
  "gpt4all-13b-snoozy-q4_0.gguf": "Snoozy",
  "em_german_mistral_v01.Q4_0.gguf": "EM German Mistral",
  "gpt-3.5-turbo": "ChatGPT-3.5 turbo",
  "gpt-4o": "GPT-4o",
  "claude-3-5-sonnet-20240620": "Claude-3-5-Sonnet-20240620",
  "gemini-1.5-flash": "Gemini-1.5-flash",
  // "text-embedding-3-small": "Text Embedding 3 (Small)",
  "dall-e-3": "DALL\xB7E 3",
  "dall-e-2": "DALL\xB7E 2"
};

// src/utils/utils.ts
var import_obsidian2 = require("obsidian");

// node_modules/openai/error.mjs
var error_exports = {};
__export(error_exports, {
  APIConnectionError: () => APIConnectionError,
  APIConnectionTimeoutError: () => APIConnectionTimeoutError,
  APIError: () => APIError,
  APIUserAbortError: () => APIUserAbortError,
  AuthenticationError: () => AuthenticationError,
  BadRequestError: () => BadRequestError,
  ConflictError: () => ConflictError,
  InternalServerError: () => InternalServerError,
  NotFoundError: () => NotFoundError,
  OpenAIError: () => OpenAIError,
  PermissionDeniedError: () => PermissionDeniedError,
  RateLimitError: () => RateLimitError,
  UnprocessableEntityError: () => UnprocessableEntityError
});

// node_modules/openai/version.mjs
var VERSION = "4.53.0";

// node_modules/openai/_shims/registry.mjs
var auto = false;
var kind = void 0;
var fetch2 = void 0;
var Request2 = void 0;
var Response2 = void 0;
var Headers2 = void 0;
var FormData2 = void 0;
var Blob2 = void 0;
var File2 = void 0;
var ReadableStream2 = void 0;
var getMultipartRequestOptions = void 0;
var getDefaultAgent = void 0;
var fileFromPath = void 0;
var isFsReadStream = void 0;
function setShims(shims, options = { auto: false }) {
  if (auto) {
    throw new Error(`you must \`import 'openai/shims/${shims.kind}'\` before importing anything else from openai`);
  }
  if (kind) {
    throw new Error(`can't \`import 'openai/shims/${shims.kind}'\` after \`import 'openai/shims/${kind}'\``);
  }
  auto = options.auto;
  kind = shims.kind;
  fetch2 = shims.fetch;
  Request2 = shims.Request;
  Response2 = shims.Response;
  Headers2 = shims.Headers;
  FormData2 = shims.FormData;
  Blob2 = shims.Blob;
  File2 = shims.File;
  ReadableStream2 = shims.ReadableStream;
  getMultipartRequestOptions = shims.getMultipartRequestOptions;
  getDefaultAgent = shims.getDefaultAgent;
  fileFromPath = shims.fileFromPath;
  isFsReadStream = shims.isFsReadStream;
}

// node_modules/openai/_shims/MultipartBody.mjs
var MultipartBody = class {
  constructor(body) {
    this.body = body;
  }
  get [Symbol.toStringTag]() {
    return "MultipartBody";
  }
};

// node_modules/openai/_shims/web-runtime.mjs
function getRuntime({ manuallyImported } = {}) {
  const recommendation = manuallyImported ? `You may need to use polyfills` : `Add one of these imports before your first \`import \u2026 from 'openai'\`:
- \`import 'openai/shims/node'\` (if you're running on Node)
- \`import 'openai/shims/web'\` (otherwise)
`;
  let _fetch, _Request, _Response, _Headers;
  try {
    _fetch = fetch;
    _Request = Request;
    _Response = Response;
    _Headers = Headers;
  } catch (error) {
    throw new Error(`this environment is missing the following Web Fetch API type: ${error.message}. ${recommendation}`);
  }
  return {
    kind: "web",
    fetch: _fetch,
    Request: _Request,
    Response: _Response,
    Headers: _Headers,
    FormData: (
      // @ts-ignore
      typeof FormData !== "undefined" ? FormData : class FormData {
        // @ts-ignore
        constructor() {
          throw new Error(`file uploads aren't supported in this environment yet as 'FormData' is undefined. ${recommendation}`);
        }
      }
    ),
    Blob: typeof Blob !== "undefined" ? Blob : class Blob {
      constructor() {
        throw new Error(`file uploads aren't supported in this environment yet as 'Blob' is undefined. ${recommendation}`);
      }
    },
    File: (
      // @ts-ignore
      typeof File !== "undefined" ? File : class File {
        // @ts-ignore
        constructor() {
          throw new Error(`file uploads aren't supported in this environment yet as 'File' is undefined. ${recommendation}`);
        }
      }
    ),
    ReadableStream: (
      // @ts-ignore
      typeof ReadableStream !== "undefined" ? ReadableStream : class ReadableStream {
        // @ts-ignore
        constructor() {
          throw new Error(`streaming isn't supported in this environment yet as 'ReadableStream' is undefined. ${recommendation}`);
        }
      }
    ),
    getMultipartRequestOptions: async (form, opts) => ({
      ...opts,
      body: new MultipartBody(form)
    }),
    getDefaultAgent: (url) => void 0,
    fileFromPath: () => {
      throw new Error("The `fileFromPath` function is only supported in Node. See the README for more details: https://www.github.com/openai/openai-node#file-uploads");
    },
    isFsReadStream: (value) => false
  };
}

// node_modules/openai/_shims/index.mjs
if (!kind)
  setShims(getRuntime(), { auto: true });

// node_modules/openai/streaming.mjs
var Stream = class {
  constructor(iterator, controller) {
    this.iterator = iterator;
    this.controller = controller;
  }
  static fromSSEResponse(response, controller) {
    let consumed = false;
    async function* iterator() {
      if (consumed) {
        throw new Error("Cannot iterate over a consumed stream, use `.tee()` to split the stream.");
      }
      consumed = true;
      let done = false;
      try {
        for await (const sse of _iterSSEMessages(response, controller)) {
          if (done)
            continue;
          if (sse.data.startsWith("[DONE]")) {
            done = true;
            continue;
          }
          if (sse.event === null) {
            let data;
            try {
              data = JSON.parse(sse.data);
            } catch (e) {
              console.error(`Could not parse message into JSON:`, sse.data);
              console.error(`From chunk:`, sse.raw);
              throw e;
            }
            if (data && data.error) {
              throw new APIError(void 0, data.error, void 0, void 0);
            }
            yield data;
          } else {
            let data;
            try {
              data = JSON.parse(sse.data);
            } catch (e) {
              console.error(`Could not parse message into JSON:`, sse.data);
              console.error(`From chunk:`, sse.raw);
              throw e;
            }
            if (sse.event == "error") {
              throw new APIError(void 0, data.error, data.message, void 0);
            }
            yield { event: sse.event, data };
          }
        }
        done = true;
      } catch (e) {
        if (e instanceof Error && e.name === "AbortError")
          return;
        throw e;
      } finally {
        if (!done)
          controller.abort();
      }
    }
    return new Stream(iterator, controller);
  }
  /**
   * Generates a Stream from a newline-separated ReadableStream
   * where each item is a JSON value.
   */
  static fromReadableStream(readableStream, controller) {
    let consumed = false;
    async function* iterLines() {
      const lineDecoder = new LineDecoder();
      const iter = readableStreamAsyncIterable(readableStream);
      for await (const chunk of iter) {
        for (const line of lineDecoder.decode(chunk)) {
          yield line;
        }
      }
      for (const line of lineDecoder.flush()) {
        yield line;
      }
    }
    async function* iterator() {
      if (consumed) {
        throw new Error("Cannot iterate over a consumed stream, use `.tee()` to split the stream.");
      }
      consumed = true;
      let done = false;
      try {
        for await (const line of iterLines()) {
          if (done)
            continue;
          if (line)
            yield JSON.parse(line);
        }
        done = true;
      } catch (e) {
        if (e instanceof Error && e.name === "AbortError")
          return;
        throw e;
      } finally {
        if (!done)
          controller.abort();
      }
    }
    return new Stream(iterator, controller);
  }
  [Symbol.asyncIterator]() {
    return this.iterator();
  }
  /**
   * Splits the stream into two streams which can be
   * independently read from at different speeds.
   */
  tee() {
    const left = [];
    const right = [];
    const iterator = this.iterator();
    const teeIterator = (queue) => {
      return {
        next: () => {
          if (queue.length === 0) {
            const result = iterator.next();
            left.push(result);
            right.push(result);
          }
          return queue.shift();
        }
      };
    };
    return [
      new Stream(() => teeIterator(left), this.controller),
      new Stream(() => teeIterator(right), this.controller)
    ];
  }
  /**
   * Converts this stream to a newline-separated ReadableStream of
   * JSON stringified values in the stream
   * which can be turned back into a Stream with `Stream.fromReadableStream()`.
   */
  toReadableStream() {
    const self = this;
    let iter;
    const encoder = new TextEncoder();
    return new ReadableStream2({
      async start() {
        iter = self[Symbol.asyncIterator]();
      },
      async pull(ctrl) {
        try {
          const { value, done } = await iter.next();
          if (done)
            return ctrl.close();
          const bytes = encoder.encode(JSON.stringify(value) + "\n");
          ctrl.enqueue(bytes);
        } catch (err) {
          ctrl.error(err);
        }
      },
      async cancel() {
        var _a3;
        await ((_a3 = iter.return) == null ? void 0 : _a3.call(iter));
      }
    });
  }
};
async function* _iterSSEMessages(response, controller) {
  if (!response.body) {
    controller.abort();
    throw new OpenAIError(`Attempted to iterate over a response with no body`);
  }
  const sseDecoder = new SSEDecoder();
  const lineDecoder = new LineDecoder();
  const iter = readableStreamAsyncIterable(response.body);
  for await (const sseChunk of iterSSEChunks(iter)) {
    for (const line of lineDecoder.decode(sseChunk)) {
      const sse = sseDecoder.decode(line);
      if (sse)
        yield sse;
    }
  }
  for (const line of lineDecoder.flush()) {
    const sse = sseDecoder.decode(line);
    if (sse)
      yield sse;
  }
}
async function* iterSSEChunks(iterator) {
  let data = new Uint8Array();
  for await (const chunk of iterator) {
    if (chunk == null) {
      continue;
    }
    const binaryChunk = chunk instanceof ArrayBuffer ? new Uint8Array(chunk) : typeof chunk === "string" ? new TextEncoder().encode(chunk) : chunk;
    let newData = new Uint8Array(data.length + binaryChunk.length);
    newData.set(data);
    newData.set(binaryChunk, data.length);
    data = newData;
    let patternIndex;
    while ((patternIndex = findDoubleNewlineIndex(data)) !== -1) {
      yield data.slice(0, patternIndex);
      data = data.slice(patternIndex);
    }
  }
  if (data.length > 0) {
    yield data;
  }
}
function findDoubleNewlineIndex(buffer) {
  const newline = 10;
  const carriage = 13;
  for (let i = 0; i < buffer.length - 2; i++) {
    if (buffer[i] === newline && buffer[i + 1] === newline) {
      return i + 2;
    }
    if (buffer[i] === carriage && buffer[i + 1] === carriage) {
      return i + 2;
    }
    if (buffer[i] === carriage && buffer[i + 1] === newline && i + 3 < buffer.length && buffer[i + 2] === carriage && buffer[i + 3] === newline) {
      return i + 4;
    }
  }
  return -1;
}
var SSEDecoder = class {
  constructor() {
    this.event = null;
    this.data = [];
    this.chunks = [];
  }
  decode(line) {
    if (line.endsWith("\r")) {
      line = line.substring(0, line.length - 1);
    }
    if (!line) {
      if (!this.event && !this.data.length)
        return null;
      const sse = {
        event: this.event,
        data: this.data.join("\n"),
        raw: this.chunks
      };
      this.event = null;
      this.data = [];
      this.chunks = [];
      return sse;
    }
    this.chunks.push(line);
    if (line.startsWith(":")) {
      return null;
    }
    let [fieldname, _, value] = partition(line, ":");
    if (value.startsWith(" ")) {
      value = value.substring(1);
    }
    if (fieldname === "event") {
      this.event = value;
    } else if (fieldname === "data") {
      this.data.push(value);
    }
    return null;
  }
};
var LineDecoder = class {
  constructor() {
    this.buffer = [];
    this.trailingCR = false;
  }
  decode(chunk) {
    let text = this.decodeText(chunk);
    if (this.trailingCR) {
      text = "\r" + text;
      this.trailingCR = false;
    }
    if (text.endsWith("\r")) {
      this.trailingCR = true;
      text = text.slice(0, -1);
    }
    if (!text) {
      return [];
    }
    const trailingNewline = LineDecoder.NEWLINE_CHARS.has(text[text.length - 1] || "");
    let lines = text.split(LineDecoder.NEWLINE_REGEXP);
    if (trailingNewline) {
      lines.pop();
    }
    if (lines.length === 1 && !trailingNewline) {
      this.buffer.push(lines[0]);
      return [];
    }
    if (this.buffer.length > 0) {
      lines = [this.buffer.join("") + lines[0], ...lines.slice(1)];
      this.buffer = [];
    }
    if (!trailingNewline) {
      this.buffer = [lines.pop() || ""];
    }
    return lines;
  }
  decodeText(bytes) {
    var _a3;
    if (bytes == null)
      return "";
    if (typeof bytes === "string")
      return bytes;
    if (typeof Buffer !== "undefined") {
      if (bytes instanceof Buffer) {
        return bytes.toString();
      }
      if (bytes instanceof Uint8Array) {
        return Buffer.from(bytes).toString();
      }
      throw new OpenAIError(`Unexpected: received non-Uint8Array (${bytes.constructor.name}) stream chunk in an environment with a global "Buffer" defined, which this library assumes to be Node. Please report this error.`);
    }
    if (typeof TextDecoder !== "undefined") {
      if (bytes instanceof Uint8Array || bytes instanceof ArrayBuffer) {
        (_a3 = this.textDecoder) != null ? _a3 : this.textDecoder = new TextDecoder("utf8");
        return this.textDecoder.decode(bytes);
      }
      throw new OpenAIError(`Unexpected: received non-Uint8Array/ArrayBuffer (${bytes.constructor.name}) in a web platform. Please report this error.`);
    }
    throw new OpenAIError(`Unexpected: neither Buffer nor TextDecoder are available as globals. Please report this error.`);
  }
  flush() {
    if (!this.buffer.length && !this.trailingCR) {
      return [];
    }
    const lines = [this.buffer.join("")];
    this.buffer = [];
    this.trailingCR = false;
    return lines;
  }
};
LineDecoder.NEWLINE_CHARS = /* @__PURE__ */ new Set(["\n", "\r"]);
LineDecoder.NEWLINE_REGEXP = /\r\n|[\n\r]/g;
function partition(str2, delimiter) {
  const index = str2.indexOf(delimiter);
  if (index !== -1) {
    return [str2.substring(0, index), delimiter, str2.substring(index + delimiter.length)];
  }
  return [str2, "", ""];
}
function readableStreamAsyncIterable(stream) {
  if (stream[Symbol.asyncIterator])
    return stream;
  const reader = stream.getReader();
  return {
    async next() {
      try {
        const result = await reader.read();
        if (result == null ? void 0 : result.done)
          reader.releaseLock();
        return result;
      } catch (e) {
        reader.releaseLock();
        throw e;
      }
    },
    async return() {
      const cancelPromise = reader.cancel();
      reader.releaseLock();
      await cancelPromise;
      return { done: true, value: void 0 };
    },
    [Symbol.asyncIterator]() {
      return this;
    }
  };
}

// node_modules/openai/uploads.mjs
var isResponseLike = (value) => value != null && typeof value === "object" && typeof value.url === "string" && typeof value.blob === "function";
var isFileLike = (value) => value != null && typeof value === "object" && typeof value.name === "string" && typeof value.lastModified === "number" && isBlobLike(value);
var isBlobLike = (value) => value != null && typeof value === "object" && typeof value.size === "number" && typeof value.type === "string" && typeof value.text === "function" && typeof value.slice === "function" && typeof value.arrayBuffer === "function";
var isUploadable = (value) => {
  return isFileLike(value) || isResponseLike(value) || isFsReadStream(value);
};
async function toFile(value, name, options) {
  var _a3, _b, _c;
  value = await value;
  options != null ? options : options = isFileLike(value) ? { lastModified: value.lastModified, type: value.type } : {};
  if (isResponseLike(value)) {
    const blob = await value.blob();
    name || (name = (_a3 = new URL(value.url).pathname.split(/[\\/]/).pop()) != null ? _a3 : "unknown_file");
    return new File2([blob], name, options);
  }
  const bits = await getBytes(value);
  name || (name = (_b = getName(value)) != null ? _b : "unknown_file");
  if (!options.type) {
    const type = (_c = bits[0]) == null ? void 0 : _c.type;
    if (typeof type === "string") {
      options = { ...options, type };
    }
  }
  return new File2(bits, name, options);
}
async function getBytes(value) {
  var _a3;
  let parts = [];
  if (typeof value === "string" || ArrayBuffer.isView(value) || // includes Uint8Array, Buffer, etc.
  value instanceof ArrayBuffer) {
    parts.push(value);
  } else if (isBlobLike(value)) {
    parts.push(await value.arrayBuffer());
  } else if (isAsyncIterableIterator(value)) {
    for await (const chunk of value) {
      parts.push(chunk);
    }
  } else {
    throw new Error(`Unexpected data type: ${typeof value}; constructor: ${(_a3 = value == null ? void 0 : value.constructor) == null ? void 0 : _a3.name}; props: ${propsForError(value)}`);
  }
  return parts;
}
function propsForError(value) {
  const props = Object.getOwnPropertyNames(value);
  return `[${props.map((p) => `"${p}"`).join(", ")}]`;
}
function getName(value) {
  var _a3;
  return getStringFromMaybeBuffer(value.name) || getStringFromMaybeBuffer(value.filename) || // For fs.ReadStream
  ((_a3 = getStringFromMaybeBuffer(value.path)) == null ? void 0 : _a3.split(/[\\/]/).pop());
}
var getStringFromMaybeBuffer = (x) => {
  if (typeof x === "string")
    return x;
  if (typeof Buffer !== "undefined" && x instanceof Buffer)
    return String(x);
  return void 0;
};
var isAsyncIterableIterator = (value) => value != null && typeof value === "object" && typeof value[Symbol.asyncIterator] === "function";
var isMultipartBody = (body) => body && typeof body === "object" && body.body && body[Symbol.toStringTag] === "MultipartBody";
var multipartFormRequestOptions = async (opts) => {
  const form = await createForm(opts.body);
  return getMultipartRequestOptions(form, opts);
};
var createForm = async (body) => {
  const form = new FormData2();
  await Promise.all(Object.entries(body || {}).map(([key, value]) => addFormValue(form, key, value)));
  return form;
};
var addFormValue = async (form, key, value) => {
  if (value === void 0)
    return;
  if (value == null) {
    throw new TypeError(`Received null for "${key}"; to pass null in FormData, you must use the string 'null'`);
  }
  if (typeof value === "string" || typeof value === "number" || typeof value === "boolean") {
    form.append(key, String(value));
  } else if (isUploadable(value)) {
    const file = await toFile(value);
    form.append(key, file);
  } else if (Array.isArray(value)) {
    await Promise.all(value.map((entry) => addFormValue(form, key + "[]", entry)));
  } else if (typeof value === "object") {
    await Promise.all(Object.entries(value).map(([name, prop]) => addFormValue(form, `${key}[${name}]`, prop)));
  } else {
    throw new TypeError(`Invalid value given to form, expected a string, number, boolean, object, Array, File or Blob but got ${value} instead`);
  }
};

// node_modules/openai/core.mjs
var __classPrivateFieldSet = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var __classPrivateFieldGet = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var _AbstractPage_client;
async function defaultParseResponse(props) {
  const { response } = props;
  if (props.options.stream) {
    debug("response", response.status, response.url, response.headers, response.body);
    if (props.options.__streamClass) {
      return props.options.__streamClass.fromSSEResponse(response, props.controller);
    }
    return Stream.fromSSEResponse(response, props.controller);
  }
  if (response.status === 204) {
    return null;
  }
  if (props.options.__binaryResponse) {
    return response;
  }
  const contentType = response.headers.get("content-type");
  const isJSON = (contentType == null ? void 0 : contentType.includes("application/json")) || (contentType == null ? void 0 : contentType.includes("application/vnd.api+json"));
  if (isJSON) {
    const json = await response.json();
    debug("response", response.status, response.url, response.headers, json);
    return json;
  }
  const text = await response.text();
  debug("response", response.status, response.url, response.headers, text);
  return text;
}
var APIPromise = class extends Promise {
  constructor(responsePromise, parseResponse = defaultParseResponse) {
    super((resolve) => {
      resolve(null);
    });
    this.responsePromise = responsePromise;
    this.parseResponse = parseResponse;
  }
  _thenUnwrap(transform) {
    return new APIPromise(this.responsePromise, async (props) => transform(await this.parseResponse(props)));
  }
  /**
   * Gets the raw `Response` instance instead of parsing the response
   * data.
   *
   * If you want to parse the response body but still get the `Response`
   * instance, you can use {@link withResponse()}.
   *
   * 👋 Getting the wrong TypeScript type for `Response`?
   * Try setting `"moduleResolution": "NodeNext"` if you can,
   * or add one of these imports before your first `import … from 'openai'`:
   * - `import 'openai/shims/node'` (if you're running on Node)
   * - `import 'openai/shims/web'` (otherwise)
   */
  asResponse() {
    return this.responsePromise.then((p) => p.response);
  }
  /**
   * Gets the parsed response data and the raw `Response` instance.
   *
   * If you just want to get the raw `Response` instance without parsing it,
   * you can use {@link asResponse()}.
   *
   *
   * 👋 Getting the wrong TypeScript type for `Response`?
   * Try setting `"moduleResolution": "NodeNext"` if you can,
   * or add one of these imports before your first `import … from 'openai'`:
   * - `import 'openai/shims/node'` (if you're running on Node)
   * - `import 'openai/shims/web'` (otherwise)
   */
  async withResponse() {
    const [data, response] = await Promise.all([this.parse(), this.asResponse()]);
    return { data, response };
  }
  parse() {
    if (!this.parsedPromise) {
      this.parsedPromise = this.responsePromise.then(this.parseResponse);
    }
    return this.parsedPromise;
  }
  then(onfulfilled, onrejected) {
    return this.parse().then(onfulfilled, onrejected);
  }
  catch(onrejected) {
    return this.parse().catch(onrejected);
  }
  finally(onfinally) {
    return this.parse().finally(onfinally);
  }
};
var APIClient = class {
  constructor({
    baseURL,
    maxRetries = 2,
    timeout = 6e5,
    // 10 minutes
    httpAgent,
    fetch: overridenFetch
  }) {
    this.baseURL = baseURL;
    this.maxRetries = validatePositiveInteger("maxRetries", maxRetries);
    this.timeout = validatePositiveInteger("timeout", timeout);
    this.httpAgent = httpAgent;
    this.fetch = overridenFetch != null ? overridenFetch : fetch2;
  }
  authHeaders(opts) {
    return {};
  }
  /**
   * Override this to add your own default headers, for example:
   *
   *  {
   *    ...super.defaultHeaders(),
   *    Authorization: 'Bearer 123',
   *  }
   */
  defaultHeaders(opts) {
    return {
      Accept: "application/json",
      "Content-Type": "application/json",
      "User-Agent": this.getUserAgent(),
      ...getPlatformHeaders(),
      ...this.authHeaders(opts)
    };
  }
  /**
   * Override this to add your own headers validation:
   */
  validateHeaders(headers, customHeaders) {
  }
  defaultIdempotencyKey() {
    return `stainless-node-retry-${uuid4()}`;
  }
  get(path, opts) {
    return this.methodRequest("get", path, opts);
  }
  post(path, opts) {
    return this.methodRequest("post", path, opts);
  }
  patch(path, opts) {
    return this.methodRequest("patch", path, opts);
  }
  put(path, opts) {
    return this.methodRequest("put", path, opts);
  }
  delete(path, opts) {
    return this.methodRequest("delete", path, opts);
  }
  methodRequest(method, path, opts) {
    return this.request(Promise.resolve(opts).then(async (opts2) => {
      const body = opts2 && isBlobLike(opts2 == null ? void 0 : opts2.body) ? new DataView(await opts2.body.arrayBuffer()) : (opts2 == null ? void 0 : opts2.body) instanceof DataView ? opts2.body : (opts2 == null ? void 0 : opts2.body) instanceof ArrayBuffer ? new DataView(opts2.body) : opts2 && ArrayBuffer.isView(opts2 == null ? void 0 : opts2.body) ? new DataView(opts2.body.buffer) : opts2 == null ? void 0 : opts2.body;
      return { method, path, ...opts2, body };
    }));
  }
  getAPIList(path, Page2, opts) {
    return this.requestAPIList(Page2, { method: "get", path, ...opts });
  }
  calculateContentLength(body) {
    if (typeof body === "string") {
      if (typeof Buffer !== "undefined") {
        return Buffer.byteLength(body, "utf8").toString();
      }
      if (typeof TextEncoder !== "undefined") {
        const encoder = new TextEncoder();
        const encoded = encoder.encode(body);
        return encoded.length.toString();
      }
    } else if (ArrayBuffer.isView(body)) {
      return body.byteLength.toString();
    }
    return null;
  }
  buildRequest(options) {
    var _a3, _b, _c, _d, _e, _f;
    const { method, path, query, headers = {} } = options;
    const body = ArrayBuffer.isView(options.body) || options.__binaryRequest && typeof options.body === "string" ? options.body : isMultipartBody(options.body) ? options.body.body : options.body ? JSON.stringify(options.body, null, 2) : null;
    const contentLength = this.calculateContentLength(body);
    const url = this.buildURL(path, query);
    if ("timeout" in options)
      validatePositiveInteger("timeout", options.timeout);
    const timeout = (_a3 = options.timeout) != null ? _a3 : this.timeout;
    const httpAgent = (_c = (_b = options.httpAgent) != null ? _b : this.httpAgent) != null ? _c : getDefaultAgent(url);
    const minAgentTimeout = timeout + 1e3;
    if (typeof ((_d = httpAgent == null ? void 0 : httpAgent.options) == null ? void 0 : _d.timeout) === "number" && minAgentTimeout > ((_e = httpAgent.options.timeout) != null ? _e : 0)) {
      httpAgent.options.timeout = minAgentTimeout;
    }
    if (this.idempotencyHeader && method !== "get") {
      if (!options.idempotencyKey)
        options.idempotencyKey = this.defaultIdempotencyKey();
      headers[this.idempotencyHeader] = options.idempotencyKey;
    }
    const reqHeaders = this.buildHeaders({ options, headers, contentLength });
    const req = {
      method,
      ...body && { body },
      headers: reqHeaders,
      ...httpAgent && { agent: httpAgent },
      // @ts-ignore node-fetch uses a custom AbortSignal type that is
      // not compatible with standard web types
      signal: (_f = options.signal) != null ? _f : null
    };
    return { req, url, timeout };
  }
  buildHeaders({ options, headers, contentLength }) {
    const reqHeaders = {};
    if (contentLength) {
      reqHeaders["content-length"] = contentLength;
    }
    const defaultHeaders = this.defaultHeaders(options);
    applyHeadersMut(reqHeaders, defaultHeaders);
    applyHeadersMut(reqHeaders, headers);
    if (isMultipartBody(options.body) && kind !== "node") {
      delete reqHeaders["content-type"];
    }
    this.validateHeaders(reqHeaders, headers);
    return reqHeaders;
  }
  /**
   * Used as a callback for mutating the given `FinalRequestOptions` object.
   */
  async prepareOptions(options) {
  }
  /**
   * Used as a callback for mutating the given `RequestInit` object.
   *
   * This is useful for cases where you want to add certain headers based off of
   * the request properties, e.g. `method` or `url`.
   */
  async prepareRequest(request, { url, options }) {
  }
  parseHeaders(headers) {
    return !headers ? {} : Symbol.iterator in headers ? Object.fromEntries(Array.from(headers).map((header) => [...header])) : { ...headers };
  }
  makeStatusError(status, error, message, headers) {
    return APIError.generate(status, error, message, headers);
  }
  request(options, remainingRetries = null) {
    return new APIPromise(this.makeRequest(options, remainingRetries));
  }
  async makeRequest(optionsInput, retriesRemaining) {
    var _a3, _b, _c;
    const options = await optionsInput;
    if (retriesRemaining == null) {
      retriesRemaining = (_a3 = options.maxRetries) != null ? _a3 : this.maxRetries;
    }
    await this.prepareOptions(options);
    const { req, url, timeout } = this.buildRequest(options);
    await this.prepareRequest(req, { url, options });
    debug("request", url, options, req.headers);
    if ((_b = options.signal) == null ? void 0 : _b.aborted) {
      throw new APIUserAbortError();
    }
    const controller = new AbortController();
    const response = await this.fetchWithTimeout(url, req, timeout, controller).catch(castToError);
    if (response instanceof Error) {
      if ((_c = options.signal) == null ? void 0 : _c.aborted) {
        throw new APIUserAbortError();
      }
      if (retriesRemaining) {
        return this.retryRequest(options, retriesRemaining);
      }
      if (response.name === "AbortError") {
        throw new APIConnectionTimeoutError();
      }
      throw new APIConnectionError({ cause: response });
    }
    const responseHeaders = createResponseHeaders(response.headers);
    if (!response.ok) {
      if (retriesRemaining && this.shouldRetry(response)) {
        const retryMessage2 = `retrying, ${retriesRemaining} attempts remaining`;
        debug(`response (error; ${retryMessage2})`, response.status, url, responseHeaders);
        return this.retryRequest(options, retriesRemaining, responseHeaders);
      }
      const errText = await response.text().catch((e) => castToError(e).message);
      const errJSON = safeJSON(errText);
      const errMessage = errJSON ? void 0 : errText;
      const retryMessage = retriesRemaining ? `(error; no more retries left)` : `(error; not retryable)`;
      debug(`response (error; ${retryMessage})`, response.status, url, responseHeaders, errMessage);
      const err = this.makeStatusError(response.status, errJSON, errMessage, responseHeaders);
      throw err;
    }
    return { response, options, controller };
  }
  requestAPIList(Page2, options) {
    const request = this.makeRequest(options, null);
    return new PagePromise(this, request, Page2);
  }
  buildURL(path, query) {
    const url = isAbsoluteURL(path) ? new URL(path) : new URL(this.baseURL + (this.baseURL.endsWith("/") && path.startsWith("/") ? path.slice(1) : path));
    const defaultQuery = this.defaultQuery();
    if (!isEmptyObj(defaultQuery)) {
      query = { ...defaultQuery, ...query };
    }
    if (typeof query === "object" && query && !Array.isArray(query)) {
      url.search = this.stringifyQuery(query);
    }
    return url.toString();
  }
  stringifyQuery(query) {
    return Object.entries(query).filter(([_, value]) => typeof value !== "undefined").map(([key, value]) => {
      if (typeof value === "string" || typeof value === "number" || typeof value === "boolean") {
        return `${encodeURIComponent(key)}=${encodeURIComponent(value)}`;
      }
      if (value === null) {
        return `${encodeURIComponent(key)}=`;
      }
      throw new OpenAIError(`Cannot stringify type ${typeof value}; Expected string, number, boolean, or null. If you need to pass nested query parameters, you can manually encode them, e.g. { query: { 'foo[key1]': value1, 'foo[key2]': value2 } }, and please open a GitHub issue requesting better support for your use case.`);
    }).join("&");
  }
  async fetchWithTimeout(url, init, ms, controller) {
    const { signal, ...options } = init || {};
    if (signal)
      signal.addEventListener("abort", () => controller.abort());
    const timeout = setTimeout(() => controller.abort(), ms);
    return this.getRequestClient().fetch.call(void 0, url, { signal: controller.signal, ...options }).finally(() => {
      clearTimeout(timeout);
    });
  }
  getRequestClient() {
    return { fetch: this.fetch };
  }
  shouldRetry(response) {
    const shouldRetryHeader = response.headers.get("x-should-retry");
    if (shouldRetryHeader === "true")
      return true;
    if (shouldRetryHeader === "false")
      return false;
    if (response.status === 408)
      return true;
    if (response.status === 409)
      return true;
    if (response.status === 429)
      return true;
    if (response.status >= 500)
      return true;
    return false;
  }
  async retryRequest(options, retriesRemaining, responseHeaders) {
    var _a3;
    let timeoutMillis;
    const retryAfterMillisHeader = responseHeaders == null ? void 0 : responseHeaders["retry-after-ms"];
    if (retryAfterMillisHeader) {
      const timeoutMs = parseFloat(retryAfterMillisHeader);
      if (!Number.isNaN(timeoutMs)) {
        timeoutMillis = timeoutMs;
      }
    }
    const retryAfterHeader = responseHeaders == null ? void 0 : responseHeaders["retry-after"];
    if (retryAfterHeader && !timeoutMillis) {
      const timeoutSeconds = parseFloat(retryAfterHeader);
      if (!Number.isNaN(timeoutSeconds)) {
        timeoutMillis = timeoutSeconds * 1e3;
      } else {
        timeoutMillis = Date.parse(retryAfterHeader) - Date.now();
      }
    }
    if (!(timeoutMillis && 0 <= timeoutMillis && timeoutMillis < 60 * 1e3)) {
      const maxRetries = (_a3 = options.maxRetries) != null ? _a3 : this.maxRetries;
      timeoutMillis = this.calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries);
    }
    await sleep(timeoutMillis);
    return this.makeRequest(options, retriesRemaining - 1);
  }
  calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries) {
    const initialRetryDelay = 0.5;
    const maxRetryDelay = 8;
    const numRetries = maxRetries - retriesRemaining;
    const sleepSeconds = Math.min(initialRetryDelay * Math.pow(2, numRetries), maxRetryDelay);
    const jitter = 1 - Math.random() * 0.25;
    return sleepSeconds * jitter * 1e3;
  }
  getUserAgent() {
    return `${this.constructor.name}/JS ${VERSION}`;
  }
};
var AbstractPage = class {
  constructor(client, response, body, options) {
    _AbstractPage_client.set(this, void 0);
    __classPrivateFieldSet(this, _AbstractPage_client, client, "f");
    this.options = options;
    this.response = response;
    this.body = body;
  }
  hasNextPage() {
    const items = this.getPaginatedItems();
    if (!items.length)
      return false;
    return this.nextPageInfo() != null;
  }
  async getNextPage() {
    const nextInfo = this.nextPageInfo();
    if (!nextInfo) {
      throw new OpenAIError("No next page expected; please check `.hasNextPage()` before calling `.getNextPage()`.");
    }
    const nextOptions = { ...this.options };
    if ("params" in nextInfo && typeof nextOptions.query === "object") {
      nextOptions.query = { ...nextOptions.query, ...nextInfo.params };
    } else if ("url" in nextInfo) {
      const params = [...Object.entries(nextOptions.query || {}), ...nextInfo.url.searchParams.entries()];
      for (const [key, value] of params) {
        nextInfo.url.searchParams.set(key, value);
      }
      nextOptions.query = void 0;
      nextOptions.path = nextInfo.url.toString();
    }
    return await __classPrivateFieldGet(this, _AbstractPage_client, "f").requestAPIList(this.constructor, nextOptions);
  }
  async *iterPages() {
    let page = this;
    yield page;
    while (page.hasNextPage()) {
      page = await page.getNextPage();
      yield page;
    }
  }
  async *[(_AbstractPage_client = /* @__PURE__ */ new WeakMap(), Symbol.asyncIterator)]() {
    for await (const page of this.iterPages()) {
      for (const item of page.getPaginatedItems()) {
        yield item;
      }
    }
  }
};
var PagePromise = class extends APIPromise {
  constructor(client, request, Page2) {
    super(request, async (props) => new Page2(client, props.response, await defaultParseResponse(props), props.options));
  }
  /**
   * Allow auto-paginating iteration on an unawaited list call, eg:
   *
   *    for await (const item of client.items.list()) {
   *      console.log(item)
   *    }
   */
  async *[Symbol.asyncIterator]() {
    const page = await this;
    for await (const item of page) {
      yield item;
    }
  }
};
var createResponseHeaders = (headers) => {
  return new Proxy(Object.fromEntries(
    // @ts-ignore
    headers.entries()
  ), {
    get(target, name) {
      const key = name.toString();
      return target[key.toLowerCase()] || target[key];
    }
  });
};
var requestOptionsKeys = {
  method: true,
  path: true,
  query: true,
  body: true,
  headers: true,
  maxRetries: true,
  stream: true,
  timeout: true,
  httpAgent: true,
  signal: true,
  idempotencyKey: true,
  __binaryRequest: true,
  __binaryResponse: true,
  __streamClass: true
};
var isRequestOptions = (obj) => {
  return typeof obj === "object" && obj !== null && !isEmptyObj(obj) && Object.keys(obj).every((k) => hasOwn(requestOptionsKeys, k));
};
var getPlatformProperties = () => {
  var _a3, _b;
  if (typeof Deno !== "undefined" && Deno.build != null) {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION,
      "X-Stainless-OS": normalizePlatform(Deno.build.os),
      "X-Stainless-Arch": normalizeArch(Deno.build.arch),
      "X-Stainless-Runtime": "deno",
      "X-Stainless-Runtime-Version": typeof Deno.version === "string" ? Deno.version : (_b = (_a3 = Deno.version) == null ? void 0 : _a3.deno) != null ? _b : "unknown"
    };
  }
  if (typeof EdgeRuntime !== "undefined") {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION,
      "X-Stainless-OS": "Unknown",
      "X-Stainless-Arch": `other:${EdgeRuntime}`,
      "X-Stainless-Runtime": "edge",
      "X-Stainless-Runtime-Version": process.version
    };
  }
  if (Object.prototype.toString.call(typeof process !== "undefined" ? process : 0) === "[object process]") {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION,
      "X-Stainless-OS": normalizePlatform(process.platform),
      "X-Stainless-Arch": normalizeArch(process.arch),
      "X-Stainless-Runtime": "node",
      "X-Stainless-Runtime-Version": process.version
    };
  }
  const browserInfo = getBrowserInfo();
  if (browserInfo) {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION,
      "X-Stainless-OS": "Unknown",
      "X-Stainless-Arch": "unknown",
      "X-Stainless-Runtime": `browser:${browserInfo.browser}`,
      "X-Stainless-Runtime-Version": browserInfo.version
    };
  }
  return {
    "X-Stainless-Lang": "js",
    "X-Stainless-Package-Version": VERSION,
    "X-Stainless-OS": "Unknown",
    "X-Stainless-Arch": "unknown",
    "X-Stainless-Runtime": "unknown",
    "X-Stainless-Runtime-Version": "unknown"
  };
};
function getBrowserInfo() {
  if (typeof navigator === "undefined" || !navigator) {
    return null;
  }
  const browserPatterns = [
    { key: "edge", pattern: /Edge(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "ie", pattern: /MSIE(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "ie", pattern: /Trident(?:.*rv\:(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "chrome", pattern: /Chrome(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "firefox", pattern: /Firefox(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "safari", pattern: /(?:Version\W+(\d+)\.(\d+)(?:\.(\d+))?)?(?:\W+Mobile\S*)?\W+Safari/ }
  ];
  for (const { key, pattern } of browserPatterns) {
    const match = pattern.exec(navigator.userAgent);
    if (match) {
      const major = match[1] || 0;
      const minor = match[2] || 0;
      const patch = match[3] || 0;
      return { browser: key, version: `${major}.${minor}.${patch}` };
    }
  }
  return null;
}
var normalizeArch = (arch) => {
  if (arch === "x32")
    return "x32";
  if (arch === "x86_64" || arch === "x64")
    return "x64";
  if (arch === "arm")
    return "arm";
  if (arch === "aarch64" || arch === "arm64")
    return "arm64";
  if (arch)
    return `other:${arch}`;
  return "unknown";
};
var normalizePlatform = (platform) => {
  platform = platform.toLowerCase();
  if (platform.includes("ios"))
    return "iOS";
  if (platform === "android")
    return "Android";
  if (platform === "darwin")
    return "MacOS";
  if (platform === "win32")
    return "Windows";
  if (platform === "freebsd")
    return "FreeBSD";
  if (platform === "openbsd")
    return "OpenBSD";
  if (platform === "linux")
    return "Linux";
  if (platform)
    return `Other:${platform}`;
  return "Unknown";
};
var _platformHeaders;
var getPlatformHeaders = () => {
  return _platformHeaders != null ? _platformHeaders : _platformHeaders = getPlatformProperties();
};
var safeJSON = (text) => {
  try {
    return JSON.parse(text);
  } catch (err) {
    return void 0;
  }
};
var startsWithSchemeRegexp = new RegExp("^(?:[a-z]+:)?//", "i");
var isAbsoluteURL = (url) => {
  return startsWithSchemeRegexp.test(url);
};
var sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));
var validatePositiveInteger = (name, n) => {
  if (typeof n !== "number" || !Number.isInteger(n)) {
    throw new OpenAIError(`${name} must be an integer`);
  }
  if (n < 0) {
    throw new OpenAIError(`${name} must be a positive integer`);
  }
  return n;
};
var castToError = (err) => {
  if (err instanceof Error)
    return err;
  return new Error(err);
};
var readEnv = (env) => {
  var _a3, _b, _c, _d, _e, _f;
  if (typeof process !== "undefined") {
    return (_c = (_b = (_a3 = process.env) == null ? void 0 : _a3[env]) == null ? void 0 : _b.trim()) != null ? _c : void 0;
  }
  if (typeof Deno !== "undefined") {
    return (_f = (_e = (_d = Deno.env) == null ? void 0 : _d.get) == null ? void 0 : _e.call(_d, env)) == null ? void 0 : _f.trim();
  }
  return void 0;
};
function isEmptyObj(obj) {
  if (!obj)
    return true;
  for (const _k in obj)
    return false;
  return true;
}
function hasOwn(obj, key) {
  return Object.prototype.hasOwnProperty.call(obj, key);
}
function applyHeadersMut(targetHeaders, newHeaders) {
  for (const k in newHeaders) {
    if (!hasOwn(newHeaders, k))
      continue;
    const lowerKey = k.toLowerCase();
    if (!lowerKey)
      continue;
    const val = newHeaders[k];
    if (val === null) {
      delete targetHeaders[lowerKey];
    } else if (val !== void 0) {
      targetHeaders[lowerKey] = val;
    }
  }
}
function debug(action, ...args) {
  var _a3;
  if (typeof process !== "undefined" && ((_a3 = process == null ? void 0 : process.env) == null ? void 0 : _a3["DEBUG"]) === "true") {
    console.log(`OpenAI:DEBUG:${action}`, ...args);
  }
}
var uuid4 = () => {
  return "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx".replace(/[xy]/g, (c) => {
    const r = Math.random() * 16 | 0;
    const v = c === "x" ? r : r & 3 | 8;
    return v.toString(16);
  });
};
var isRunningInBrowser = () => {
  return (
    // @ts-ignore
    typeof window !== "undefined" && // @ts-ignore
    typeof window.document !== "undefined" && // @ts-ignore
    typeof navigator !== "undefined"
  );
};
function isObj(obj) {
  return obj != null && typeof obj === "object" && !Array.isArray(obj);
}

// node_modules/openai/error.mjs
var OpenAIError = class extends Error {
};
var APIError = class extends OpenAIError {
  constructor(status, error, message, headers) {
    super(`${APIError.makeMessage(status, error, message)}`);
    this.status = status;
    this.headers = headers;
    this.request_id = headers == null ? void 0 : headers["x-request-id"];
    const data = error;
    this.error = data;
    this.code = data == null ? void 0 : data["code"];
    this.param = data == null ? void 0 : data["param"];
    this.type = data == null ? void 0 : data["type"];
  }
  static makeMessage(status, error, message) {
    const msg = (error == null ? void 0 : error.message) ? typeof error.message === "string" ? error.message : JSON.stringify(error.message) : error ? JSON.stringify(error) : message;
    if (status && msg) {
      return `${status} ${msg}`;
    }
    if (status) {
      return `${status} status code (no body)`;
    }
    if (msg) {
      return msg;
    }
    return "(no status code or body)";
  }
  static generate(status, errorResponse, message, headers) {
    if (!status) {
      return new APIConnectionError({ cause: castToError(errorResponse) });
    }
    const error = errorResponse == null ? void 0 : errorResponse["error"];
    if (status === 400) {
      return new BadRequestError(status, error, message, headers);
    }
    if (status === 401) {
      return new AuthenticationError(status, error, message, headers);
    }
    if (status === 403) {
      return new PermissionDeniedError(status, error, message, headers);
    }
    if (status === 404) {
      return new NotFoundError(status, error, message, headers);
    }
    if (status === 409) {
      return new ConflictError(status, error, message, headers);
    }
    if (status === 422) {
      return new UnprocessableEntityError(status, error, message, headers);
    }
    if (status === 429) {
      return new RateLimitError(status, error, message, headers);
    }
    if (status >= 500) {
      return new InternalServerError(status, error, message, headers);
    }
    return new APIError(status, error, message, headers);
  }
};
var APIUserAbortError = class extends APIError {
  constructor({ message } = {}) {
    super(void 0, void 0, message || "Request was aborted.", void 0);
    this.status = void 0;
  }
};
var APIConnectionError = class extends APIError {
  constructor({ message, cause }) {
    super(void 0, void 0, message || "Connection error.", void 0);
    this.status = void 0;
    if (cause)
      this.cause = cause;
  }
};
var APIConnectionTimeoutError = class extends APIConnectionError {
  constructor({ message } = {}) {
    super({ message: message != null ? message : "Request timed out." });
  }
};
var BadRequestError = class extends APIError {
  constructor() {
    super(...arguments);
    this.status = 400;
  }
};
var AuthenticationError = class extends APIError {
  constructor() {
    super(...arguments);
    this.status = 401;
  }
};
var PermissionDeniedError = class extends APIError {
  constructor() {
    super(...arguments);
    this.status = 403;
  }
};
var NotFoundError = class extends APIError {
  constructor() {
    super(...arguments);
    this.status = 404;
  }
};
var ConflictError = class extends APIError {
  constructor() {
    super(...arguments);
    this.status = 409;
  }
};
var UnprocessableEntityError = class extends APIError {
  constructor() {
    super(...arguments);
    this.status = 422;
  }
};
var RateLimitError = class extends APIError {
  constructor() {
    super(...arguments);
    this.status = 429;
  }
};
var InternalServerError = class extends APIError {
};

// node_modules/openai/pagination.mjs
var Page = class extends AbstractPage {
  constructor(client, response, body, options) {
    super(client, response, body, options);
    this.data = body.data || [];
    this.object = body.object;
  }
  getPaginatedItems() {
    var _a3;
    return (_a3 = this.data) != null ? _a3 : [];
  }
  // @deprecated Please use `nextPageInfo()` instead
  /**
   * This page represents a response that isn't actually paginated at the API level
   * so there will never be any next page params.
   */
  nextPageParams() {
    return null;
  }
  nextPageInfo() {
    return null;
  }
};
var CursorPage = class extends AbstractPage {
  constructor(client, response, body, options) {
    super(client, response, body, options);
    this.data = body.data || [];
  }
  getPaginatedItems() {
    var _a3;
    return (_a3 = this.data) != null ? _a3 : [];
  }
  // @deprecated Please use `nextPageInfo()` instead
  nextPageParams() {
    const info = this.nextPageInfo();
    if (!info)
      return null;
    if ("params" in info)
      return info.params;
    const params = Object.fromEntries(info.url.searchParams);
    if (!Object.keys(params).length)
      return null;
    return params;
  }
  nextPageInfo() {
    var _a3;
    const data = this.getPaginatedItems();
    if (!data.length) {
      return null;
    }
    const id = (_a3 = data[data.length - 1]) == null ? void 0 : _a3.id;
    if (!id) {
      return null;
    }
    return { params: { after: id } };
  }
};

// node_modules/openai/resource.mjs
var APIResource = class {
  constructor(client) {
    this._client = client;
  }
};

// node_modules/openai/resources/chat/completions.mjs
var Completions = class extends APIResource {
  create(body, options) {
    var _a3;
    return this._client.post("/chat/completions", { body, ...options, stream: (_a3 = body.stream) != null ? _a3 : false });
  }
};
(function(Completions5) {
})(Completions || (Completions = {}));

// node_modules/openai/resources/chat/chat.mjs
var Chat = class extends APIResource {
  constructor() {
    super(...arguments);
    this.completions = new Completions(this._client);
  }
};
(function(Chat3) {
  Chat3.Completions = Completions;
})(Chat || (Chat = {}));

// node_modules/openai/resources/audio/speech.mjs
var Speech = class extends APIResource {
  /**
   * Generates audio from the input text.
   */
  create(body, options) {
    return this._client.post("/audio/speech", { body, ...options, __binaryResponse: true });
  }
};
(function(Speech2) {
})(Speech || (Speech = {}));

// node_modules/openai/resources/audio/transcriptions.mjs
var Transcriptions = class extends APIResource {
  /**
   * Transcribes audio into the input language.
   */
  create(body, options) {
    return this._client.post("/audio/transcriptions", multipartFormRequestOptions({ body, ...options }));
  }
};
(function(Transcriptions2) {
})(Transcriptions || (Transcriptions = {}));

// node_modules/openai/resources/audio/translations.mjs
var Translations = class extends APIResource {
  /**
   * Translates audio into English.
   */
  create(body, options) {
    return this._client.post("/audio/translations", multipartFormRequestOptions({ body, ...options }));
  }
};
(function(Translations2) {
})(Translations || (Translations = {}));

// node_modules/openai/resources/audio/audio.mjs
var Audio = class extends APIResource {
  constructor() {
    super(...arguments);
    this.transcriptions = new Transcriptions(this._client);
    this.translations = new Translations(this._client);
    this.speech = new Speech(this._client);
  }
};
(function(Audio2) {
  Audio2.Transcriptions = Transcriptions;
  Audio2.Translations = Translations;
  Audio2.Speech = Speech;
})(Audio || (Audio = {}));

// node_modules/openai/resources/batches.mjs
var Batches = class extends APIResource {
  /**
   * Creates and executes a batch from an uploaded file of requests
   */
  create(body, options) {
    return this._client.post("/batches", { body, ...options });
  }
  /**
   * Retrieves a batch.
   */
  retrieve(batchId, options) {
    return this._client.get(`/batches/${batchId}`, options);
  }
  list(query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list({}, query);
    }
    return this._client.getAPIList("/batches", BatchesPage, { query, ...options });
  }
  /**
   * Cancels an in-progress batch. The batch will be in status `cancelling` for up to
   * 10 minutes, before changing to `cancelled`, where it will have partial results
   * (if any) available in the output file.
   */
  cancel(batchId, options) {
    return this._client.post(`/batches/${batchId}/cancel`, options);
  }
};
var BatchesPage = class extends CursorPage {
};
(function(Batches2) {
  Batches2.BatchesPage = BatchesPage;
})(Batches || (Batches = {}));

// node_modules/openai/resources/beta/assistants.mjs
var Assistants2 = class extends APIResource {
  /**
   * Create an assistant with a model and instructions.
   */
  create(body, options) {
    return this._client.post("/assistants", {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Retrieves an assistant.
   */
  retrieve(assistantId, options) {
    return this._client.get(`/assistants/${assistantId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Modifies an assistant.
   */
  update(assistantId, body, options) {
    return this._client.post(`/assistants/${assistantId}`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  list(query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list({}, query);
    }
    return this._client.getAPIList("/assistants", AssistantsPage, {
      query,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Delete an assistant.
   */
  del(assistantId, options) {
    return this._client.delete(`/assistants/${assistantId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
};
var AssistantsPage = class extends CursorPage {
};
(function(Assistants3) {
  Assistants3.AssistantsPage = AssistantsPage;
})(Assistants2 || (Assistants2 = {}));

// node_modules/openai/lib/RunnableFunction.mjs
function isRunnableFunctionWithParse(fn) {
  return typeof fn.parse === "function";
}

// node_modules/openai/lib/chatCompletionUtils.mjs
var isAssistantMessage = (message) => {
  return (message == null ? void 0 : message.role) === "assistant";
};
var isFunctionMessage = (message) => {
  return (message == null ? void 0 : message.role) === "function";
};
var isToolMessage = (message) => {
  return (message == null ? void 0 : message.role) === "tool";
};

// node_modules/openai/lib/AbstractChatCompletionRunner.mjs
var __classPrivateFieldSet2 = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var __classPrivateFieldGet2 = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var _AbstractChatCompletionRunner_instances;
var _AbstractChatCompletionRunner_connectedPromise;
var _AbstractChatCompletionRunner_resolveConnectedPromise;
var _AbstractChatCompletionRunner_rejectConnectedPromise;
var _AbstractChatCompletionRunner_endPromise;
var _AbstractChatCompletionRunner_resolveEndPromise;
var _AbstractChatCompletionRunner_rejectEndPromise;
var _AbstractChatCompletionRunner_listeners;
var _AbstractChatCompletionRunner_ended;
var _AbstractChatCompletionRunner_errored;
var _AbstractChatCompletionRunner_aborted;
var _AbstractChatCompletionRunner_catchingPromiseCreated;
var _AbstractChatCompletionRunner_getFinalContent;
var _AbstractChatCompletionRunner_getFinalMessage;
var _AbstractChatCompletionRunner_getFinalFunctionCall;
var _AbstractChatCompletionRunner_getFinalFunctionCallResult;
var _AbstractChatCompletionRunner_calculateTotalUsage;
var _AbstractChatCompletionRunner_handleError;
var _AbstractChatCompletionRunner_validateParams;
var _AbstractChatCompletionRunner_stringifyFunctionCallResult;
var DEFAULT_MAX_CHAT_COMPLETIONS = 10;
var AbstractChatCompletionRunner = class {
  constructor() {
    _AbstractChatCompletionRunner_instances.add(this);
    this.controller = new AbortController();
    _AbstractChatCompletionRunner_connectedPromise.set(this, void 0);
    _AbstractChatCompletionRunner_resolveConnectedPromise.set(this, () => {
    });
    _AbstractChatCompletionRunner_rejectConnectedPromise.set(this, () => {
    });
    _AbstractChatCompletionRunner_endPromise.set(this, void 0);
    _AbstractChatCompletionRunner_resolveEndPromise.set(this, () => {
    });
    _AbstractChatCompletionRunner_rejectEndPromise.set(this, () => {
    });
    _AbstractChatCompletionRunner_listeners.set(this, {});
    this._chatCompletions = [];
    this.messages = [];
    _AbstractChatCompletionRunner_ended.set(this, false);
    _AbstractChatCompletionRunner_errored.set(this, false);
    _AbstractChatCompletionRunner_aborted.set(this, false);
    _AbstractChatCompletionRunner_catchingPromiseCreated.set(this, false);
    _AbstractChatCompletionRunner_handleError.set(this, (error) => {
      __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_errored, true, "f");
      if (error instanceof Error && error.name === "AbortError") {
        error = new APIUserAbortError();
      }
      if (error instanceof APIUserAbortError) {
        __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_aborted, true, "f");
        return this._emit("abort", error);
      }
      if (error instanceof OpenAIError) {
        return this._emit("error", error);
      }
      if (error instanceof Error) {
        const openAIError = new OpenAIError(error.message);
        openAIError.cause = error;
        return this._emit("error", openAIError);
      }
      return this._emit("error", new OpenAIError(String(error)));
    });
    __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_connectedPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_resolveConnectedPromise, resolve, "f");
      __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_rejectConnectedPromise, reject, "f");
    }), "f");
    __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_endPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_resolveEndPromise, resolve, "f");
      __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_rejectEndPromise, reject, "f");
    }), "f");
    __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_connectedPromise, "f").catch(() => {
    });
    __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_endPromise, "f").catch(() => {
    });
  }
  _run(executor) {
    setTimeout(() => {
      executor().then(() => {
        this._emitFinal();
        this._emit("end");
      }, __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_handleError, "f"));
    }, 0);
  }
  _addChatCompletion(chatCompletion) {
    var _a3;
    this._chatCompletions.push(chatCompletion);
    this._emit("chatCompletion", chatCompletion);
    const message = (_a3 = chatCompletion.choices[0]) == null ? void 0 : _a3.message;
    if (message)
      this._addMessage(message);
    return chatCompletion;
  }
  _addMessage(message, emit = true) {
    if (!("content" in message))
      message.content = null;
    this.messages.push(message);
    if (emit) {
      this._emit("message", message);
      if ((isFunctionMessage(message) || isToolMessage(message)) && message.content) {
        this._emit("functionCallResult", message.content);
      } else if (isAssistantMessage(message) && message.function_call) {
        this._emit("functionCall", message.function_call);
      } else if (isAssistantMessage(message) && message.tool_calls) {
        for (const tool_call of message.tool_calls) {
          if (tool_call.type === "function") {
            this._emit("functionCall", tool_call.function);
          }
        }
      }
    }
  }
  _connected() {
    if (this.ended)
      return;
    __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_resolveConnectedPromise, "f").call(this);
    this._emit("connect");
  }
  get ended() {
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_ended, "f");
  }
  get errored() {
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_errored, "f");
  }
  get aborted() {
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_aborted, "f");
  }
  abort() {
    this.controller.abort();
  }
  /**
   * Adds the listener function to the end of the listeners array for the event.
   * No checks are made to see if the listener has already been added. Multiple calls passing
   * the same combination of event and listener will result in the listener being added, and
   * called, multiple times.
   * @returns this ChatCompletionStream, so that calls can be chained
   */
  on(event, listener) {
    const listeners = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_listeners, "f")[event] || (__classPrivateFieldGet2(this, _AbstractChatCompletionRunner_listeners, "f")[event] = []);
    listeners.push({ listener });
    return this;
  }
  /**
   * Removes the specified listener from the listener array for the event.
   * off() will remove, at most, one instance of a listener from the listener array. If any single
   * listener has been added multiple times to the listener array for the specified event, then
   * off() must be called multiple times to remove each instance.
   * @returns this ChatCompletionStream, so that calls can be chained
   */
  off(event, listener) {
    const listeners = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_listeners, "f")[event];
    if (!listeners)
      return this;
    const index = listeners.findIndex((l) => l.listener === listener);
    if (index >= 0)
      listeners.splice(index, 1);
    return this;
  }
  /**
   * Adds a one-time listener function for the event. The next time the event is triggered,
   * this listener is removed and then invoked.
   * @returns this ChatCompletionStream, so that calls can be chained
   */
  once(event, listener) {
    const listeners = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_listeners, "f")[event] || (__classPrivateFieldGet2(this, _AbstractChatCompletionRunner_listeners, "f")[event] = []);
    listeners.push({ listener, once: true });
    return this;
  }
  /**
   * This is similar to `.once()`, but returns a Promise that resolves the next time
   * the event is triggered, instead of calling a listener callback.
   * @returns a Promise that resolves the next time given event is triggered,
   * or rejects if an error is emitted.  (If you request the 'error' event,
   * returns a promise that resolves with the error).
   *
   * Example:
   *
   *   const message = await stream.emitted('message') // rejects if the stream errors
   */
  emitted(event) {
    return new Promise((resolve, reject) => {
      __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_catchingPromiseCreated, true, "f");
      if (event !== "error")
        this.once("error", reject);
      this.once(event, resolve);
    });
  }
  async done() {
    __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_catchingPromiseCreated, true, "f");
    await __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_endPromise, "f");
  }
  /**
   * @returns a promise that resolves with the final ChatCompletion, or rejects
   * if an error occurred or the stream ended prematurely without producing a ChatCompletion.
   */
  async finalChatCompletion() {
    await this.done();
    const completion = this._chatCompletions[this._chatCompletions.length - 1];
    if (!completion)
      throw new OpenAIError("stream ended without producing a ChatCompletion");
    return completion;
  }
  /**
   * @returns a promise that resolves with the content of the final ChatCompletionMessage, or rejects
   * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.
   */
  async finalContent() {
    await this.done();
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalContent).call(this);
  }
  /**
   * @returns a promise that resolves with the the final assistant ChatCompletionMessage response,
   * or rejects if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.
   */
  async finalMessage() {
    await this.done();
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalMessage).call(this);
  }
  /**
   * @returns a promise that resolves with the content of the final FunctionCall, or rejects
   * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.
   */
  async finalFunctionCall() {
    await this.done();
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionCall).call(this);
  }
  async finalFunctionCallResult() {
    await this.done();
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionCallResult).call(this);
  }
  async totalUsage() {
    await this.done();
    return __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_calculateTotalUsage).call(this);
  }
  allChatCompletions() {
    return [...this._chatCompletions];
  }
  _emit(event, ...args) {
    if (__classPrivateFieldGet2(this, _AbstractChatCompletionRunner_ended, "f")) {
      return;
    }
    if (event === "end") {
      __classPrivateFieldSet2(this, _AbstractChatCompletionRunner_ended, true, "f");
      __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_resolveEndPromise, "f").call(this);
    }
    const listeners = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_listeners, "f")[event];
    if (listeners) {
      __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_listeners, "f")[event] = listeners.filter((l) => !l.once);
      listeners.forEach(({ listener }) => listener(...args));
    }
    if (event === "abort") {
      const error = args[0];
      if (!__classPrivateFieldGet2(this, _AbstractChatCompletionRunner_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_rejectEndPromise, "f").call(this, error);
      this._emit("end");
      return;
    }
    if (event === "error") {
      const error = args[0];
      if (!__classPrivateFieldGet2(this, _AbstractChatCompletionRunner_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_rejectEndPromise, "f").call(this, error);
      this._emit("end");
    }
  }
  _emitFinal() {
    const completion = this._chatCompletions[this._chatCompletions.length - 1];
    if (completion)
      this._emit("finalChatCompletion", completion);
    const finalMessage = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalMessage).call(this);
    if (finalMessage)
      this._emit("finalMessage", finalMessage);
    const finalContent = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalContent).call(this);
    if (finalContent)
      this._emit("finalContent", finalContent);
    const finalFunctionCall = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionCall).call(this);
    if (finalFunctionCall)
      this._emit("finalFunctionCall", finalFunctionCall);
    const finalFunctionCallResult = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionCallResult).call(this);
    if (finalFunctionCallResult != null)
      this._emit("finalFunctionCallResult", finalFunctionCallResult);
    if (this._chatCompletions.some((c) => c.usage)) {
      this._emit("totalUsage", __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_calculateTotalUsage).call(this));
    }
  }
  async _createChatCompletion(completions, params, options) {
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_validateParams).call(this, params);
    const chatCompletion = await completions.create({ ...params, stream: false }, { ...options, signal: this.controller.signal });
    this._connected();
    return this._addChatCompletion(chatCompletion);
  }
  async _runChatCompletion(completions, params, options) {
    for (const message of params.messages) {
      this._addMessage(message, false);
    }
    return await this._createChatCompletion(completions, params, options);
  }
  async _runFunctions(completions, params, options) {
    var _a3;
    const role = "function";
    const { function_call = "auto", stream, ...restParams } = params;
    const singleFunctionToCall = typeof function_call !== "string" && (function_call == null ? void 0 : function_call.name);
    const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};
    const functionsByName = {};
    for (const f of params.functions) {
      functionsByName[f.name || f.function.name] = f;
    }
    const functions = params.functions.map((f) => ({
      name: f.name || f.function.name,
      parameters: f.parameters,
      description: f.description
    }));
    for (const message of params.messages) {
      this._addMessage(message, false);
    }
    for (let i = 0; i < maxChatCompletions; ++i) {
      const chatCompletion = await this._createChatCompletion(completions, {
        ...restParams,
        function_call,
        functions,
        messages: [...this.messages]
      }, options);
      const message = (_a3 = chatCompletion.choices[0]) == null ? void 0 : _a3.message;
      if (!message) {
        throw new OpenAIError(`missing message in ChatCompletion response`);
      }
      if (!message.function_call)
        return;
      const { name, arguments: args } = message.function_call;
      const fn = functionsByName[name];
      if (!fn) {
        const content2 = `Invalid function_call: ${JSON.stringify(name)}. Available options are: ${functions.map((f) => JSON.stringify(f.name)).join(", ")}. Please try again`;
        this._addMessage({ role, name, content: content2 });
        continue;
      } else if (singleFunctionToCall && singleFunctionToCall !== name) {
        const content2 = `Invalid function_call: ${JSON.stringify(name)}. ${JSON.stringify(singleFunctionToCall)} requested. Please try again`;
        this._addMessage({ role, name, content: content2 });
        continue;
      }
      let parsed;
      try {
        parsed = isRunnableFunctionWithParse(fn) ? await fn.parse(args) : args;
      } catch (error) {
        this._addMessage({
          role,
          name,
          content: error instanceof Error ? error.message : String(error)
        });
        continue;
      }
      const rawContent = await fn.function(parsed, this);
      const content = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_stringifyFunctionCallResult).call(this, rawContent);
      this._addMessage({ role, name, content });
      if (singleFunctionToCall)
        return;
    }
  }
  async _runTools(completions, params, options) {
    var _a3, _b;
    const role = "tool";
    const { tool_choice = "auto", stream, ...restParams } = params;
    const singleFunctionToCall = typeof tool_choice !== "string" && ((_a3 = tool_choice == null ? void 0 : tool_choice.function) == null ? void 0 : _a3.name);
    const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};
    const functionsByName = {};
    for (const f of params.tools) {
      if (f.type === "function") {
        functionsByName[f.function.name || f.function.function.name] = f.function;
      }
    }
    const tools = "tools" in params ? params.tools.map((t) => t.type === "function" ? {
      type: "function",
      function: {
        name: t.function.name || t.function.function.name,
        parameters: t.function.parameters,
        description: t.function.description
      }
    } : t) : void 0;
    for (const message of params.messages) {
      this._addMessage(message, false);
    }
    for (let i = 0; i < maxChatCompletions; ++i) {
      const chatCompletion = await this._createChatCompletion(completions, {
        ...restParams,
        tool_choice,
        tools,
        messages: [...this.messages]
      }, options);
      const message = (_b = chatCompletion.choices[0]) == null ? void 0 : _b.message;
      if (!message) {
        throw new OpenAIError(`missing message in ChatCompletion response`);
      }
      if (!message.tool_calls) {
        return;
      }
      for (const tool_call of message.tool_calls) {
        if (tool_call.type !== "function")
          continue;
        const tool_call_id = tool_call.id;
        const { name, arguments: args } = tool_call.function;
        const fn = functionsByName[name];
        if (!fn) {
          const content2 = `Invalid tool_call: ${JSON.stringify(name)}. Available options are: ${tools.map((f) => JSON.stringify(f.function.name)).join(", ")}. Please try again`;
          this._addMessage({ role, tool_call_id, content: content2 });
          continue;
        } else if (singleFunctionToCall && singleFunctionToCall !== name) {
          const content2 = `Invalid tool_call: ${JSON.stringify(name)}. ${JSON.stringify(singleFunctionToCall)} requested. Please try again`;
          this._addMessage({ role, tool_call_id, content: content2 });
          continue;
        }
        let parsed;
        try {
          parsed = isRunnableFunctionWithParse(fn) ? await fn.parse(args) : args;
        } catch (error) {
          const content2 = error instanceof Error ? error.message : String(error);
          this._addMessage({ role, tool_call_id, content: content2 });
          continue;
        }
        const rawContent = await fn.function(parsed, this);
        const content = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_stringifyFunctionCallResult).call(this, rawContent);
        this._addMessage({ role, tool_call_id, content });
        if (singleFunctionToCall) {
          return;
        }
      }
    }
    return;
  }
};
_AbstractChatCompletionRunner_connectedPromise = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_resolveConnectedPromise = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_rejectConnectedPromise = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_endPromise = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_resolveEndPromise = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_rejectEndPromise = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_listeners = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_ended = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_errored = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_aborted = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_catchingPromiseCreated = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_handleError = /* @__PURE__ */ new WeakMap(), _AbstractChatCompletionRunner_instances = /* @__PURE__ */ new WeakSet(), _AbstractChatCompletionRunner_getFinalContent = function _AbstractChatCompletionRunner_getFinalContent2() {
  var _a3;
  return (_a3 = __classPrivateFieldGet2(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalMessage).call(this).content) != null ? _a3 : null;
}, _AbstractChatCompletionRunner_getFinalMessage = function _AbstractChatCompletionRunner_getFinalMessage2() {
  var _a3;
  let i = this.messages.length;
  while (i-- > 0) {
    const message = this.messages[i];
    if (isAssistantMessage(message)) {
      const { function_call, ...rest } = message;
      const ret = { ...rest, content: (_a3 = message.content) != null ? _a3 : null };
      if (function_call) {
        ret.function_call = function_call;
      }
      return ret;
    }
  }
  throw new OpenAIError("stream ended without producing a ChatCompletionMessage with role=assistant");
}, _AbstractChatCompletionRunner_getFinalFunctionCall = function _AbstractChatCompletionRunner_getFinalFunctionCall2() {
  var _a3, _b;
  for (let i = this.messages.length - 1; i >= 0; i--) {
    const message = this.messages[i];
    if (isAssistantMessage(message) && (message == null ? void 0 : message.function_call)) {
      return message.function_call;
    }
    if (isAssistantMessage(message) && ((_a3 = message == null ? void 0 : message.tool_calls) == null ? void 0 : _a3.length)) {
      return (_b = message.tool_calls.at(-1)) == null ? void 0 : _b.function;
    }
  }
  return;
}, _AbstractChatCompletionRunner_getFinalFunctionCallResult = function _AbstractChatCompletionRunner_getFinalFunctionCallResult2() {
  for (let i = this.messages.length - 1; i >= 0; i--) {
    const message = this.messages[i];
    if (isFunctionMessage(message) && message.content != null) {
      return message.content;
    }
    if (isToolMessage(message) && message.content != null && this.messages.some((x) => {
      var _a3;
      return x.role === "assistant" && ((_a3 = x.tool_calls) == null ? void 0 : _a3.some((y) => y.type === "function" && y.id === message.tool_call_id));
    })) {
      return message.content;
    }
  }
  return;
}, _AbstractChatCompletionRunner_calculateTotalUsage = function _AbstractChatCompletionRunner_calculateTotalUsage2() {
  const total = {
    completion_tokens: 0,
    prompt_tokens: 0,
    total_tokens: 0
  };
  for (const { usage } of this._chatCompletions) {
    if (usage) {
      total.completion_tokens += usage.completion_tokens;
      total.prompt_tokens += usage.prompt_tokens;
      total.total_tokens += usage.total_tokens;
    }
  }
  return total;
}, _AbstractChatCompletionRunner_validateParams = function _AbstractChatCompletionRunner_validateParams2(params) {
  if (params.n != null && params.n > 1) {
    throw new OpenAIError("ChatCompletion convenience helpers only support n=1 at this time. To use n>1, please use chat.completions.create() directly.");
  }
}, _AbstractChatCompletionRunner_stringifyFunctionCallResult = function _AbstractChatCompletionRunner_stringifyFunctionCallResult2(rawContent) {
  return typeof rawContent === "string" ? rawContent : rawContent === void 0 ? "undefined" : JSON.stringify(rawContent);
};

// node_modules/openai/lib/ChatCompletionRunner.mjs
var ChatCompletionRunner = class extends AbstractChatCompletionRunner {
  /** @deprecated - please use `runTools` instead. */
  static runFunctions(completions, params, options) {
    const runner = new ChatCompletionRunner();
    const opts = {
      ...options,
      headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "runFunctions" }
    };
    runner._run(() => runner._runFunctions(completions, params, opts));
    return runner;
  }
  static runTools(completions, params, options) {
    const runner = new ChatCompletionRunner();
    const opts = {
      ...options,
      headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "runTools" }
    };
    runner._run(() => runner._runTools(completions, params, opts));
    return runner;
  }
  _addMessage(message) {
    super._addMessage(message);
    if (isAssistantMessage(message) && message.content) {
      this._emit("content", message.content);
    }
  }
};

// node_modules/openai/lib/ChatCompletionStream.mjs
var __classPrivateFieldGet3 = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var __classPrivateFieldSet3 = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var _ChatCompletionStream_instances;
var _ChatCompletionStream_currentChatCompletionSnapshot;
var _ChatCompletionStream_beginRequest;
var _ChatCompletionStream_addChunk;
var _ChatCompletionStream_endRequest;
var _ChatCompletionStream_accumulateChatCompletion;
var ChatCompletionStream = class extends AbstractChatCompletionRunner {
  constructor() {
    super(...arguments);
    _ChatCompletionStream_instances.add(this);
    _ChatCompletionStream_currentChatCompletionSnapshot.set(this, void 0);
  }
  get currentChatCompletionSnapshot() {
    return __classPrivateFieldGet3(this, _ChatCompletionStream_currentChatCompletionSnapshot, "f");
  }
  /**
   * Intended for use on the frontend, consuming a stream produced with
   * `.toReadableStream()` on the backend.
   *
   * Note that messages sent to the model do not appear in `.on('message')`
   * in this context.
   */
  static fromReadableStream(stream) {
    const runner = new ChatCompletionStream();
    runner._run(() => runner._fromReadableStream(stream));
    return runner;
  }
  static createChatCompletion(completions, params, options) {
    const runner = new ChatCompletionStream();
    runner._run(() => runner._runChatCompletion(completions, { ...params, stream: true }, { ...options, headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "stream" } }));
    return runner;
  }
  async _createChatCompletion(completions, params, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    __classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_beginRequest).call(this);
    const stream = await completions.create({ ...params, stream: true }, { ...options, signal: this.controller.signal });
    this._connected();
    for await (const chunk of stream) {
      __classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_addChunk).call(this, chunk);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError();
    }
    return this._addChatCompletion(__classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_endRequest).call(this));
  }
  async _fromReadableStream(readableStream, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    __classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_beginRequest).call(this);
    this._connected();
    const stream = Stream.fromReadableStream(readableStream, this.controller);
    let chatId;
    for await (const chunk of stream) {
      if (chatId && chatId !== chunk.id) {
        this._addChatCompletion(__classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_endRequest).call(this));
      }
      __classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_addChunk).call(this, chunk);
      chatId = chunk.id;
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError();
    }
    return this._addChatCompletion(__classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_endRequest).call(this));
  }
  [(_ChatCompletionStream_currentChatCompletionSnapshot = /* @__PURE__ */ new WeakMap(), _ChatCompletionStream_instances = /* @__PURE__ */ new WeakSet(), _ChatCompletionStream_beginRequest = function _ChatCompletionStream_beginRequest2() {
    if (this.ended)
      return;
    __classPrivateFieldSet3(this, _ChatCompletionStream_currentChatCompletionSnapshot, void 0, "f");
  }, _ChatCompletionStream_addChunk = function _ChatCompletionStream_addChunk2(chunk) {
    var _a3, _b, _c;
    if (this.ended)
      return;
    const completion = __classPrivateFieldGet3(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_accumulateChatCompletion).call(this, chunk);
    this._emit("chunk", chunk, completion);
    const delta = (_b = (_a3 = chunk.choices[0]) == null ? void 0 : _a3.delta) == null ? void 0 : _b.content;
    const snapshot = (_c = completion.choices[0]) == null ? void 0 : _c.message;
    if (delta != null && (snapshot == null ? void 0 : snapshot.role) === "assistant" && (snapshot == null ? void 0 : snapshot.content)) {
      this._emit("content", delta, snapshot.content);
    }
  }, _ChatCompletionStream_endRequest = function _ChatCompletionStream_endRequest2() {
    if (this.ended) {
      throw new OpenAIError(`stream has ended, this shouldn't happen`);
    }
    const snapshot = __classPrivateFieldGet3(this, _ChatCompletionStream_currentChatCompletionSnapshot, "f");
    if (!snapshot) {
      throw new OpenAIError(`request ended without sending any chunks`);
    }
    __classPrivateFieldSet3(this, _ChatCompletionStream_currentChatCompletionSnapshot, void 0, "f");
    return finalizeChatCompletion(snapshot);
  }, _ChatCompletionStream_accumulateChatCompletion = function _ChatCompletionStream_accumulateChatCompletion2(chunk) {
    var _a4, _b2, _c2, _d;
    var _a3, _b, _c;
    let snapshot = __classPrivateFieldGet3(this, _ChatCompletionStream_currentChatCompletionSnapshot, "f");
    const { choices, ...rest } = chunk;
    if (!snapshot) {
      snapshot = __classPrivateFieldSet3(this, _ChatCompletionStream_currentChatCompletionSnapshot, {
        ...rest,
        choices: []
      }, "f");
    } else {
      Object.assign(snapshot, rest);
    }
    for (const { delta, finish_reason, index, logprobs = null, ...other } of chunk.choices) {
      let choice = snapshot.choices[index];
      if (!choice) {
        choice = snapshot.choices[index] = { finish_reason, index, message: {}, logprobs, ...other };
      }
      if (logprobs) {
        if (!choice.logprobs) {
          choice.logprobs = Object.assign({}, logprobs);
        } else {
          const { content: content2, ...rest3 } = logprobs;
          Object.assign(choice.logprobs, rest3);
          if (content2) {
            (_a4 = (_a3 = choice.logprobs).content) != null ? _a4 : _a3.content = [];
            choice.logprobs.content.push(...content2);
          }
        }
      }
      if (finish_reason)
        choice.finish_reason = finish_reason;
      Object.assign(choice, other);
      if (!delta)
        continue;
      const { content, function_call, role, tool_calls, ...rest2 } = delta;
      Object.assign(choice.message, rest2);
      if (content)
        choice.message.content = (choice.message.content || "") + content;
      if (role)
        choice.message.role = role;
      if (function_call) {
        if (!choice.message.function_call) {
          choice.message.function_call = function_call;
        } else {
          if (function_call.name)
            choice.message.function_call.name = function_call.name;
          if (function_call.arguments) {
            (_b2 = (_b = choice.message.function_call).arguments) != null ? _b2 : _b.arguments = "";
            choice.message.function_call.arguments += function_call.arguments;
          }
        }
      }
      if (tool_calls) {
        if (!choice.message.tool_calls)
          choice.message.tool_calls = [];
        for (const { index: index2, id, type, function: fn, ...rest3 } of tool_calls) {
          const tool_call = (_c2 = (_c = choice.message.tool_calls)[index2]) != null ? _c2 : _c[index2] = {};
          Object.assign(tool_call, rest3);
          if (id)
            tool_call.id = id;
          if (type)
            tool_call.type = type;
          if (fn)
            (_d = tool_call.function) != null ? _d : tool_call.function = { arguments: "" };
          if (fn == null ? void 0 : fn.name)
            tool_call.function.name = fn.name;
          if (fn == null ? void 0 : fn.arguments)
            tool_call.function.arguments += fn.arguments;
        }
      }
    }
    return snapshot;
  }, Symbol.asyncIterator)]() {
    const pushQueue = [];
    const readQueue = [];
    let done = false;
    this.on("chunk", (chunk) => {
      const reader = readQueue.shift();
      if (reader) {
        reader.resolve(chunk);
      } else {
        pushQueue.push(chunk);
      }
    });
    this.on("end", () => {
      done = true;
      for (const reader of readQueue) {
        reader.resolve(void 0);
      }
      readQueue.length = 0;
    });
    this.on("abort", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    this.on("error", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    return {
      next: async () => {
        if (!pushQueue.length) {
          if (done) {
            return { value: void 0, done: true };
          }
          return new Promise((resolve, reject) => readQueue.push({ resolve, reject })).then((chunk2) => chunk2 ? { value: chunk2, done: false } : { value: void 0, done: true });
        }
        const chunk = pushQueue.shift();
        return { value: chunk, done: false };
      },
      return: async () => {
        this.abort();
        return { value: void 0, done: true };
      }
    };
  }
  toReadableStream() {
    const stream = new Stream(this[Symbol.asyncIterator].bind(this), this.controller);
    return stream.toReadableStream();
  }
};
function finalizeChatCompletion(snapshot) {
  const { id, choices, created, model, system_fingerprint, ...rest } = snapshot;
  return {
    ...rest,
    id,
    choices: choices.map(({ message, finish_reason, index, logprobs, ...choiceRest }) => {
      if (!finish_reason)
        throw new OpenAIError(`missing finish_reason for choice ${index}`);
      const { content = null, function_call, tool_calls, ...messageRest } = message;
      const role = message.role;
      if (!role)
        throw new OpenAIError(`missing role for choice ${index}`);
      if (function_call) {
        const { arguments: args, name } = function_call;
        if (args == null)
          throw new OpenAIError(`missing function_call.arguments for choice ${index}`);
        if (!name)
          throw new OpenAIError(`missing function_call.name for choice ${index}`);
        return {
          ...choiceRest,
          message: { content, function_call: { arguments: args, name }, role },
          finish_reason,
          index,
          logprobs
        };
      }
      if (tool_calls) {
        return {
          ...choiceRest,
          index,
          finish_reason,
          logprobs,
          message: {
            ...messageRest,
            role,
            content,
            tool_calls: tool_calls.map((tool_call, i) => {
              const { function: fn, type, id: id2, ...toolRest } = tool_call;
              const { arguments: args, name, ...fnRest } = fn || {};
              if (id2 == null)
                throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].id
${str(snapshot)}`);
              if (type == null)
                throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].type
${str(snapshot)}`);
              if (name == null)
                throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].function.name
${str(snapshot)}`);
              if (args == null)
                throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].function.arguments
${str(snapshot)}`);
              return { ...toolRest, id: id2, type, function: { ...fnRest, name, arguments: args } };
            })
          }
        };
      }
      return {
        ...choiceRest,
        message: { ...messageRest, content, role },
        finish_reason,
        index,
        logprobs
      };
    }),
    created,
    model,
    object: "chat.completion",
    ...system_fingerprint ? { system_fingerprint } : {}
  };
}
function str(x) {
  return JSON.stringify(x);
}

// node_modules/openai/lib/ChatCompletionStreamingRunner.mjs
var ChatCompletionStreamingRunner = class extends ChatCompletionStream {
  static fromReadableStream(stream) {
    const runner = new ChatCompletionStreamingRunner();
    runner._run(() => runner._fromReadableStream(stream));
    return runner;
  }
  /** @deprecated - please use `runTools` instead. */
  static runFunctions(completions, params, options) {
    const runner = new ChatCompletionStreamingRunner();
    const opts = {
      ...options,
      headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "runFunctions" }
    };
    runner._run(() => runner._runFunctions(completions, params, opts));
    return runner;
  }
  static runTools(completions, params, options) {
    const runner = new ChatCompletionStreamingRunner();
    const opts = {
      ...options,
      headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "runTools" }
    };
    runner._run(() => runner._runTools(completions, params, opts));
    return runner;
  }
};

// node_modules/openai/resources/beta/chat/completions.mjs
var Completions2 = class extends APIResource {
  runFunctions(body, options) {
    if (body.stream) {
      return ChatCompletionStreamingRunner.runFunctions(this._client.chat.completions, body, options);
    }
    return ChatCompletionRunner.runFunctions(this._client.chat.completions, body, options);
  }
  runTools(body, options) {
    if (body.stream) {
      return ChatCompletionStreamingRunner.runTools(this._client.chat.completions, body, options);
    }
    return ChatCompletionRunner.runTools(this._client.chat.completions, body, options);
  }
  /**
   * Creates a chat completion stream
   */
  stream(body, options) {
    return ChatCompletionStream.createChatCompletion(this._client.chat.completions, body, options);
  }
};

// node_modules/openai/resources/beta/chat/chat.mjs
var Chat2 = class extends APIResource {
  constructor() {
    super(...arguments);
    this.completions = new Completions2(this._client);
  }
};
(function(Chat3) {
  Chat3.Completions = Completions2;
})(Chat2 || (Chat2 = {}));

// node_modules/openai/lib/AbstractAssistantStreamRunner.mjs
var __classPrivateFieldSet4 = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var __classPrivateFieldGet4 = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var _AbstractAssistantStreamRunner_connectedPromise;
var _AbstractAssistantStreamRunner_resolveConnectedPromise;
var _AbstractAssistantStreamRunner_rejectConnectedPromise;
var _AbstractAssistantStreamRunner_endPromise;
var _AbstractAssistantStreamRunner_resolveEndPromise;
var _AbstractAssistantStreamRunner_rejectEndPromise;
var _AbstractAssistantStreamRunner_listeners;
var _AbstractAssistantStreamRunner_ended;
var _AbstractAssistantStreamRunner_errored;
var _AbstractAssistantStreamRunner_aborted;
var _AbstractAssistantStreamRunner_catchingPromiseCreated;
var _AbstractAssistantStreamRunner_handleError;
var AbstractAssistantStreamRunner = class {
  constructor() {
    this.controller = new AbortController();
    _AbstractAssistantStreamRunner_connectedPromise.set(this, void 0);
    _AbstractAssistantStreamRunner_resolveConnectedPromise.set(this, () => {
    });
    _AbstractAssistantStreamRunner_rejectConnectedPromise.set(this, () => {
    });
    _AbstractAssistantStreamRunner_endPromise.set(this, void 0);
    _AbstractAssistantStreamRunner_resolveEndPromise.set(this, () => {
    });
    _AbstractAssistantStreamRunner_rejectEndPromise.set(this, () => {
    });
    _AbstractAssistantStreamRunner_listeners.set(this, {});
    _AbstractAssistantStreamRunner_ended.set(this, false);
    _AbstractAssistantStreamRunner_errored.set(this, false);
    _AbstractAssistantStreamRunner_aborted.set(this, false);
    _AbstractAssistantStreamRunner_catchingPromiseCreated.set(this, false);
    _AbstractAssistantStreamRunner_handleError.set(this, (error) => {
      __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_errored, true, "f");
      if (error instanceof Error && error.name === "AbortError") {
        error = new APIUserAbortError();
      }
      if (error instanceof APIUserAbortError) {
        __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_aborted, true, "f");
        return this._emit("abort", error);
      }
      if (error instanceof OpenAIError) {
        return this._emit("error", error);
      }
      if (error instanceof Error) {
        const openAIError = new OpenAIError(error.message);
        openAIError.cause = error;
        return this._emit("error", openAIError);
      }
      return this._emit("error", new OpenAIError(String(error)));
    });
    __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_connectedPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_resolveConnectedPromise, resolve, "f");
      __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_rejectConnectedPromise, reject, "f");
    }), "f");
    __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_endPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_resolveEndPromise, resolve, "f");
      __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_rejectEndPromise, reject, "f");
    }), "f");
    __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_connectedPromise, "f").catch(() => {
    });
    __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_endPromise, "f").catch(() => {
    });
  }
  _run(executor) {
    setTimeout(() => {
      executor().then(() => {
        this._emit("end");
      }, __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_handleError, "f"));
    }, 0);
  }
  _addRun(run) {
    return run;
  }
  _connected() {
    if (this.ended)
      return;
    __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_resolveConnectedPromise, "f").call(this);
    this._emit("connect");
  }
  get ended() {
    return __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_ended, "f");
  }
  get errored() {
    return __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_errored, "f");
  }
  get aborted() {
    return __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_aborted, "f");
  }
  abort() {
    this.controller.abort();
  }
  /**
   * Adds the listener function to the end of the listeners array for the event.
   * No checks are made to see if the listener has already been added. Multiple calls passing
   * the same combination of event and listener will result in the listener being added, and
   * called, multiple times.
   * @returns this ChatCompletionStream, so that calls can be chained
   */
  on(event, listener) {
    const listeners = __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_listeners, "f")[event] || (__classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_listeners, "f")[event] = []);
    listeners.push({ listener });
    return this;
  }
  /**
   * Removes the specified listener from the listener array for the event.
   * off() will remove, at most, one instance of a listener from the listener array. If any single
   * listener has been added multiple times to the listener array for the specified event, then
   * off() must be called multiple times to remove each instance.
   * @returns this ChatCompletionStream, so that calls can be chained
   */
  off(event, listener) {
    const listeners = __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_listeners, "f")[event];
    if (!listeners)
      return this;
    const index = listeners.findIndex((l) => l.listener === listener);
    if (index >= 0)
      listeners.splice(index, 1);
    return this;
  }
  /**
   * Adds a one-time listener function for the event. The next time the event is triggered,
   * this listener is removed and then invoked.
   * @returns this ChatCompletionStream, so that calls can be chained
   */
  once(event, listener) {
    const listeners = __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_listeners, "f")[event] || (__classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_listeners, "f")[event] = []);
    listeners.push({ listener, once: true });
    return this;
  }
  /**
   * This is similar to `.once()`, but returns a Promise that resolves the next time
   * the event is triggered, instead of calling a listener callback.
   * @returns a Promise that resolves the next time given event is triggered,
   * or rejects if an error is emitted.  (If you request the 'error' event,
   * returns a promise that resolves with the error).
   *
   * Example:
   *
   *   const message = await stream.emitted('message') // rejects if the stream errors
   */
  emitted(event) {
    return new Promise((resolve, reject) => {
      __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, true, "f");
      if (event !== "error")
        this.once("error", reject);
      this.once(event, resolve);
    });
  }
  async done() {
    __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, true, "f");
    await __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_endPromise, "f");
  }
  _emit(event, ...args) {
    if (__classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_ended, "f")) {
      return;
    }
    if (event === "end") {
      __classPrivateFieldSet4(this, _AbstractAssistantStreamRunner_ended, true, "f");
      __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_resolveEndPromise, "f").call(this);
    }
    const listeners = __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_listeners, "f")[event];
    if (listeners) {
      __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_listeners, "f")[event] = listeners.filter((l) => !l.once);
      listeners.forEach(({ listener }) => listener(...args));
    }
    if (event === "abort") {
      const error = args[0];
      if (!__classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_rejectEndPromise, "f").call(this, error);
      this._emit("end");
      return;
    }
    if (event === "error") {
      const error = args[0];
      if (!__classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet4(this, _AbstractAssistantStreamRunner_rejectEndPromise, "f").call(this, error);
      this._emit("end");
    }
  }
  async _threadAssistantStream(body, thread, options) {
    return await this._createThreadAssistantStream(thread, body, options);
  }
  async _runAssistantStream(threadId, runs, params, options) {
    return await this._createAssistantStream(runs, threadId, params, options);
  }
  async _runToolAssistantStream(threadId, runId, runs, params, options) {
    return await this._createToolAssistantStream(runs, threadId, runId, params, options);
  }
  async _createThreadAssistantStream(thread, body, options) {
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    const runResult = await thread.createAndRun({ ...body, stream: false }, { ...options, signal: this.controller.signal });
    this._connected();
    return this._addRun(runResult);
  }
  async _createToolAssistantStream(run, threadId, runId, params, options) {
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    const runResult = await run.submitToolOutputs(threadId, runId, { ...params, stream: false }, { ...options, signal: this.controller.signal });
    this._connected();
    return this._addRun(runResult);
  }
  async _createAssistantStream(run, threadId, params, options) {
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    const runResult = await run.create(threadId, { ...params, stream: false }, { ...options, signal: this.controller.signal });
    this._connected();
    return this._addRun(runResult);
  }
};
_AbstractAssistantStreamRunner_connectedPromise = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_resolveConnectedPromise = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_rejectConnectedPromise = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_endPromise = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_resolveEndPromise = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_rejectEndPromise = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_listeners = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_ended = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_errored = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_aborted = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_catchingPromiseCreated = /* @__PURE__ */ new WeakMap(), _AbstractAssistantStreamRunner_handleError = /* @__PURE__ */ new WeakMap();

// node_modules/openai/lib/AssistantStream.mjs
var __classPrivateFieldGet5 = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var __classPrivateFieldSet5 = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var _AssistantStream_instances;
var _AssistantStream_events;
var _AssistantStream_runStepSnapshots;
var _AssistantStream_messageSnapshots;
var _AssistantStream_messageSnapshot;
var _AssistantStream_finalRun;
var _AssistantStream_currentContentIndex;
var _AssistantStream_currentContent;
var _AssistantStream_currentToolCallIndex;
var _AssistantStream_currentToolCall;
var _AssistantStream_currentEvent;
var _AssistantStream_currentRunSnapshot;
var _AssistantStream_currentRunStepSnapshot;
var _AssistantStream_addEvent;
var _AssistantStream_endRequest;
var _AssistantStream_handleMessage;
var _AssistantStream_handleRunStep;
var _AssistantStream_handleEvent;
var _AssistantStream_accumulateRunStep;
var _AssistantStream_accumulateMessage;
var _AssistantStream_accumulateContent;
var _AssistantStream_handleRun;
var AssistantStream = class extends AbstractAssistantStreamRunner {
  constructor() {
    super(...arguments);
    _AssistantStream_instances.add(this);
    _AssistantStream_events.set(this, []);
    _AssistantStream_runStepSnapshots.set(this, {});
    _AssistantStream_messageSnapshots.set(this, {});
    _AssistantStream_messageSnapshot.set(this, void 0);
    _AssistantStream_finalRun.set(this, void 0);
    _AssistantStream_currentContentIndex.set(this, void 0);
    _AssistantStream_currentContent.set(this, void 0);
    _AssistantStream_currentToolCallIndex.set(this, void 0);
    _AssistantStream_currentToolCall.set(this, void 0);
    _AssistantStream_currentEvent.set(this, void 0);
    _AssistantStream_currentRunSnapshot.set(this, void 0);
    _AssistantStream_currentRunStepSnapshot.set(this, void 0);
  }
  [(_AssistantStream_events = /* @__PURE__ */ new WeakMap(), _AssistantStream_runStepSnapshots = /* @__PURE__ */ new WeakMap(), _AssistantStream_messageSnapshots = /* @__PURE__ */ new WeakMap(), _AssistantStream_messageSnapshot = /* @__PURE__ */ new WeakMap(), _AssistantStream_finalRun = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentContentIndex = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentContent = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentToolCallIndex = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentToolCall = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentEvent = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentRunSnapshot = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentRunStepSnapshot = /* @__PURE__ */ new WeakMap(), _AssistantStream_instances = /* @__PURE__ */ new WeakSet(), Symbol.asyncIterator)]() {
    const pushQueue = [];
    const readQueue = [];
    let done = false;
    this.on("event", (event) => {
      const reader = readQueue.shift();
      if (reader) {
        reader.resolve(event);
      } else {
        pushQueue.push(event);
      }
    });
    this.on("end", () => {
      done = true;
      for (const reader of readQueue) {
        reader.resolve(void 0);
      }
      readQueue.length = 0;
    });
    this.on("abort", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    this.on("error", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    return {
      next: async () => {
        if (!pushQueue.length) {
          if (done) {
            return { value: void 0, done: true };
          }
          return new Promise((resolve, reject) => readQueue.push({ resolve, reject })).then((chunk2) => chunk2 ? { value: chunk2, done: false } : { value: void 0, done: true });
        }
        const chunk = pushQueue.shift();
        return { value: chunk, done: false };
      },
      return: async () => {
        this.abort();
        return { value: void 0, done: true };
      }
    };
  }
  static fromReadableStream(stream) {
    const runner = new AssistantStream();
    runner._run(() => runner._fromReadableStream(stream));
    return runner;
  }
  async _fromReadableStream(readableStream, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    this._connected();
    const stream = Stream.fromReadableStream(readableStream, this.controller);
    for await (const event of stream) {
      __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError();
    }
    return this._addRun(__classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
  }
  toReadableStream() {
    const stream = new Stream(this[Symbol.asyncIterator].bind(this), this.controller);
    return stream.toReadableStream();
  }
  static createToolAssistantStream(threadId, runId, runs, body, options) {
    const runner = new AssistantStream();
    runner._run(() => runner._runToolAssistantStream(threadId, runId, runs, body, {
      ...options,
      headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "stream" }
    }));
    return runner;
  }
  async _createToolAssistantStream(run, threadId, runId, params, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    const body = { ...params, stream: true };
    const stream = await run.submitToolOutputs(threadId, runId, body, {
      ...options,
      signal: this.controller.signal
    });
    this._connected();
    for await (const event of stream) {
      __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError();
    }
    return this._addRun(__classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
  }
  static createThreadAssistantStream(body, thread, options) {
    const runner = new AssistantStream();
    runner._run(() => runner._threadAssistantStream(body, thread, {
      ...options,
      headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "stream" }
    }));
    return runner;
  }
  static createAssistantStream(threadId, runs, params, options) {
    const runner = new AssistantStream();
    runner._run(() => runner._runAssistantStream(threadId, runs, params, {
      ...options,
      headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "stream" }
    }));
    return runner;
  }
  currentEvent() {
    return __classPrivateFieldGet5(this, _AssistantStream_currentEvent, "f");
  }
  currentRun() {
    return __classPrivateFieldGet5(this, _AssistantStream_currentRunSnapshot, "f");
  }
  currentMessageSnapshot() {
    return __classPrivateFieldGet5(this, _AssistantStream_messageSnapshot, "f");
  }
  currentRunStepSnapshot() {
    return __classPrivateFieldGet5(this, _AssistantStream_currentRunStepSnapshot, "f");
  }
  async finalRunSteps() {
    await this.done();
    return Object.values(__classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f"));
  }
  async finalMessages() {
    await this.done();
    return Object.values(__classPrivateFieldGet5(this, _AssistantStream_messageSnapshots, "f"));
  }
  async finalRun() {
    await this.done();
    if (!__classPrivateFieldGet5(this, _AssistantStream_finalRun, "f"))
      throw Error("Final run was not received.");
    return __classPrivateFieldGet5(this, _AssistantStream_finalRun, "f");
  }
  async _createThreadAssistantStream(thread, params, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    const body = { ...params, stream: true };
    const stream = await thread.createAndRun(body, { ...options, signal: this.controller.signal });
    this._connected();
    for await (const event of stream) {
      __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError();
    }
    return this._addRun(__classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
  }
  async _createAssistantStream(run, threadId, params, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    const body = { ...params, stream: true };
    const stream = await run.create(threadId, body, { ...options, signal: this.controller.signal });
    this._connected();
    for await (const event of stream) {
      __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError();
    }
    return this._addRun(__classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
  }
  static accumulateDelta(acc, delta) {
    for (const [key, deltaValue] of Object.entries(delta)) {
      if (!acc.hasOwnProperty(key)) {
        acc[key] = deltaValue;
        continue;
      }
      let accValue = acc[key];
      if (accValue === null || accValue === void 0) {
        acc[key] = deltaValue;
        continue;
      }
      if (key === "index" || key === "type") {
        acc[key] = deltaValue;
        continue;
      }
      if (typeof accValue === "string" && typeof deltaValue === "string") {
        accValue += deltaValue;
      } else if (typeof accValue === "number" && typeof deltaValue === "number") {
        accValue += deltaValue;
      } else if (isObj(accValue) && isObj(deltaValue)) {
        accValue = this.accumulateDelta(accValue, deltaValue);
      } else if (Array.isArray(accValue) && Array.isArray(deltaValue)) {
        if (accValue.every((x) => typeof x === "string" || typeof x === "number")) {
          accValue.push(...deltaValue);
          continue;
        }
      } else {
        throw Error(`Unhandled record type: ${key}, deltaValue: ${deltaValue}, accValue: ${accValue}`);
      }
      acc[key] = accValue;
    }
    return acc;
  }
};
_AssistantStream_addEvent = function _AssistantStream_addEvent2(event) {
  if (this.ended)
    return;
  __classPrivateFieldSet5(this, _AssistantStream_currentEvent, event, "f");
  __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_handleEvent).call(this, event);
  switch (event.event) {
    case "thread.created":
      break;
    case "thread.run.created":
    case "thread.run.queued":
    case "thread.run.in_progress":
    case "thread.run.requires_action":
    case "thread.run.completed":
    case "thread.run.failed":
    case "thread.run.cancelling":
    case "thread.run.cancelled":
    case "thread.run.expired":
      __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_handleRun).call(this, event);
      break;
    case "thread.run.step.created":
    case "thread.run.step.in_progress":
    case "thread.run.step.delta":
    case "thread.run.step.completed":
    case "thread.run.step.failed":
    case "thread.run.step.cancelled":
    case "thread.run.step.expired":
      __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_handleRunStep).call(this, event);
      break;
    case "thread.message.created":
    case "thread.message.in_progress":
    case "thread.message.delta":
    case "thread.message.completed":
    case "thread.message.incomplete":
      __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_handleMessage).call(this, event);
      break;
    case "error":
      throw new Error("Encountered an error event in event processing - errors should be processed earlier");
  }
}, _AssistantStream_endRequest = function _AssistantStream_endRequest2() {
  if (this.ended) {
    throw new OpenAIError(`stream has ended, this shouldn't happen`);
  }
  if (!__classPrivateFieldGet5(this, _AssistantStream_finalRun, "f"))
    throw Error("Final run has not been received");
  return __classPrivateFieldGet5(this, _AssistantStream_finalRun, "f");
}, _AssistantStream_handleMessage = function _AssistantStream_handleMessage2(event) {
  const [accumulatedMessage, newContent] = __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_accumulateMessage).call(this, event, __classPrivateFieldGet5(this, _AssistantStream_messageSnapshot, "f"));
  __classPrivateFieldSet5(this, _AssistantStream_messageSnapshot, accumulatedMessage, "f");
  __classPrivateFieldGet5(this, _AssistantStream_messageSnapshots, "f")[accumulatedMessage.id] = accumulatedMessage;
  for (const content of newContent) {
    const snapshotContent = accumulatedMessage.content[content.index];
    if ((snapshotContent == null ? void 0 : snapshotContent.type) == "text") {
      this._emit("textCreated", snapshotContent.text);
    }
  }
  switch (event.event) {
    case "thread.message.created":
      this._emit("messageCreated", event.data);
      break;
    case "thread.message.in_progress":
      break;
    case "thread.message.delta":
      this._emit("messageDelta", event.data.delta, accumulatedMessage);
      if (event.data.delta.content) {
        for (const content of event.data.delta.content) {
          if (content.type == "text" && content.text) {
            let textDelta = content.text;
            let snapshot = accumulatedMessage.content[content.index];
            if (snapshot && snapshot.type == "text") {
              this._emit("textDelta", textDelta, snapshot.text);
            } else {
              throw Error("The snapshot associated with this text delta is not text or missing");
            }
          }
          if (content.index != __classPrivateFieldGet5(this, _AssistantStream_currentContentIndex, "f")) {
            if (__classPrivateFieldGet5(this, _AssistantStream_currentContent, "f")) {
              switch (__classPrivateFieldGet5(this, _AssistantStream_currentContent, "f").type) {
                case "text":
                  this._emit("textDone", __classPrivateFieldGet5(this, _AssistantStream_currentContent, "f").text, __classPrivateFieldGet5(this, _AssistantStream_messageSnapshot, "f"));
                  break;
                case "image_file":
                  this._emit("imageFileDone", __classPrivateFieldGet5(this, _AssistantStream_currentContent, "f").image_file, __classPrivateFieldGet5(this, _AssistantStream_messageSnapshot, "f"));
                  break;
              }
            }
            __classPrivateFieldSet5(this, _AssistantStream_currentContentIndex, content.index, "f");
          }
          __classPrivateFieldSet5(this, _AssistantStream_currentContent, accumulatedMessage.content[content.index], "f");
        }
      }
      break;
    case "thread.message.completed":
    case "thread.message.incomplete":
      if (__classPrivateFieldGet5(this, _AssistantStream_currentContentIndex, "f") !== void 0) {
        const currentContent = event.data.content[__classPrivateFieldGet5(this, _AssistantStream_currentContentIndex, "f")];
        if (currentContent) {
          switch (currentContent.type) {
            case "image_file":
              this._emit("imageFileDone", currentContent.image_file, __classPrivateFieldGet5(this, _AssistantStream_messageSnapshot, "f"));
              break;
            case "text":
              this._emit("textDone", currentContent.text, __classPrivateFieldGet5(this, _AssistantStream_messageSnapshot, "f"));
              break;
          }
        }
      }
      if (__classPrivateFieldGet5(this, _AssistantStream_messageSnapshot, "f")) {
        this._emit("messageDone", event.data);
      }
      __classPrivateFieldSet5(this, _AssistantStream_messageSnapshot, void 0, "f");
  }
}, _AssistantStream_handleRunStep = function _AssistantStream_handleRunStep2(event) {
  const accumulatedRunStep = __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_accumulateRunStep).call(this, event);
  __classPrivateFieldSet5(this, _AssistantStream_currentRunStepSnapshot, accumulatedRunStep, "f");
  switch (event.event) {
    case "thread.run.step.created":
      this._emit("runStepCreated", event.data);
      break;
    case "thread.run.step.delta":
      const delta = event.data.delta;
      if (delta.step_details && delta.step_details.type == "tool_calls" && delta.step_details.tool_calls && accumulatedRunStep.step_details.type == "tool_calls") {
        for (const toolCall of delta.step_details.tool_calls) {
          if (toolCall.index == __classPrivateFieldGet5(this, _AssistantStream_currentToolCallIndex, "f")) {
            this._emit("toolCallDelta", toolCall, accumulatedRunStep.step_details.tool_calls[toolCall.index]);
          } else {
            if (__classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f")) {
              this._emit("toolCallDone", __classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f"));
            }
            __classPrivateFieldSet5(this, _AssistantStream_currentToolCallIndex, toolCall.index, "f");
            __classPrivateFieldSet5(this, _AssistantStream_currentToolCall, accumulatedRunStep.step_details.tool_calls[toolCall.index], "f");
            if (__classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f"))
              this._emit("toolCallCreated", __classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f"));
          }
        }
      }
      this._emit("runStepDelta", event.data.delta, accumulatedRunStep);
      break;
    case "thread.run.step.completed":
    case "thread.run.step.failed":
    case "thread.run.step.cancelled":
    case "thread.run.step.expired":
      __classPrivateFieldSet5(this, _AssistantStream_currentRunStepSnapshot, void 0, "f");
      const details = event.data.step_details;
      if (details.type == "tool_calls") {
        if (__classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f")) {
          this._emit("toolCallDone", __classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f"));
          __classPrivateFieldSet5(this, _AssistantStream_currentToolCall, void 0, "f");
        }
      }
      this._emit("runStepDone", event.data, accumulatedRunStep);
      break;
    case "thread.run.step.in_progress":
      break;
  }
}, _AssistantStream_handleEvent = function _AssistantStream_handleEvent2(event) {
  __classPrivateFieldGet5(this, _AssistantStream_events, "f").push(event);
  this._emit("event", event);
}, _AssistantStream_accumulateRunStep = function _AssistantStream_accumulateRunStep2(event) {
  switch (event.event) {
    case "thread.run.step.created":
      __classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f")[event.data.id] = event.data;
      return event.data;
    case "thread.run.step.delta":
      let snapshot = __classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f")[event.data.id];
      if (!snapshot) {
        throw Error("Received a RunStepDelta before creation of a snapshot");
      }
      let data = event.data;
      if (data.delta) {
        const accumulated = AssistantStream.accumulateDelta(snapshot, data.delta);
        __classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f")[event.data.id] = accumulated;
      }
      return __classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f")[event.data.id];
    case "thread.run.step.completed":
    case "thread.run.step.failed":
    case "thread.run.step.cancelled":
    case "thread.run.step.expired":
    case "thread.run.step.in_progress":
      __classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f")[event.data.id] = event.data;
      break;
  }
  if (__classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f")[event.data.id])
    return __classPrivateFieldGet5(this, _AssistantStream_runStepSnapshots, "f")[event.data.id];
  throw new Error("No snapshot available");
}, _AssistantStream_accumulateMessage = function _AssistantStream_accumulateMessage2(event, snapshot) {
  let newContent = [];
  switch (event.event) {
    case "thread.message.created":
      return [event.data, newContent];
    case "thread.message.delta":
      if (!snapshot) {
        throw Error("Received a delta with no existing snapshot (there should be one from message creation)");
      }
      let data = event.data;
      if (data.delta.content) {
        for (const contentElement of data.delta.content) {
          if (contentElement.index in snapshot.content) {
            let currentContent = snapshot.content[contentElement.index];
            snapshot.content[contentElement.index] = __classPrivateFieldGet5(this, _AssistantStream_instances, "m", _AssistantStream_accumulateContent).call(this, contentElement, currentContent);
          } else {
            snapshot.content[contentElement.index] = contentElement;
            newContent.push(contentElement);
          }
        }
      }
      return [snapshot, newContent];
    case "thread.message.in_progress":
    case "thread.message.completed":
    case "thread.message.incomplete":
      if (snapshot) {
        return [snapshot, newContent];
      } else {
        throw Error("Received thread message event with no existing snapshot");
      }
  }
  throw Error("Tried to accumulate a non-message event");
}, _AssistantStream_accumulateContent = function _AssistantStream_accumulateContent2(contentElement, currentContent) {
  return AssistantStream.accumulateDelta(currentContent, contentElement);
}, _AssistantStream_handleRun = function _AssistantStream_handleRun2(event) {
  __classPrivateFieldSet5(this, _AssistantStream_currentRunSnapshot, event.data, "f");
  switch (event.event) {
    case "thread.run.created":
      break;
    case "thread.run.queued":
      break;
    case "thread.run.in_progress":
      break;
    case "thread.run.requires_action":
    case "thread.run.cancelled":
    case "thread.run.failed":
    case "thread.run.completed":
    case "thread.run.expired":
      __classPrivateFieldSet5(this, _AssistantStream_finalRun, event.data, "f");
      if (__classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f")) {
        this._emit("toolCallDone", __classPrivateFieldGet5(this, _AssistantStream_currentToolCall, "f"));
        __classPrivateFieldSet5(this, _AssistantStream_currentToolCall, void 0, "f");
      }
      break;
    case "thread.run.cancelling":
      break;
  }
};

// node_modules/openai/resources/beta/threads/messages.mjs
var Messages = class extends APIResource {
  /**
   * Create a message.
   */
  create(threadId, body, options) {
    return this._client.post(`/threads/${threadId}/messages`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Retrieve a message.
   */
  retrieve(threadId, messageId, options) {
    return this._client.get(`/threads/${threadId}/messages/${messageId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Modifies a message.
   */
  update(threadId, messageId, body, options) {
    return this._client.post(`/threads/${threadId}/messages/${messageId}`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  list(threadId, query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list(threadId, {}, query);
    }
    return this._client.getAPIList(`/threads/${threadId}/messages`, MessagesPage, {
      query,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Deletes a message.
   */
  del(threadId, messageId, options) {
    return this._client.delete(`/threads/${threadId}/messages/${messageId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
};
var MessagesPage = class extends CursorPage {
};
(function(Messages4) {
  Messages4.MessagesPage = MessagesPage;
})(Messages || (Messages = {}));

// node_modules/openai/resources/beta/threads/runs/steps.mjs
var Steps = class extends APIResource {
  /**
   * Retrieves a run step.
   */
  retrieve(threadId, runId, stepId, options) {
    return this._client.get(`/threads/${threadId}/runs/${runId}/steps/${stepId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  list(threadId, runId, query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list(threadId, runId, {}, query);
    }
    return this._client.getAPIList(`/threads/${threadId}/runs/${runId}/steps`, RunStepsPage, {
      query,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
};
var RunStepsPage = class extends CursorPage {
};
(function(Steps2) {
  Steps2.RunStepsPage = RunStepsPage;
})(Steps || (Steps = {}));

// node_modules/openai/resources/beta/threads/runs/runs.mjs
var Runs = class extends APIResource {
  constructor() {
    super(...arguments);
    this.steps = new Steps(this._client);
  }
  create(threadId, body, options) {
    var _a3;
    return this._client.post(`/threads/${threadId}/runs`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers },
      stream: (_a3 = body.stream) != null ? _a3 : false
    });
  }
  /**
   * Retrieves a run.
   */
  retrieve(threadId, runId, options) {
    return this._client.get(`/threads/${threadId}/runs/${runId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Modifies a run.
   */
  update(threadId, runId, body, options) {
    return this._client.post(`/threads/${threadId}/runs/${runId}`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  list(threadId, query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list(threadId, {}, query);
    }
    return this._client.getAPIList(`/threads/${threadId}/runs`, RunsPage, {
      query,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Cancels a run that is `in_progress`.
   */
  cancel(threadId, runId, options) {
    return this._client.post(`/threads/${threadId}/runs/${runId}/cancel`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * A helper to create a run an poll for a terminal state. More information on Run
   * lifecycles can be found here:
   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
   */
  async createAndPoll(threadId, body, options) {
    const run = await this.create(threadId, body, options);
    return await this.poll(threadId, run.id, options);
  }
  /**
   * Create a Run stream
   *
   * @deprecated use `stream` instead
   */
  createAndStream(threadId, body, options) {
    return AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);
  }
  /**
   * A helper to poll a run status until it reaches a terminal state. More
   * information on Run lifecycles can be found here:
   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
   */
  async poll(threadId, runId, options) {
    const headers = { ...options == null ? void 0 : options.headers, "X-Stainless-Poll-Helper": "true" };
    if (options == null ? void 0 : options.pollIntervalMs) {
      headers["X-Stainless-Custom-Poll-Interval"] = options.pollIntervalMs.toString();
    }
    while (true) {
      const { data: run, response } = await this.retrieve(threadId, runId, {
        ...options,
        headers: { ...options == null ? void 0 : options.headers, ...headers }
      }).withResponse();
      switch (run.status) {
        case "queued":
        case "in_progress":
        case "cancelling":
          let sleepInterval = 5e3;
          if (options == null ? void 0 : options.pollIntervalMs) {
            sleepInterval = options.pollIntervalMs;
          } else {
            const headerInterval = response.headers.get("openai-poll-after-ms");
            if (headerInterval) {
              const headerIntervalMs = parseInt(headerInterval);
              if (!isNaN(headerIntervalMs)) {
                sleepInterval = headerIntervalMs;
              }
            }
          }
          await sleep(sleepInterval);
          break;
        case "requires_action":
        case "incomplete":
        case "cancelled":
        case "completed":
        case "failed":
        case "expired":
          return run;
      }
    }
  }
  /**
   * Create a Run stream
   */
  stream(threadId, body, options) {
    return AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);
  }
  submitToolOutputs(threadId, runId, body, options) {
    var _a3;
    return this._client.post(`/threads/${threadId}/runs/${runId}/submit_tool_outputs`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers },
      stream: (_a3 = body.stream) != null ? _a3 : false
    });
  }
  /**
   * A helper to submit a tool output to a run and poll for a terminal run state.
   * More information on Run lifecycles can be found here:
   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
   */
  async submitToolOutputsAndPoll(threadId, runId, body, options) {
    const run = await this.submitToolOutputs(threadId, runId, body, options);
    return await this.poll(threadId, run.id, options);
  }
  /**
   * Submit the tool outputs from a previous run and stream the run to a terminal
   * state. More information on Run lifecycles can be found here:
   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
   */
  submitToolOutputsStream(threadId, runId, body, options) {
    return AssistantStream.createToolAssistantStream(threadId, runId, this._client.beta.threads.runs, body, options);
  }
};
var RunsPage = class extends CursorPage {
};
(function(Runs2) {
  Runs2.RunsPage = RunsPage;
  Runs2.Steps = Steps;
  Runs2.RunStepsPage = RunStepsPage;
})(Runs || (Runs = {}));

// node_modules/openai/resources/beta/threads/threads.mjs
var Threads = class extends APIResource {
  constructor() {
    super(...arguments);
    this.runs = new Runs(this._client);
    this.messages = new Messages(this._client);
  }
  create(body = {}, options) {
    if (isRequestOptions(body)) {
      return this.create({}, body);
    }
    return this._client.post("/threads", {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Retrieves a thread.
   */
  retrieve(threadId, options) {
    return this._client.get(`/threads/${threadId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Modifies a thread.
   */
  update(threadId, body, options) {
    return this._client.post(`/threads/${threadId}`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Delete a thread.
   */
  del(threadId, options) {
    return this._client.delete(`/threads/${threadId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  createAndRun(body, options) {
    var _a3;
    return this._client.post("/threads/runs", {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers },
      stream: (_a3 = body.stream) != null ? _a3 : false
    });
  }
  /**
   * A helper to create a thread, start a run and then poll for a terminal state.
   * More information on Run lifecycles can be found here:
   * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
   */
  async createAndRunPoll(body, options) {
    const run = await this.createAndRun(body, options);
    return await this.runs.poll(run.thread_id, run.id, options);
  }
  /**
   * Create a thread and stream the run back
   */
  createAndRunStream(body, options) {
    return AssistantStream.createThreadAssistantStream(body, this._client.beta.threads, options);
  }
};
(function(Threads2) {
  Threads2.Runs = Runs;
  Threads2.RunsPage = RunsPage;
  Threads2.Messages = Messages;
  Threads2.MessagesPage = MessagesPage;
})(Threads || (Threads = {}));

// node_modules/openai/lib/Util.mjs
var allSettledWithThrow = async (promises) => {
  const results = await Promise.allSettled(promises);
  const rejected = results.filter((result) => result.status === "rejected");
  if (rejected.length) {
    for (const result of rejected) {
      console.error(result.reason);
    }
    throw new Error(`${rejected.length} promise(s) failed - see the above errors`);
  }
  const values = [];
  for (const result of results) {
    if (result.status === "fulfilled") {
      values.push(result.value);
    }
  }
  return values;
};

// node_modules/openai/resources/beta/vector-stores/files.mjs
var Files = class extends APIResource {
  /**
   * Create a vector store file by attaching a
   * [File](https://platform.openai.com/docs/api-reference/files) to a
   * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object).
   */
  create(vectorStoreId, body, options) {
    return this._client.post(`/vector_stores/${vectorStoreId}/files`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Retrieves a vector store file.
   */
  retrieve(vectorStoreId, fileId, options) {
    return this._client.get(`/vector_stores/${vectorStoreId}/files/${fileId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  list(vectorStoreId, query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list(vectorStoreId, {}, query);
    }
    return this._client.getAPIList(`/vector_stores/${vectorStoreId}/files`, VectorStoreFilesPage, {
      query,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Delete a vector store file. This will remove the file from the vector store but
   * the file itself will not be deleted. To delete the file, use the
   * [delete file](https://platform.openai.com/docs/api-reference/files/delete)
   * endpoint.
   */
  del(vectorStoreId, fileId, options) {
    return this._client.delete(`/vector_stores/${vectorStoreId}/files/${fileId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Attach a file to the given vector store and wait for it to be processed.
   */
  async createAndPoll(vectorStoreId, body, options) {
    const file = await this.create(vectorStoreId, body, options);
    return await this.poll(vectorStoreId, file.id, options);
  }
  /**
   * Wait for the vector store file to finish processing.
   *
   * Note: this will return even if the file failed to process, you need to check
   * file.last_error and file.status to handle these cases
   */
  async poll(vectorStoreId, fileId, options) {
    const headers = { ...options == null ? void 0 : options.headers, "X-Stainless-Poll-Helper": "true" };
    if (options == null ? void 0 : options.pollIntervalMs) {
      headers["X-Stainless-Custom-Poll-Interval"] = options.pollIntervalMs.toString();
    }
    while (true) {
      const fileResponse = await this.retrieve(vectorStoreId, fileId, {
        ...options,
        headers
      }).withResponse();
      const file = fileResponse.data;
      switch (file.status) {
        case "in_progress":
          let sleepInterval = 5e3;
          if (options == null ? void 0 : options.pollIntervalMs) {
            sleepInterval = options.pollIntervalMs;
          } else {
            const headerInterval = fileResponse.response.headers.get("openai-poll-after-ms");
            if (headerInterval) {
              const headerIntervalMs = parseInt(headerInterval);
              if (!isNaN(headerIntervalMs)) {
                sleepInterval = headerIntervalMs;
              }
            }
          }
          await sleep(sleepInterval);
          break;
        case "failed":
        case "completed":
          return file;
      }
    }
  }
  /**
   * Upload a file to the `files` API and then attach it to the given vector store.
   *
   * Note the file will be asynchronously processed (you can use the alternative
   * polling helper method to wait for processing to complete).
   */
  async upload(vectorStoreId, file, options) {
    const fileInfo = await this._client.files.create({ file, purpose: "assistants" }, options);
    return this.create(vectorStoreId, { file_id: fileInfo.id }, options);
  }
  /**
   * Add a file to a vector store and poll until processing is complete.
   */
  async uploadAndPoll(vectorStoreId, file, options) {
    const fileInfo = await this.upload(vectorStoreId, file, options);
    return await this.poll(vectorStoreId, fileInfo.id, options);
  }
};
var VectorStoreFilesPage = class extends CursorPage {
};
(function(Files3) {
  Files3.VectorStoreFilesPage = VectorStoreFilesPage;
})(Files || (Files = {}));

// node_modules/openai/resources/beta/vector-stores/file-batches.mjs
var FileBatches = class extends APIResource {
  /**
   * Create a vector store file batch.
   */
  create(vectorStoreId, body, options) {
    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Retrieves a vector store file batch.
   */
  retrieve(vectorStoreId, batchId, options) {
    return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Cancel a vector store file batch. This attempts to cancel the processing of
   * files in this batch as soon as possible.
   */
  cancel(vectorStoreId, batchId, options) {
    return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Create a vector store batch and poll until all files have been processed.
   */
  async createAndPoll(vectorStoreId, body, options) {
    const batch = await this.create(vectorStoreId, body);
    return await this.poll(vectorStoreId, batch.id, options);
  }
  listFiles(vectorStoreId, batchId, query = {}, options) {
    if (isRequestOptions(query)) {
      return this.listFiles(vectorStoreId, batchId, {}, query);
    }
    return this._client.getAPIList(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`, VectorStoreFilesPage, { query, ...options, headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers } });
  }
  /**
   * Wait for the given file batch to be processed.
   *
   * Note: this will return even if one of the files failed to process, you need to
   * check batch.file_counts.failed_count to handle this case.
   */
  async poll(vectorStoreId, batchId, options) {
    const headers = { ...options == null ? void 0 : options.headers, "X-Stainless-Poll-Helper": "true" };
    if (options == null ? void 0 : options.pollIntervalMs) {
      headers["X-Stainless-Custom-Poll-Interval"] = options.pollIntervalMs.toString();
    }
    while (true) {
      const { data: batch, response } = await this.retrieve(vectorStoreId, batchId, {
        ...options,
        headers
      }).withResponse();
      switch (batch.status) {
        case "in_progress":
          let sleepInterval = 5e3;
          if (options == null ? void 0 : options.pollIntervalMs) {
            sleepInterval = options.pollIntervalMs;
          } else {
            const headerInterval = response.headers.get("openai-poll-after-ms");
            if (headerInterval) {
              const headerIntervalMs = parseInt(headerInterval);
              if (!isNaN(headerIntervalMs)) {
                sleepInterval = headerIntervalMs;
              }
            }
          }
          await sleep(sleepInterval);
          break;
        case "failed":
        case "cancelled":
        case "completed":
          return batch;
      }
    }
  }
  /**
   * Uploads the given files concurrently and then creates a vector store file batch.
   *
   * The concurrency limit is configurable using the `maxConcurrency` parameter.
   */
  async uploadAndPoll(vectorStoreId, { files, fileIds = [] }, options) {
    var _a3;
    if (files == null || files.length == 0) {
      throw new Error(`No \`files\` provided to process. If you've already uploaded files you should use \`.createAndPoll()\` instead`);
    }
    const configuredConcurrency = (_a3 = options == null ? void 0 : options.maxConcurrency) != null ? _a3 : 5;
    const concurrencyLimit = Math.min(configuredConcurrency, files.length);
    const client = this._client;
    const fileIterator = files.values();
    const allFileIds = [...fileIds];
    async function processFiles(iterator) {
      for (let item of iterator) {
        const fileObj = await client.files.create({ file: item, purpose: "assistants" }, options);
        allFileIds.push(fileObj.id);
      }
    }
    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);
    await allSettledWithThrow(workers);
    return await this.createAndPoll(vectorStoreId, {
      file_ids: allFileIds
    });
  }
};
(function(FileBatches2) {
})(FileBatches || (FileBatches = {}));

// node_modules/openai/resources/beta/vector-stores/vector-stores.mjs
var VectorStores = class extends APIResource {
  constructor() {
    super(...arguments);
    this.files = new Files(this._client);
    this.fileBatches = new FileBatches(this._client);
  }
  /**
   * Create a vector store.
   */
  create(body, options) {
    return this._client.post("/vector_stores", {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Retrieves a vector store.
   */
  retrieve(vectorStoreId, options) {
    return this._client.get(`/vector_stores/${vectorStoreId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Modifies a vector store.
   */
  update(vectorStoreId, body, options) {
    return this._client.post(`/vector_stores/${vectorStoreId}`, {
      body,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  list(query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list({}, query);
    }
    return this._client.getAPIList("/vector_stores", VectorStoresPage, {
      query,
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Delete a vector store.
   */
  del(vectorStoreId, options) {
    return this._client.delete(`/vector_stores/${vectorStoreId}`, {
      ...options,
      headers: { "OpenAI-Beta": "assistants=v2", ...options == null ? void 0 : options.headers }
    });
  }
};
var VectorStoresPage = class extends CursorPage {
};
(function(VectorStores2) {
  VectorStores2.VectorStoresPage = VectorStoresPage;
  VectorStores2.Files = Files;
  VectorStores2.VectorStoreFilesPage = VectorStoreFilesPage;
  VectorStores2.FileBatches = FileBatches;
})(VectorStores || (VectorStores = {}));

// node_modules/openai/resources/beta/beta.mjs
var Beta = class extends APIResource {
  constructor() {
    super(...arguments);
    this.vectorStores = new VectorStores(this._client);
    this.chat = new Chat2(this._client);
    this.assistants = new Assistants2(this._client);
    this.threads = new Threads(this._client);
  }
};
(function(Beta3) {
  Beta3.VectorStores = VectorStores;
  Beta3.VectorStoresPage = VectorStoresPage;
  Beta3.Chat = Chat2;
  Beta3.Assistants = Assistants2;
  Beta3.AssistantsPage = AssistantsPage;
  Beta3.Threads = Threads;
})(Beta || (Beta = {}));

// node_modules/openai/resources/completions.mjs
var Completions3 = class extends APIResource {
  create(body, options) {
    var _a3;
    return this._client.post("/completions", { body, ...options, stream: (_a3 = body.stream) != null ? _a3 : false });
  }
};
(function(Completions5) {
})(Completions3 || (Completions3 = {}));

// node_modules/openai/resources/embeddings.mjs
var Embeddings = class extends APIResource {
  /**
   * Creates an embedding vector representing the input text.
   */
  create(body, options) {
    return this._client.post("/embeddings", { body, ...options });
  }
};
(function(Embeddings2) {
})(Embeddings || (Embeddings = {}));

// node_modules/openai/resources/files.mjs
var Files2 = class extends APIResource {
  /**
   * Upload a file that can be used across various endpoints. Individual files can be
   * up to 512 MB, and the size of all files uploaded by one organization can be up
   * to 100 GB.
   *
   * The Assistants API supports files up to 2 million tokens and of specific file
   * types. See the
   * [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools) for
   * details.
   *
   * The Fine-tuning API only supports `.jsonl` files. The input also has certain
   * required formats for fine-tuning
   * [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input) or
   * [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)
   * models.
   *
   * The Batch API only supports `.jsonl` files up to 100 MB in size. The input also
   * has a specific required
   * [format](https://platform.openai.com/docs/api-reference/batch/request-input).
   *
   * Please [contact us](https://help.openai.com/) if you need to increase these
   * storage limits.
   */
  create(body, options) {
    return this._client.post("/files", multipartFormRequestOptions({ body, ...options }));
  }
  /**
   * Returns information about a specific file.
   */
  retrieve(fileId, options) {
    return this._client.get(`/files/${fileId}`, options);
  }
  list(query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list({}, query);
    }
    return this._client.getAPIList("/files", FileObjectsPage, { query, ...options });
  }
  /**
   * Delete a file.
   */
  del(fileId, options) {
    return this._client.delete(`/files/${fileId}`, options);
  }
  /**
   * Returns the contents of the specified file.
   */
  content(fileId, options) {
    return this._client.get(`/files/${fileId}/content`, { ...options, __binaryResponse: true });
  }
  /**
   * Returns the contents of the specified file.
   *
   * @deprecated The `.content()` method should be used instead
   */
  retrieveContent(fileId, options) {
    return this._client.get(`/files/${fileId}/content`, {
      ...options,
      headers: { Accept: "application/json", ...options == null ? void 0 : options.headers }
    });
  }
  /**
   * Waits for the given file to be processed, default timeout is 30 mins.
   */
  async waitForProcessing(id, { pollInterval = 5e3, maxWait = 30 * 60 * 1e3 } = {}) {
    const TERMINAL_STATES = /* @__PURE__ */ new Set(["processed", "error", "deleted"]);
    const start = Date.now();
    let file = await this.retrieve(id);
    while (!file.status || !TERMINAL_STATES.has(file.status)) {
      await sleep(pollInterval);
      file = await this.retrieve(id);
      if (Date.now() - start > maxWait) {
        throw new APIConnectionTimeoutError({
          message: `Giving up on waiting for file ${id} to finish processing after ${maxWait} milliseconds.`
        });
      }
    }
    return file;
  }
};
var FileObjectsPage = class extends Page {
};
(function(Files3) {
  Files3.FileObjectsPage = FileObjectsPage;
})(Files2 || (Files2 = {}));

// node_modules/openai/resources/fine-tuning/jobs/checkpoints.mjs
var Checkpoints = class extends APIResource {
  list(fineTuningJobId, query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list(fineTuningJobId, {}, query);
    }
    return this._client.getAPIList(`/fine_tuning/jobs/${fineTuningJobId}/checkpoints`, FineTuningJobCheckpointsPage, { query, ...options });
  }
};
var FineTuningJobCheckpointsPage = class extends CursorPage {
};
(function(Checkpoints2) {
  Checkpoints2.FineTuningJobCheckpointsPage = FineTuningJobCheckpointsPage;
})(Checkpoints || (Checkpoints = {}));

// node_modules/openai/resources/fine-tuning/jobs/jobs.mjs
var Jobs = class extends APIResource {
  constructor() {
    super(...arguments);
    this.checkpoints = new Checkpoints(this._client);
  }
  /**
   * Creates a fine-tuning job which begins the process of creating a new model from
   * a given dataset.
   *
   * Response includes details of the enqueued job including job status and the name
   * of the fine-tuned models once complete.
   *
   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)
   */
  create(body, options) {
    return this._client.post("/fine_tuning/jobs", { body, ...options });
  }
  /**
   * Get info about a fine-tuning job.
   *
   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)
   */
  retrieve(fineTuningJobId, options) {
    return this._client.get(`/fine_tuning/jobs/${fineTuningJobId}`, options);
  }
  list(query = {}, options) {
    if (isRequestOptions(query)) {
      return this.list({}, query);
    }
    return this._client.getAPIList("/fine_tuning/jobs", FineTuningJobsPage, { query, ...options });
  }
  /**
   * Immediately cancel a fine-tune job.
   */
  cancel(fineTuningJobId, options) {
    return this._client.post(`/fine_tuning/jobs/${fineTuningJobId}/cancel`, options);
  }
  listEvents(fineTuningJobId, query = {}, options) {
    if (isRequestOptions(query)) {
      return this.listEvents(fineTuningJobId, {}, query);
    }
    return this._client.getAPIList(`/fine_tuning/jobs/${fineTuningJobId}/events`, FineTuningJobEventsPage, {
      query,
      ...options
    });
  }
};
var FineTuningJobsPage = class extends CursorPage {
};
var FineTuningJobEventsPage = class extends CursorPage {
};
(function(Jobs2) {
  Jobs2.FineTuningJobsPage = FineTuningJobsPage;
  Jobs2.FineTuningJobEventsPage = FineTuningJobEventsPage;
  Jobs2.Checkpoints = Checkpoints;
  Jobs2.FineTuningJobCheckpointsPage = FineTuningJobCheckpointsPage;
})(Jobs || (Jobs = {}));

// node_modules/openai/resources/fine-tuning/fine-tuning.mjs
var FineTuning = class extends APIResource {
  constructor() {
    super(...arguments);
    this.jobs = new Jobs(this._client);
  }
};
(function(FineTuning2) {
  FineTuning2.Jobs = Jobs;
  FineTuning2.FineTuningJobsPage = FineTuningJobsPage;
  FineTuning2.FineTuningJobEventsPage = FineTuningJobEventsPage;
})(FineTuning || (FineTuning = {}));

// node_modules/openai/resources/images.mjs
var Images = class extends APIResource {
  /**
   * Creates a variation of a given image.
   */
  createVariation(body, options) {
    return this._client.post("/images/variations", multipartFormRequestOptions({ body, ...options }));
  }
  /**
   * Creates an edited or extended image given an original image and a prompt.
   */
  edit(body, options) {
    return this._client.post("/images/edits", multipartFormRequestOptions({ body, ...options }));
  }
  /**
   * Creates an image given a prompt.
   */
  generate(body, options) {
    return this._client.post("/images/generations", { body, ...options });
  }
};
(function(Images2) {
})(Images || (Images = {}));

// node_modules/openai/resources/models.mjs
var Models = class extends APIResource {
  /**
   * Retrieves a model instance, providing basic information about the model such as
   * the owner and permissioning.
   */
  retrieve(model, options) {
    return this._client.get(`/models/${model}`, options);
  }
  /**
   * Lists the currently available models, and provides basic information about each
   * one such as the owner and availability.
   */
  list(options) {
    return this._client.getAPIList("/models", ModelsPage, options);
  }
  /**
   * Delete a fine-tuned model. You must have the Owner role in your organization to
   * delete a model.
   */
  del(model, options) {
    return this._client.delete(`/models/${model}`, options);
  }
};
var ModelsPage = class extends Page {
};
(function(Models2) {
  Models2.ModelsPage = ModelsPage;
})(Models || (Models = {}));

// node_modules/openai/resources/moderations.mjs
var Moderations = class extends APIResource {
  /**
   * Classifies if text is potentially harmful.
   */
  create(body, options) {
    return this._client.post("/moderations", { body, ...options });
  }
};
(function(Moderations2) {
})(Moderations || (Moderations = {}));

// node_modules/openai/resources/uploads/parts.mjs
var Parts = class extends APIResource {
  /**
   * Adds a
   * [Part](https://platform.openai.com/docs/api-reference/uploads/part-object) to an
   * [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object.
   * A Part represents a chunk of bytes from the file you are trying to upload.
   *
   * Each Part can be at most 64 MB, and you can add Parts until you hit the Upload
   * maximum of 8 GB.
   *
   * It is possible to add multiple Parts in parallel. You can decide the intended
   * order of the Parts when you
   * [complete the Upload](https://platform.openai.com/docs/api-reference/uploads/complete).
   */
  create(uploadId, body, options) {
    return this._client.post(`/uploads/${uploadId}/parts`, multipartFormRequestOptions({ body, ...options }));
  }
};
(function(Parts2) {
})(Parts || (Parts = {}));

// node_modules/openai/resources/uploads/uploads.mjs
var Uploads = class extends APIResource {
  constructor() {
    super(...arguments);
    this.parts = new Parts(this._client);
  }
  /**
   * Creates an intermediate
   * [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object
   * that you can add
   * [Parts](https://platform.openai.com/docs/api-reference/uploads/part-object) to.
   * Currently, an Upload can accept at most 8 GB in total and expires after an hour
   * after you create it.
   *
   * Once you complete the Upload, we will create a
   * [File](https://platform.openai.com/docs/api-reference/files/object) object that
   * contains all the parts you uploaded. This File is usable in the rest of our
   * platform as a regular File object.
   *
   * For certain `purpose`s, the correct `mime_type` must be specified. Please refer
   * to documentation for the supported MIME types for your use case:
   *
   * - [Assistants](https://platform.openai.com/docs/assistants/tools/file-search/supported-files)
   *
   * For guidance on the proper filename extensions for each purpose, please follow
   * the documentation on
   * [creating a File](https://platform.openai.com/docs/api-reference/files/create).
   */
  create(body, options) {
    return this._client.post("/uploads", { body, ...options });
  }
  /**
   * Cancels the Upload. No Parts may be added after an Upload is cancelled.
   */
  cancel(uploadId, options) {
    return this._client.post(`/uploads/${uploadId}/cancel`, options);
  }
  /**
   * Completes the
   * [Upload](https://platform.openai.com/docs/api-reference/uploads/object).
   *
   * Within the returned Upload object, there is a nested
   * [File](https://platform.openai.com/docs/api-reference/files/object) object that
   * is ready to use in the rest of the platform.
   *
   * You can specify the order of the Parts by passing in an ordered list of the Part
   * IDs.
   *
   * The number of bytes uploaded upon completion must match the number of bytes
   * initially specified when creating the Upload object. No Parts may be added after
   * an Upload is completed.
   */
  complete(uploadId, body, options) {
    return this._client.post(`/uploads/${uploadId}/complete`, { body, ...options });
  }
};
(function(Uploads2) {
  Uploads2.Parts = Parts;
})(Uploads || (Uploads = {}));

// node_modules/openai/index.mjs
var _a;
var OpenAI = class extends APIClient {
  /**
   * API Client for interfacing with the OpenAI API.
   *
   * @param {string | undefined} [opts.apiKey=process.env['OPENAI_API_KEY'] ?? undefined]
   * @param {string | null | undefined} [opts.organization=process.env['OPENAI_ORG_ID'] ?? null]
   * @param {string | null | undefined} [opts.project=process.env['OPENAI_PROJECT_ID'] ?? null]
   * @param {string} [opts.baseURL=process.env['OPENAI_BASE_URL'] ?? https://api.openai.com/v1] - Override the default base URL for the API.
   * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.
   * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.
   * @param {Core.Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.
   * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.
   * @param {Core.Headers} opts.defaultHeaders - Default headers to include with every request to the API.
   * @param {Core.DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.
   * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.
   */
  constructor({ baseURL = readEnv("OPENAI_BASE_URL"), apiKey = readEnv("OPENAI_API_KEY"), organization = ((_a3) => (_a3 = readEnv("OPENAI_ORG_ID")) != null ? _a3 : null)(), project = ((_b) => (_b = readEnv("OPENAI_PROJECT_ID")) != null ? _b : null)(), ...opts } = {}) {
    var _a4;
    if (apiKey === void 0) {
      throw new OpenAIError("The OPENAI_API_KEY environment variable is missing or empty; either provide it, or instantiate the OpenAI client with an apiKey option, like new OpenAI({ apiKey: 'My API Key' }).");
    }
    const options = {
      apiKey,
      organization,
      project,
      ...opts,
      baseURL: baseURL || `https://api.openai.com/v1`
    };
    if (!options.dangerouslyAllowBrowser && isRunningInBrowser()) {
      throw new OpenAIError("It looks like you're running in a browser-like environment.\n\nThis is disabled by default, as it risks exposing your secret API credentials to attackers.\nIf you understand the risks and have appropriate mitigations in place,\nyou can set the `dangerouslyAllowBrowser` option to `true`, e.g.,\n\nnew OpenAI({ apiKey, dangerouslyAllowBrowser: true });\n\nhttps://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n");
    }
    super({
      baseURL: options.baseURL,
      timeout: (_a4 = options.timeout) != null ? _a4 : 6e5,
      httpAgent: options.httpAgent,
      maxRetries: options.maxRetries,
      fetch: options.fetch
    });
    this.completions = new Completions3(this);
    this.chat = new Chat(this);
    this.embeddings = new Embeddings(this);
    this.files = new Files2(this);
    this.images = new Images(this);
    this.audio = new Audio(this);
    this.moderations = new Moderations(this);
    this.models = new Models(this);
    this.fineTuning = new FineTuning(this);
    this.beta = new Beta(this);
    this.batches = new Batches(this);
    this.uploads = new Uploads(this);
    this._options = options;
    this.apiKey = apiKey;
    this.organization = organization;
    this.project = project;
  }
  defaultQuery() {
    return this._options.defaultQuery;
  }
  defaultHeaders(opts) {
    return {
      ...super.defaultHeaders(opts),
      "OpenAI-Organization": this.organization,
      "OpenAI-Project": this.project,
      ...this._options.defaultHeaders
    };
  }
  authHeaders(opts) {
    return { Authorization: `Bearer ${this.apiKey}` };
  }
};
_a = OpenAI;
OpenAI.OpenAI = _a;
OpenAI.OpenAIError = OpenAIError;
OpenAI.APIError = APIError;
OpenAI.APIConnectionError = APIConnectionError;
OpenAI.APIConnectionTimeoutError = APIConnectionTimeoutError;
OpenAI.APIUserAbortError = APIUserAbortError;
OpenAI.NotFoundError = NotFoundError;
OpenAI.ConflictError = ConflictError;
OpenAI.RateLimitError = RateLimitError;
OpenAI.BadRequestError = BadRequestError;
OpenAI.AuthenticationError = AuthenticationError;
OpenAI.InternalServerError = InternalServerError;
OpenAI.PermissionDeniedError = PermissionDeniedError;
OpenAI.UnprocessableEntityError = UnprocessableEntityError;
OpenAI.toFile = toFile;
OpenAI.fileFromPath = fileFromPath;
var { OpenAIError: OpenAIError2, APIError: APIError2, APIConnectionError: APIConnectionError2, APIConnectionTimeoutError: APIConnectionTimeoutError2, APIUserAbortError: APIUserAbortError2, NotFoundError: NotFoundError2, ConflictError: ConflictError2, RateLimitError: RateLimitError2, BadRequestError: BadRequestError2, AuthenticationError: AuthenticationError2, InternalServerError: InternalServerError2, PermissionDeniedError: PermissionDeniedError2, UnprocessableEntityError: UnprocessableEntityError2 } = error_exports;
var toFile2 = toFile;
(function(OpenAI2) {
  OpenAI2.Page = Page;
  OpenAI2.CursorPage = CursorPage;
  OpenAI2.Completions = Completions3;
  OpenAI2.Chat = Chat;
  OpenAI2.Embeddings = Embeddings;
  OpenAI2.Files = Files2;
  OpenAI2.FileObjectsPage = FileObjectsPage;
  OpenAI2.Images = Images;
  OpenAI2.Audio = Audio;
  OpenAI2.Moderations = Moderations;
  OpenAI2.Models = Models;
  OpenAI2.ModelsPage = ModelsPage;
  OpenAI2.FineTuning = FineTuning;
  OpenAI2.Beta = Beta;
  OpenAI2.Batches = Batches;
  OpenAI2.BatchesPage = BatchesPage;
  OpenAI2.Uploads = Uploads;
})(OpenAI || (OpenAI = {}));
var openai_default = OpenAI;

// node_modules/@anthropic-ai/sdk/error.mjs
var error_exports2 = {};
__export(error_exports2, {
  APIConnectionError: () => APIConnectionError3,
  APIConnectionTimeoutError: () => APIConnectionTimeoutError3,
  APIError: () => APIError3,
  APIUserAbortError: () => APIUserAbortError3,
  AnthropicError: () => AnthropicError,
  AuthenticationError: () => AuthenticationError3,
  BadRequestError: () => BadRequestError3,
  ConflictError: () => ConflictError3,
  InternalServerError: () => InternalServerError3,
  NotFoundError: () => NotFoundError3,
  PermissionDeniedError: () => PermissionDeniedError3,
  RateLimitError: () => RateLimitError3,
  UnprocessableEntityError: () => UnprocessableEntityError3
});

// node_modules/@anthropic-ai/sdk/version.mjs
var VERSION2 = "0.27.3";

// node_modules/@anthropic-ai/sdk/_shims/registry.mjs
var auto2 = false;
var kind2 = void 0;
var fetch3 = void 0;
var Request3 = void 0;
var Response3 = void 0;
var Headers3 = void 0;
var FormData3 = void 0;
var Blob3 = void 0;
var File3 = void 0;
var ReadableStream3 = void 0;
var getMultipartRequestOptions2 = void 0;
var getDefaultAgent2 = void 0;
var fileFromPath2 = void 0;
var isFsReadStream2 = void 0;
function setShims2(shims, options = { auto: false }) {
  if (auto2) {
    throw new Error(`you must \`import '@anthropic-ai/sdk/shims/${shims.kind}'\` before importing anything else from @anthropic-ai/sdk`);
  }
  if (kind2) {
    throw new Error(`can't \`import '@anthropic-ai/sdk/shims/${shims.kind}'\` after \`import '@anthropic-ai/sdk/shims/${kind2}'\``);
  }
  auto2 = options.auto;
  kind2 = shims.kind;
  fetch3 = shims.fetch;
  Request3 = shims.Request;
  Response3 = shims.Response;
  Headers3 = shims.Headers;
  FormData3 = shims.FormData;
  Blob3 = shims.Blob;
  File3 = shims.File;
  ReadableStream3 = shims.ReadableStream;
  getMultipartRequestOptions2 = shims.getMultipartRequestOptions;
  getDefaultAgent2 = shims.getDefaultAgent;
  fileFromPath2 = shims.fileFromPath;
  isFsReadStream2 = shims.isFsReadStream;
}

// node_modules/@anthropic-ai/sdk/_shims/MultipartBody.mjs
var MultipartBody2 = class {
  constructor(body) {
    this.body = body;
  }
  get [Symbol.toStringTag]() {
    return "MultipartBody";
  }
};

// node_modules/@anthropic-ai/sdk/_shims/web-runtime.mjs
function getRuntime2({ manuallyImported } = {}) {
  const recommendation = manuallyImported ? `You may need to use polyfills` : `Add one of these imports before your first \`import \u2026 from '@anthropic-ai/sdk'\`:
- \`import '@anthropic-ai/sdk/shims/node'\` (if you're running on Node)
- \`import '@anthropic-ai/sdk/shims/web'\` (otherwise)
`;
  let _fetch, _Request, _Response, _Headers;
  try {
    _fetch = fetch;
    _Request = Request;
    _Response = Response;
    _Headers = Headers;
  } catch (error) {
    throw new Error(`this environment is missing the following Web Fetch API type: ${error.message}. ${recommendation}`);
  }
  return {
    kind: "web",
    fetch: _fetch,
    Request: _Request,
    Response: _Response,
    Headers: _Headers,
    FormData: (
      // @ts-ignore
      typeof FormData !== "undefined" ? FormData : class FormData {
        // @ts-ignore
        constructor() {
          throw new Error(`file uploads aren't supported in this environment yet as 'FormData' is undefined. ${recommendation}`);
        }
      }
    ),
    Blob: typeof Blob !== "undefined" ? Blob : class Blob {
      constructor() {
        throw new Error(`file uploads aren't supported in this environment yet as 'Blob' is undefined. ${recommendation}`);
      }
    },
    File: (
      // @ts-ignore
      typeof File !== "undefined" ? File : class File {
        // @ts-ignore
        constructor() {
          throw new Error(`file uploads aren't supported in this environment yet as 'File' is undefined. ${recommendation}`);
        }
      }
    ),
    ReadableStream: (
      // @ts-ignore
      typeof ReadableStream !== "undefined" ? ReadableStream : class ReadableStream {
        // @ts-ignore
        constructor() {
          throw new Error(`streaming isn't supported in this environment yet as 'ReadableStream' is undefined. ${recommendation}`);
        }
      }
    ),
    getMultipartRequestOptions: async (form, opts) => ({
      ...opts,
      body: new MultipartBody2(form)
    }),
    getDefaultAgent: (url) => void 0,
    fileFromPath: () => {
      throw new Error("The `fileFromPath` function is only supported in Node. See the README for more details: https://www.github.com/anthropics/anthropic-sdk-typescript#file-uploads");
    },
    isFsReadStream: (value) => false
  };
}

// node_modules/@anthropic-ai/sdk/_shims/index.mjs
if (!kind2)
  setShims2(getRuntime2(), { auto: true });

// node_modules/@anthropic-ai/sdk/streaming.mjs
var Stream2 = class {
  constructor(iterator, controller) {
    this.iterator = iterator;
    this.controller = controller;
  }
  static fromSSEResponse(response, controller) {
    let consumed = false;
    async function* iterator() {
      if (consumed) {
        throw new Error("Cannot iterate over a consumed stream, use `.tee()` to split the stream.");
      }
      consumed = true;
      let done = false;
      try {
        for await (const sse of _iterSSEMessages2(response, controller)) {
          if (sse.event === "completion") {
            try {
              yield JSON.parse(sse.data);
            } catch (e) {
              console.error(`Could not parse message into JSON:`, sse.data);
              console.error(`From chunk:`, sse.raw);
              throw e;
            }
          }
          if (sse.event === "message_start" || sse.event === "message_delta" || sse.event === "message_stop" || sse.event === "content_block_start" || sse.event === "content_block_delta" || sse.event === "content_block_stop") {
            try {
              yield JSON.parse(sse.data);
            } catch (e) {
              console.error(`Could not parse message into JSON:`, sse.data);
              console.error(`From chunk:`, sse.raw);
              throw e;
            }
          }
          if (sse.event === "ping") {
            continue;
          }
          if (sse.event === "error") {
            throw APIError3.generate(void 0, `SSE Error: ${sse.data}`, sse.data, createResponseHeaders2(response.headers));
          }
        }
        done = true;
      } catch (e) {
        if (e instanceof Error && e.name === "AbortError")
          return;
        throw e;
      } finally {
        if (!done)
          controller.abort();
      }
    }
    return new Stream2(iterator, controller);
  }
  /**
   * Generates a Stream from a newline-separated ReadableStream
   * where each item is a JSON value.
   */
  static fromReadableStream(readableStream, controller) {
    let consumed = false;
    async function* iterLines() {
      const lineDecoder = new LineDecoder2();
      const iter = readableStreamAsyncIterable2(readableStream);
      for await (const chunk of iter) {
        for (const line of lineDecoder.decode(chunk)) {
          yield line;
        }
      }
      for (const line of lineDecoder.flush()) {
        yield line;
      }
    }
    async function* iterator() {
      if (consumed) {
        throw new Error("Cannot iterate over a consumed stream, use `.tee()` to split the stream.");
      }
      consumed = true;
      let done = false;
      try {
        for await (const line of iterLines()) {
          if (done)
            continue;
          if (line)
            yield JSON.parse(line);
        }
        done = true;
      } catch (e) {
        if (e instanceof Error && e.name === "AbortError")
          return;
        throw e;
      } finally {
        if (!done)
          controller.abort();
      }
    }
    return new Stream2(iterator, controller);
  }
  [Symbol.asyncIterator]() {
    return this.iterator();
  }
  /**
   * Splits the stream into two streams which can be
   * independently read from at different speeds.
   */
  tee() {
    const left = [];
    const right = [];
    const iterator = this.iterator();
    const teeIterator = (queue) => {
      return {
        next: () => {
          if (queue.length === 0) {
            const result = iterator.next();
            left.push(result);
            right.push(result);
          }
          return queue.shift();
        }
      };
    };
    return [
      new Stream2(() => teeIterator(left), this.controller),
      new Stream2(() => teeIterator(right), this.controller)
    ];
  }
  /**
   * Converts this stream to a newline-separated ReadableStream of
   * JSON stringified values in the stream
   * which can be turned back into a Stream with `Stream.fromReadableStream()`.
   */
  toReadableStream() {
    const self = this;
    let iter;
    const encoder = new TextEncoder();
    return new ReadableStream3({
      async start() {
        iter = self[Symbol.asyncIterator]();
      },
      async pull(ctrl) {
        try {
          const { value, done } = await iter.next();
          if (done)
            return ctrl.close();
          const bytes = encoder.encode(JSON.stringify(value) + "\n");
          ctrl.enqueue(bytes);
        } catch (err) {
          ctrl.error(err);
        }
      },
      async cancel() {
        var _a3;
        await ((_a3 = iter.return) == null ? void 0 : _a3.call(iter));
      }
    });
  }
};
async function* _iterSSEMessages2(response, controller) {
  if (!response.body) {
    controller.abort();
    throw new AnthropicError(`Attempted to iterate over a response with no body`);
  }
  const sseDecoder = new SSEDecoder2();
  const lineDecoder = new LineDecoder2();
  const iter = readableStreamAsyncIterable2(response.body);
  for await (const sseChunk of iterSSEChunks2(iter)) {
    for (const line of lineDecoder.decode(sseChunk)) {
      const sse = sseDecoder.decode(line);
      if (sse)
        yield sse;
    }
  }
  for (const line of lineDecoder.flush()) {
    const sse = sseDecoder.decode(line);
    if (sse)
      yield sse;
  }
}
async function* iterSSEChunks2(iterator) {
  let data = new Uint8Array();
  for await (const chunk of iterator) {
    if (chunk == null) {
      continue;
    }
    const binaryChunk = chunk instanceof ArrayBuffer ? new Uint8Array(chunk) : typeof chunk === "string" ? new TextEncoder().encode(chunk) : chunk;
    let newData = new Uint8Array(data.length + binaryChunk.length);
    newData.set(data);
    newData.set(binaryChunk, data.length);
    data = newData;
    let patternIndex;
    while ((patternIndex = findDoubleNewlineIndex2(data)) !== -1) {
      yield data.slice(0, patternIndex);
      data = data.slice(patternIndex);
    }
  }
  if (data.length > 0) {
    yield data;
  }
}
function findDoubleNewlineIndex2(buffer) {
  const newline = 10;
  const carriage = 13;
  for (let i = 0; i < buffer.length - 2; i++) {
    if (buffer[i] === newline && buffer[i + 1] === newline) {
      return i + 2;
    }
    if (buffer[i] === carriage && buffer[i + 1] === carriage) {
      return i + 2;
    }
    if (buffer[i] === carriage && buffer[i + 1] === newline && i + 3 < buffer.length && buffer[i + 2] === carriage && buffer[i + 3] === newline) {
      return i + 4;
    }
  }
  return -1;
}
var SSEDecoder2 = class {
  constructor() {
    this.event = null;
    this.data = [];
    this.chunks = [];
  }
  decode(line) {
    if (line.endsWith("\r")) {
      line = line.substring(0, line.length - 1);
    }
    if (!line) {
      if (!this.event && !this.data.length)
        return null;
      const sse = {
        event: this.event,
        data: this.data.join("\n"),
        raw: this.chunks
      };
      this.event = null;
      this.data = [];
      this.chunks = [];
      return sse;
    }
    this.chunks.push(line);
    if (line.startsWith(":")) {
      return null;
    }
    let [fieldname, _, value] = partition2(line, ":");
    if (value.startsWith(" ")) {
      value = value.substring(1);
    }
    if (fieldname === "event") {
      this.event = value;
    } else if (fieldname === "data") {
      this.data.push(value);
    }
    return null;
  }
};
var LineDecoder2 = class {
  constructor() {
    this.buffer = [];
    this.trailingCR = false;
  }
  decode(chunk) {
    let text = this.decodeText(chunk);
    if (this.trailingCR) {
      text = "\r" + text;
      this.trailingCR = false;
    }
    if (text.endsWith("\r")) {
      this.trailingCR = true;
      text = text.slice(0, -1);
    }
    if (!text) {
      return [];
    }
    const trailingNewline = LineDecoder2.NEWLINE_CHARS.has(text[text.length - 1] || "");
    let lines = text.split(LineDecoder2.NEWLINE_REGEXP);
    if (trailingNewline) {
      lines.pop();
    }
    if (lines.length === 1 && !trailingNewline) {
      this.buffer.push(lines[0]);
      return [];
    }
    if (this.buffer.length > 0) {
      lines = [this.buffer.join("") + lines[0], ...lines.slice(1)];
      this.buffer = [];
    }
    if (!trailingNewline) {
      this.buffer = [lines.pop() || ""];
    }
    return lines;
  }
  decodeText(bytes) {
    var _a3;
    if (bytes == null)
      return "";
    if (typeof bytes === "string")
      return bytes;
    if (typeof Buffer !== "undefined") {
      if (bytes instanceof Buffer) {
        return bytes.toString();
      }
      if (bytes instanceof Uint8Array) {
        return Buffer.from(bytes).toString();
      }
      throw new AnthropicError(`Unexpected: received non-Uint8Array (${bytes.constructor.name}) stream chunk in an environment with a global "Buffer" defined, which this library assumes to be Node. Please report this error.`);
    }
    if (typeof TextDecoder !== "undefined") {
      if (bytes instanceof Uint8Array || bytes instanceof ArrayBuffer) {
        (_a3 = this.textDecoder) != null ? _a3 : this.textDecoder = new TextDecoder("utf8");
        return this.textDecoder.decode(bytes);
      }
      throw new AnthropicError(`Unexpected: received non-Uint8Array/ArrayBuffer (${bytes.constructor.name}) in a web platform. Please report this error.`);
    }
    throw new AnthropicError(`Unexpected: neither Buffer nor TextDecoder are available as globals. Please report this error.`);
  }
  flush() {
    if (!this.buffer.length && !this.trailingCR) {
      return [];
    }
    const lines = [this.buffer.join("")];
    this.buffer = [];
    this.trailingCR = false;
    return lines;
  }
};
LineDecoder2.NEWLINE_CHARS = /* @__PURE__ */ new Set(["\n", "\r"]);
LineDecoder2.NEWLINE_REGEXP = /\r\n|[\n\r]/g;
function partition2(str2, delimiter) {
  const index = str2.indexOf(delimiter);
  if (index !== -1) {
    return [str2.substring(0, index), delimiter, str2.substring(index + delimiter.length)];
  }
  return [str2, "", ""];
}
function readableStreamAsyncIterable2(stream) {
  if (stream[Symbol.asyncIterator])
    return stream;
  const reader = stream.getReader();
  return {
    async next() {
      try {
        const result = await reader.read();
        if (result == null ? void 0 : result.done)
          reader.releaseLock();
        return result;
      } catch (e) {
        reader.releaseLock();
        throw e;
      }
    },
    async return() {
      const cancelPromise = reader.cancel();
      reader.releaseLock();
      await cancelPromise;
      return { done: true, value: void 0 };
    },
    [Symbol.asyncIterator]() {
      return this;
    }
  };
}

// node_modules/@anthropic-ai/sdk/uploads.mjs
var isResponseLike2 = (value) => value != null && typeof value === "object" && typeof value.url === "string" && typeof value.blob === "function";
var isFileLike2 = (value) => value != null && typeof value === "object" && typeof value.name === "string" && typeof value.lastModified === "number" && isBlobLike2(value);
var isBlobLike2 = (value) => value != null && typeof value === "object" && typeof value.size === "number" && typeof value.type === "string" && typeof value.text === "function" && typeof value.slice === "function" && typeof value.arrayBuffer === "function";
async function toFile3(value, name, options) {
  var _a3, _b, _c;
  value = await value;
  if (isFileLike2(value)) {
    return value;
  }
  if (isResponseLike2(value)) {
    const blob = await value.blob();
    name || (name = (_a3 = new URL(value.url).pathname.split(/[\\/]/).pop()) != null ? _a3 : "unknown_file");
    const data = isBlobLike2(blob) ? [await blob.arrayBuffer()] : [blob];
    return new File3(data, name, options);
  }
  const bits = await getBytes2(value);
  name || (name = (_b = getName2(value)) != null ? _b : "unknown_file");
  if (!(options == null ? void 0 : options.type)) {
    const type = (_c = bits[0]) == null ? void 0 : _c.type;
    if (typeof type === "string") {
      options = { ...options, type };
    }
  }
  return new File3(bits, name, options);
}
async function getBytes2(value) {
  var _a3;
  let parts = [];
  if (typeof value === "string" || ArrayBuffer.isView(value) || // includes Uint8Array, Buffer, etc.
  value instanceof ArrayBuffer) {
    parts.push(value);
  } else if (isBlobLike2(value)) {
    parts.push(await value.arrayBuffer());
  } else if (isAsyncIterableIterator2(value)) {
    for await (const chunk of value) {
      parts.push(chunk);
    }
  } else {
    throw new Error(`Unexpected data type: ${typeof value}; constructor: ${(_a3 = value == null ? void 0 : value.constructor) == null ? void 0 : _a3.name}; props: ${propsForError2(value)}`);
  }
  return parts;
}
function propsForError2(value) {
  const props = Object.getOwnPropertyNames(value);
  return `[${props.map((p) => `"${p}"`).join(", ")}]`;
}
function getName2(value) {
  var _a3;
  return getStringFromMaybeBuffer2(value.name) || getStringFromMaybeBuffer2(value.filename) || // For fs.ReadStream
  ((_a3 = getStringFromMaybeBuffer2(value.path)) == null ? void 0 : _a3.split(/[\\/]/).pop());
}
var getStringFromMaybeBuffer2 = (x) => {
  if (typeof x === "string")
    return x;
  if (typeof Buffer !== "undefined" && x instanceof Buffer)
    return String(x);
  return void 0;
};
var isAsyncIterableIterator2 = (value) => value != null && typeof value === "object" && typeof value[Symbol.asyncIterator] === "function";
var isMultipartBody2 = (body) => body && typeof body === "object" && body.body && body[Symbol.toStringTag] === "MultipartBody";

// node_modules/@anthropic-ai/sdk/core.mjs
var __classPrivateFieldSet6 = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var __classPrivateFieldGet6 = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var _AbstractPage_client2;
async function defaultParseResponse2(props) {
  const { response } = props;
  if (props.options.stream) {
    debug2("response", response.status, response.url, response.headers, response.body);
    if (props.options.__streamClass) {
      return props.options.__streamClass.fromSSEResponse(response, props.controller);
    }
    return Stream2.fromSSEResponse(response, props.controller);
  }
  if (response.status === 204) {
    return null;
  }
  if (props.options.__binaryResponse) {
    return response;
  }
  const contentType = response.headers.get("content-type");
  const isJSON = (contentType == null ? void 0 : contentType.includes("application/json")) || (contentType == null ? void 0 : contentType.includes("application/vnd.api+json"));
  if (isJSON) {
    const json = await response.json();
    debug2("response", response.status, response.url, response.headers, json);
    return json;
  }
  const text = await response.text();
  debug2("response", response.status, response.url, response.headers, text);
  return text;
}
var APIPromise2 = class extends Promise {
  constructor(responsePromise, parseResponse = defaultParseResponse2) {
    super((resolve) => {
      resolve(null);
    });
    this.responsePromise = responsePromise;
    this.parseResponse = parseResponse;
  }
  _thenUnwrap(transform) {
    return new APIPromise2(this.responsePromise, async (props) => transform(await this.parseResponse(props)));
  }
  /**
   * Gets the raw `Response` instance instead of parsing the response
   * data.
   *
   * If you want to parse the response body but still get the `Response`
   * instance, you can use {@link withResponse()}.
   *
   * 👋 Getting the wrong TypeScript type for `Response`?
   * Try setting `"moduleResolution": "NodeNext"` if you can,
   * or add one of these imports before your first `import … from '@anthropic-ai/sdk'`:
   * - `import '@anthropic-ai/sdk/shims/node'` (if you're running on Node)
   * - `import '@anthropic-ai/sdk/shims/web'` (otherwise)
   */
  asResponse() {
    return this.responsePromise.then((p) => p.response);
  }
  /**
   * Gets the parsed response data and the raw `Response` instance.
   *
   * If you just want to get the raw `Response` instance without parsing it,
   * you can use {@link asResponse()}.
   *
   *
   * 👋 Getting the wrong TypeScript type for `Response`?
   * Try setting `"moduleResolution": "NodeNext"` if you can,
   * or add one of these imports before your first `import … from '@anthropic-ai/sdk'`:
   * - `import '@anthropic-ai/sdk/shims/node'` (if you're running on Node)
   * - `import '@anthropic-ai/sdk/shims/web'` (otherwise)
   */
  async withResponse() {
    const [data, response] = await Promise.all([this.parse(), this.asResponse()]);
    return { data, response };
  }
  parse() {
    if (!this.parsedPromise) {
      this.parsedPromise = this.responsePromise.then(this.parseResponse);
    }
    return this.parsedPromise;
  }
  then(onfulfilled, onrejected) {
    return this.parse().then(onfulfilled, onrejected);
  }
  catch(onrejected) {
    return this.parse().catch(onrejected);
  }
  finally(onfinally) {
    return this.parse().finally(onfinally);
  }
};
var APIClient2 = class {
  constructor({
    baseURL,
    maxRetries = 2,
    timeout = 6e5,
    // 10 minutes
    httpAgent,
    fetch: overridenFetch
  }) {
    this.baseURL = baseURL;
    this.maxRetries = validatePositiveInteger2("maxRetries", maxRetries);
    this.timeout = validatePositiveInteger2("timeout", timeout);
    this.httpAgent = httpAgent;
    this.fetch = overridenFetch != null ? overridenFetch : fetch3;
  }
  authHeaders(opts) {
    return {};
  }
  /**
   * Override this to add your own default headers, for example:
   *
   *  {
   *    ...super.defaultHeaders(),
   *    Authorization: 'Bearer 123',
   *  }
   */
  defaultHeaders(opts) {
    return {
      Accept: "application/json",
      "Content-Type": "application/json",
      "User-Agent": this.getUserAgent(),
      ...getPlatformHeaders2(),
      ...this.authHeaders(opts)
    };
  }
  /**
   * Override this to add your own headers validation:
   */
  validateHeaders(headers, customHeaders) {
  }
  defaultIdempotencyKey() {
    return `stainless-node-retry-${uuid42()}`;
  }
  get(path, opts) {
    return this.methodRequest("get", path, opts);
  }
  post(path, opts) {
    return this.methodRequest("post", path, opts);
  }
  patch(path, opts) {
    return this.methodRequest("patch", path, opts);
  }
  put(path, opts) {
    return this.methodRequest("put", path, opts);
  }
  delete(path, opts) {
    return this.methodRequest("delete", path, opts);
  }
  methodRequest(method, path, opts) {
    return this.request(Promise.resolve(opts).then(async (opts2) => {
      const body = opts2 && isBlobLike2(opts2 == null ? void 0 : opts2.body) ? new DataView(await opts2.body.arrayBuffer()) : (opts2 == null ? void 0 : opts2.body) instanceof DataView ? opts2.body : (opts2 == null ? void 0 : opts2.body) instanceof ArrayBuffer ? new DataView(opts2.body) : opts2 && ArrayBuffer.isView(opts2 == null ? void 0 : opts2.body) ? new DataView(opts2.body.buffer) : opts2 == null ? void 0 : opts2.body;
      return { method, path, ...opts2, body };
    }));
  }
  getAPIList(path, Page2, opts) {
    return this.requestAPIList(Page2, { method: "get", path, ...opts });
  }
  calculateContentLength(body) {
    if (typeof body === "string") {
      if (typeof Buffer !== "undefined") {
        return Buffer.byteLength(body, "utf8").toString();
      }
      if (typeof TextEncoder !== "undefined") {
        const encoder = new TextEncoder();
        const encoded = encoder.encode(body);
        return encoded.length.toString();
      }
    } else if (ArrayBuffer.isView(body)) {
      return body.byteLength.toString();
    }
    return null;
  }
  buildRequest(options) {
    var _a3, _b, _c, _d, _e, _f;
    const { method, path, query, headers = {} } = options;
    const body = ArrayBuffer.isView(options.body) || options.__binaryRequest && typeof options.body === "string" ? options.body : isMultipartBody2(options.body) ? options.body.body : options.body ? JSON.stringify(options.body, null, 2) : null;
    const contentLength = this.calculateContentLength(body);
    const url = this.buildURL(path, query);
    if ("timeout" in options)
      validatePositiveInteger2("timeout", options.timeout);
    const timeout = (_a3 = options.timeout) != null ? _a3 : this.timeout;
    const httpAgent = (_c = (_b = options.httpAgent) != null ? _b : this.httpAgent) != null ? _c : getDefaultAgent2(url);
    const minAgentTimeout = timeout + 1e3;
    if (typeof ((_d = httpAgent == null ? void 0 : httpAgent.options) == null ? void 0 : _d.timeout) === "number" && minAgentTimeout > ((_e = httpAgent.options.timeout) != null ? _e : 0)) {
      httpAgent.options.timeout = minAgentTimeout;
    }
    if (this.idempotencyHeader && method !== "get") {
      if (!options.idempotencyKey)
        options.idempotencyKey = this.defaultIdempotencyKey();
      headers[this.idempotencyHeader] = options.idempotencyKey;
    }
    const reqHeaders = this.buildHeaders({ options, headers, contentLength });
    const req = {
      method,
      ...body && { body },
      headers: reqHeaders,
      ...httpAgent && { agent: httpAgent },
      // @ts-ignore node-fetch uses a custom AbortSignal type that is
      // not compatible with standard web types
      signal: (_f = options.signal) != null ? _f : null
    };
    return { req, url, timeout };
  }
  buildHeaders({ options, headers, contentLength }) {
    const reqHeaders = {};
    if (contentLength) {
      reqHeaders["content-length"] = contentLength;
    }
    const defaultHeaders = this.defaultHeaders(options);
    applyHeadersMut2(reqHeaders, defaultHeaders);
    applyHeadersMut2(reqHeaders, headers);
    if (isMultipartBody2(options.body) && kind2 !== "node") {
      delete reqHeaders["content-type"];
    }
    this.validateHeaders(reqHeaders, headers);
    return reqHeaders;
  }
  /**
   * Used as a callback for mutating the given `FinalRequestOptions` object.
   */
  async prepareOptions(options) {
  }
  /**
   * Used as a callback for mutating the given `RequestInit` object.
   *
   * This is useful for cases where you want to add certain headers based off of
   * the request properties, e.g. `method` or `url`.
   */
  async prepareRequest(request, { url, options }) {
  }
  parseHeaders(headers) {
    return !headers ? {} : Symbol.iterator in headers ? Object.fromEntries(Array.from(headers).map((header) => [...header])) : { ...headers };
  }
  makeStatusError(status, error, message, headers) {
    return APIError3.generate(status, error, message, headers);
  }
  request(options, remainingRetries = null) {
    return new APIPromise2(this.makeRequest(options, remainingRetries));
  }
  async makeRequest(optionsInput, retriesRemaining) {
    var _a3, _b, _c;
    const options = await optionsInput;
    if (retriesRemaining == null) {
      retriesRemaining = (_a3 = options.maxRetries) != null ? _a3 : this.maxRetries;
    }
    await this.prepareOptions(options);
    const { req, url, timeout } = this.buildRequest(options);
    await this.prepareRequest(req, { url, options });
    debug2("request", url, options, req.headers);
    if ((_b = options.signal) == null ? void 0 : _b.aborted) {
      throw new APIUserAbortError3();
    }
    const controller = new AbortController();
    const response = await this.fetchWithTimeout(url, req, timeout, controller).catch(castToError2);
    if (response instanceof Error) {
      if ((_c = options.signal) == null ? void 0 : _c.aborted) {
        throw new APIUserAbortError3();
      }
      if (retriesRemaining) {
        return this.retryRequest(options, retriesRemaining);
      }
      if (response.name === "AbortError") {
        throw new APIConnectionTimeoutError3();
      }
      throw new APIConnectionError3({ cause: response });
    }
    const responseHeaders = createResponseHeaders2(response.headers);
    if (!response.ok) {
      if (retriesRemaining && this.shouldRetry(response)) {
        const retryMessage2 = `retrying, ${retriesRemaining} attempts remaining`;
        debug2(`response (error; ${retryMessage2})`, response.status, url, responseHeaders);
        return this.retryRequest(options, retriesRemaining, responseHeaders);
      }
      const errText = await response.text().catch((e) => castToError2(e).message);
      const errJSON = safeJSON2(errText);
      const errMessage = errJSON ? void 0 : errText;
      const retryMessage = retriesRemaining ? `(error; no more retries left)` : `(error; not retryable)`;
      debug2(`response (error; ${retryMessage})`, response.status, url, responseHeaders, errMessage);
      const err = this.makeStatusError(response.status, errJSON, errMessage, responseHeaders);
      throw err;
    }
    return { response, options, controller };
  }
  requestAPIList(Page2, options) {
    const request = this.makeRequest(options, null);
    return new PagePromise2(this, request, Page2);
  }
  buildURL(path, query) {
    const url = isAbsoluteURL2(path) ? new URL(path) : new URL(this.baseURL + (this.baseURL.endsWith("/") && path.startsWith("/") ? path.slice(1) : path));
    const defaultQuery = this.defaultQuery();
    if (!isEmptyObj2(defaultQuery)) {
      query = { ...defaultQuery, ...query };
    }
    if (typeof query === "object" && query && !Array.isArray(query)) {
      url.search = this.stringifyQuery(query);
    }
    return url.toString();
  }
  stringifyQuery(query) {
    return Object.entries(query).filter(([_, value]) => typeof value !== "undefined").map(([key, value]) => {
      if (typeof value === "string" || typeof value === "number" || typeof value === "boolean") {
        return `${encodeURIComponent(key)}=${encodeURIComponent(value)}`;
      }
      if (value === null) {
        return `${encodeURIComponent(key)}=`;
      }
      throw new AnthropicError(`Cannot stringify type ${typeof value}; Expected string, number, boolean, or null. If you need to pass nested query parameters, you can manually encode them, e.g. { query: { 'foo[key1]': value1, 'foo[key2]': value2 } }, and please open a GitHub issue requesting better support for your use case.`);
    }).join("&");
  }
  async fetchWithTimeout(url, init, ms, controller) {
    const { signal, ...options } = init || {};
    if (signal)
      signal.addEventListener("abort", () => controller.abort());
    const timeout = setTimeout(() => controller.abort(), ms);
    return this.getRequestClient().fetch.call(void 0, url, { signal: controller.signal, ...options }).finally(() => {
      clearTimeout(timeout);
    });
  }
  getRequestClient() {
    return { fetch: this.fetch };
  }
  shouldRetry(response) {
    const shouldRetryHeader = response.headers.get("x-should-retry");
    if (shouldRetryHeader === "true")
      return true;
    if (shouldRetryHeader === "false")
      return false;
    if (response.status === 408)
      return true;
    if (response.status === 409)
      return true;
    if (response.status === 429)
      return true;
    if (response.status >= 500)
      return true;
    return false;
  }
  async retryRequest(options, retriesRemaining, responseHeaders) {
    var _a3;
    let timeoutMillis;
    const retryAfterMillisHeader = responseHeaders == null ? void 0 : responseHeaders["retry-after-ms"];
    if (retryAfterMillisHeader) {
      const timeoutMs = parseFloat(retryAfterMillisHeader);
      if (!Number.isNaN(timeoutMs)) {
        timeoutMillis = timeoutMs;
      }
    }
    const retryAfterHeader = responseHeaders == null ? void 0 : responseHeaders["retry-after"];
    if (retryAfterHeader && !timeoutMillis) {
      const timeoutSeconds = parseFloat(retryAfterHeader);
      if (!Number.isNaN(timeoutSeconds)) {
        timeoutMillis = timeoutSeconds * 1e3;
      } else {
        timeoutMillis = Date.parse(retryAfterHeader) - Date.now();
      }
    }
    if (!(timeoutMillis && 0 <= timeoutMillis && timeoutMillis < 60 * 1e3)) {
      const maxRetries = (_a3 = options.maxRetries) != null ? _a3 : this.maxRetries;
      timeoutMillis = this.calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries);
    }
    await sleep2(timeoutMillis);
    return this.makeRequest(options, retriesRemaining - 1);
  }
  calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries) {
    const initialRetryDelay = 0.5;
    const maxRetryDelay = 8;
    const numRetries = maxRetries - retriesRemaining;
    const sleepSeconds = Math.min(initialRetryDelay * Math.pow(2, numRetries), maxRetryDelay);
    const jitter = 1 - Math.random() * 0.25;
    return sleepSeconds * jitter * 1e3;
  }
  getUserAgent() {
    return `${this.constructor.name}/JS ${VERSION2}`;
  }
};
var AbstractPage2 = class {
  constructor(client, response, body, options) {
    _AbstractPage_client2.set(this, void 0);
    __classPrivateFieldSet6(this, _AbstractPage_client2, client, "f");
    this.options = options;
    this.response = response;
    this.body = body;
  }
  hasNextPage() {
    const items = this.getPaginatedItems();
    if (!items.length)
      return false;
    return this.nextPageInfo() != null;
  }
  async getNextPage() {
    const nextInfo = this.nextPageInfo();
    if (!nextInfo) {
      throw new AnthropicError("No next page expected; please check `.hasNextPage()` before calling `.getNextPage()`.");
    }
    const nextOptions = { ...this.options };
    if ("params" in nextInfo && typeof nextOptions.query === "object") {
      nextOptions.query = { ...nextOptions.query, ...nextInfo.params };
    } else if ("url" in nextInfo) {
      const params = [...Object.entries(nextOptions.query || {}), ...nextInfo.url.searchParams.entries()];
      for (const [key, value] of params) {
        nextInfo.url.searchParams.set(key, value);
      }
      nextOptions.query = void 0;
      nextOptions.path = nextInfo.url.toString();
    }
    return await __classPrivateFieldGet6(this, _AbstractPage_client2, "f").requestAPIList(this.constructor, nextOptions);
  }
  async *iterPages() {
    let page = this;
    yield page;
    while (page.hasNextPage()) {
      page = await page.getNextPage();
      yield page;
    }
  }
  async *[(_AbstractPage_client2 = /* @__PURE__ */ new WeakMap(), Symbol.asyncIterator)]() {
    for await (const page of this.iterPages()) {
      for (const item of page.getPaginatedItems()) {
        yield item;
      }
    }
  }
};
var PagePromise2 = class extends APIPromise2 {
  constructor(client, request, Page2) {
    super(request, async (props) => new Page2(client, props.response, await defaultParseResponse2(props), props.options));
  }
  /**
   * Allow auto-paginating iteration on an unawaited list call, eg:
   *
   *    for await (const item of client.items.list()) {
   *      console.log(item)
   *    }
   */
  async *[Symbol.asyncIterator]() {
    const page = await this;
    for await (const item of page) {
      yield item;
    }
  }
};
var createResponseHeaders2 = (headers) => {
  return new Proxy(Object.fromEntries(
    // @ts-ignore
    headers.entries()
  ), {
    get(target, name) {
      const key = name.toString();
      return target[key.toLowerCase()] || target[key];
    }
  });
};
var getPlatformProperties2 = () => {
  var _a3, _b;
  if (typeof Deno !== "undefined" && Deno.build != null) {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION2,
      "X-Stainless-OS": normalizePlatform2(Deno.build.os),
      "X-Stainless-Arch": normalizeArch2(Deno.build.arch),
      "X-Stainless-Runtime": "deno",
      "X-Stainless-Runtime-Version": typeof Deno.version === "string" ? Deno.version : (_b = (_a3 = Deno.version) == null ? void 0 : _a3.deno) != null ? _b : "unknown"
    };
  }
  if (typeof EdgeRuntime !== "undefined") {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION2,
      "X-Stainless-OS": "Unknown",
      "X-Stainless-Arch": `other:${EdgeRuntime}`,
      "X-Stainless-Runtime": "edge",
      "X-Stainless-Runtime-Version": process.version
    };
  }
  if (Object.prototype.toString.call(typeof process !== "undefined" ? process : 0) === "[object process]") {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION2,
      "X-Stainless-OS": normalizePlatform2(process.platform),
      "X-Stainless-Arch": normalizeArch2(process.arch),
      "X-Stainless-Runtime": "node",
      "X-Stainless-Runtime-Version": process.version
    };
  }
  const browserInfo = getBrowserInfo2();
  if (browserInfo) {
    return {
      "X-Stainless-Lang": "js",
      "X-Stainless-Package-Version": VERSION2,
      "X-Stainless-OS": "Unknown",
      "X-Stainless-Arch": "unknown",
      "X-Stainless-Runtime": `browser:${browserInfo.browser}`,
      "X-Stainless-Runtime-Version": browserInfo.version
    };
  }
  return {
    "X-Stainless-Lang": "js",
    "X-Stainless-Package-Version": VERSION2,
    "X-Stainless-OS": "Unknown",
    "X-Stainless-Arch": "unknown",
    "X-Stainless-Runtime": "unknown",
    "X-Stainless-Runtime-Version": "unknown"
  };
};
function getBrowserInfo2() {
  if (typeof navigator === "undefined" || !navigator) {
    return null;
  }
  const browserPatterns = [
    { key: "edge", pattern: /Edge(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "ie", pattern: /MSIE(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "ie", pattern: /Trident(?:.*rv\:(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "chrome", pattern: /Chrome(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "firefox", pattern: /Firefox(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/ },
    { key: "safari", pattern: /(?:Version\W+(\d+)\.(\d+)(?:\.(\d+))?)?(?:\W+Mobile\S*)?\W+Safari/ }
  ];
  for (const { key, pattern } of browserPatterns) {
    const match = pattern.exec(navigator.userAgent);
    if (match) {
      const major = match[1] || 0;
      const minor = match[2] || 0;
      const patch = match[3] || 0;
      return { browser: key, version: `${major}.${minor}.${patch}` };
    }
  }
  return null;
}
var normalizeArch2 = (arch) => {
  if (arch === "x32")
    return "x32";
  if (arch === "x86_64" || arch === "x64")
    return "x64";
  if (arch === "arm")
    return "arm";
  if (arch === "aarch64" || arch === "arm64")
    return "arm64";
  if (arch)
    return `other:${arch}`;
  return "unknown";
};
var normalizePlatform2 = (platform) => {
  platform = platform.toLowerCase();
  if (platform.includes("ios"))
    return "iOS";
  if (platform === "android")
    return "Android";
  if (platform === "darwin")
    return "MacOS";
  if (platform === "win32")
    return "Windows";
  if (platform === "freebsd")
    return "FreeBSD";
  if (platform === "openbsd")
    return "OpenBSD";
  if (platform === "linux")
    return "Linux";
  if (platform)
    return `Other:${platform}`;
  return "Unknown";
};
var _platformHeaders2;
var getPlatformHeaders2 = () => {
  return _platformHeaders2 != null ? _platformHeaders2 : _platformHeaders2 = getPlatformProperties2();
};
var safeJSON2 = (text) => {
  try {
    return JSON.parse(text);
  } catch (err) {
    return void 0;
  }
};
var startsWithSchemeRegexp2 = new RegExp("^(?:[a-z]+:)?//", "i");
var isAbsoluteURL2 = (url) => {
  return startsWithSchemeRegexp2.test(url);
};
var sleep2 = (ms) => new Promise((resolve) => setTimeout(resolve, ms));
var validatePositiveInteger2 = (name, n) => {
  if (typeof n !== "number" || !Number.isInteger(n)) {
    throw new AnthropicError(`${name} must be an integer`);
  }
  if (n < 0) {
    throw new AnthropicError(`${name} must be a positive integer`);
  }
  return n;
};
var castToError2 = (err) => {
  if (err instanceof Error)
    return err;
  return new Error(err);
};
var readEnv2 = (env) => {
  var _a3, _b, _c, _d, _e, _f;
  if (typeof process !== "undefined") {
    return (_c = (_b = (_a3 = process.env) == null ? void 0 : _a3[env]) == null ? void 0 : _b.trim()) != null ? _c : void 0;
  }
  if (typeof Deno !== "undefined") {
    return (_f = (_e = (_d = Deno.env) == null ? void 0 : _d.get) == null ? void 0 : _e.call(_d, env)) == null ? void 0 : _f.trim();
  }
  return void 0;
};
function isEmptyObj2(obj) {
  if (!obj)
    return true;
  for (const _k in obj)
    return false;
  return true;
}
function hasOwn2(obj, key) {
  return Object.prototype.hasOwnProperty.call(obj, key);
}
function applyHeadersMut2(targetHeaders, newHeaders) {
  for (const k in newHeaders) {
    if (!hasOwn2(newHeaders, k))
      continue;
    const lowerKey = k.toLowerCase();
    if (!lowerKey)
      continue;
    const val = newHeaders[k];
    if (val === null) {
      delete targetHeaders[lowerKey];
    } else if (val !== void 0) {
      targetHeaders[lowerKey] = val;
    }
  }
}
function debug2(action, ...args) {
  var _a3;
  if (typeof process !== "undefined" && ((_a3 = process == null ? void 0 : process.env) == null ? void 0 : _a3["DEBUG"]) === "true") {
    console.log(`Anthropic:DEBUG:${action}`, ...args);
  }
}
var uuid42 = () => {
  return "xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx".replace(/[xy]/g, (c) => {
    const r = Math.random() * 16 | 0;
    const v = c === "x" ? r : r & 3 | 8;
    return v.toString(16);
  });
};
var isRunningInBrowser2 = () => {
  return (
    // @ts-ignore
    typeof window !== "undefined" && // @ts-ignore
    typeof window.document !== "undefined" && // @ts-ignore
    typeof navigator !== "undefined"
  );
};

// node_modules/@anthropic-ai/sdk/error.mjs
var AnthropicError = class extends Error {
};
var APIError3 = class extends AnthropicError {
  constructor(status, error, message, headers) {
    super(`${APIError3.makeMessage(status, error, message)}`);
    this.status = status;
    this.headers = headers;
    this.request_id = headers == null ? void 0 : headers["request-id"];
    this.error = error;
  }
  static makeMessage(status, error, message) {
    const msg = (error == null ? void 0 : error.message) ? typeof error.message === "string" ? error.message : JSON.stringify(error.message) : error ? JSON.stringify(error) : message;
    if (status && msg) {
      return `${status} ${msg}`;
    }
    if (status) {
      return `${status} status code (no body)`;
    }
    if (msg) {
      return msg;
    }
    return "(no status code or body)";
  }
  static generate(status, errorResponse, message, headers) {
    if (!status) {
      return new APIConnectionError3({ message, cause: castToError2(errorResponse) });
    }
    const error = errorResponse;
    if (status === 400) {
      return new BadRequestError3(status, error, message, headers);
    }
    if (status === 401) {
      return new AuthenticationError3(status, error, message, headers);
    }
    if (status === 403) {
      return new PermissionDeniedError3(status, error, message, headers);
    }
    if (status === 404) {
      return new NotFoundError3(status, error, message, headers);
    }
    if (status === 409) {
      return new ConflictError3(status, error, message, headers);
    }
    if (status === 422) {
      return new UnprocessableEntityError3(status, error, message, headers);
    }
    if (status === 429) {
      return new RateLimitError3(status, error, message, headers);
    }
    if (status >= 500) {
      return new InternalServerError3(status, error, message, headers);
    }
    return new APIError3(status, error, message, headers);
  }
};
var APIUserAbortError3 = class extends APIError3 {
  constructor({ message } = {}) {
    super(void 0, void 0, message || "Request was aborted.", void 0);
    this.status = void 0;
  }
};
var APIConnectionError3 = class extends APIError3 {
  constructor({ message, cause }) {
    super(void 0, void 0, message || "Connection error.", void 0);
    this.status = void 0;
    if (cause)
      this.cause = cause;
  }
};
var APIConnectionTimeoutError3 = class extends APIConnectionError3 {
  constructor({ message } = {}) {
    super({ message: message != null ? message : "Request timed out." });
  }
};
var BadRequestError3 = class extends APIError3 {
  constructor() {
    super(...arguments);
    this.status = 400;
  }
};
var AuthenticationError3 = class extends APIError3 {
  constructor() {
    super(...arguments);
    this.status = 401;
  }
};
var PermissionDeniedError3 = class extends APIError3 {
  constructor() {
    super(...arguments);
    this.status = 403;
  }
};
var NotFoundError3 = class extends APIError3 {
  constructor() {
    super(...arguments);
    this.status = 404;
  }
};
var ConflictError3 = class extends APIError3 {
  constructor() {
    super(...arguments);
    this.status = 409;
  }
};
var UnprocessableEntityError3 = class extends APIError3 {
  constructor() {
    super(...arguments);
    this.status = 422;
  }
};
var RateLimitError3 = class extends APIError3 {
  constructor() {
    super(...arguments);
    this.status = 429;
  }
};
var InternalServerError3 = class extends APIError3 {
};

// node_modules/@anthropic-ai/sdk/resource.mjs
var APIResource2 = class {
  constructor(client) {
    this._client = client;
  }
};

// node_modules/@anthropic-ai/sdk/_vendor/partial-json-parser/parser.mjs
var tokenize = (input) => {
  let current = 0;
  let tokens = [];
  while (current < input.length) {
    let char = input[current];
    if (char === "\\") {
      current++;
      continue;
    }
    if (char === "{") {
      tokens.push({
        type: "brace",
        value: "{"
      });
      current++;
      continue;
    }
    if (char === "}") {
      tokens.push({
        type: "brace",
        value: "}"
      });
      current++;
      continue;
    }
    if (char === "[") {
      tokens.push({
        type: "paren",
        value: "["
      });
      current++;
      continue;
    }
    if (char === "]") {
      tokens.push({
        type: "paren",
        value: "]"
      });
      current++;
      continue;
    }
    if (char === ":") {
      tokens.push({
        type: "separator",
        value: ":"
      });
      current++;
      continue;
    }
    if (char === ",") {
      tokens.push({
        type: "delimiter",
        value: ","
      });
      current++;
      continue;
    }
    if (char === '"') {
      let value = "";
      let danglingQuote = false;
      char = input[++current];
      while (char !== '"') {
        if (current === input.length) {
          danglingQuote = true;
          break;
        }
        if (char === "\\") {
          current++;
          if (current === input.length) {
            danglingQuote = true;
            break;
          }
          value += char + input[current];
          char = input[++current];
        } else {
          value += char;
          char = input[++current];
        }
      }
      char = input[++current];
      if (!danglingQuote) {
        tokens.push({
          type: "string",
          value
        });
      }
      continue;
    }
    let WHITESPACE = /\s/;
    if (char && WHITESPACE.test(char)) {
      current++;
      continue;
    }
    let NUMBERS = /[0-9]/;
    if (char && NUMBERS.test(char) || char === "-" || char === ".") {
      let value = "";
      if (char === "-") {
        value += char;
        char = input[++current];
      }
      while (char && NUMBERS.test(char) || char === ".") {
        value += char;
        char = input[++current];
      }
      tokens.push({
        type: "number",
        value
      });
      continue;
    }
    let LETTERS = /[a-z]/i;
    if (char && LETTERS.test(char)) {
      let value = "";
      while (char && LETTERS.test(char)) {
        if (current === input.length) {
          break;
        }
        value += char;
        char = input[++current];
      }
      if (value == "true" || value == "false" || value === "null") {
        tokens.push({
          type: "name",
          value
        });
      } else {
        current++;
        continue;
      }
      continue;
    }
    current++;
  }
  return tokens;
};
var strip = (tokens) => {
  if (tokens.length === 0) {
    return tokens;
  }
  let lastToken = tokens[tokens.length - 1];
  switch (lastToken.type) {
    case "separator":
      tokens = tokens.slice(0, tokens.length - 1);
      return strip(tokens);
      break;
    case "number":
      let lastCharacterOfLastToken = lastToken.value[lastToken.value.length - 1];
      if (lastCharacterOfLastToken === "." || lastCharacterOfLastToken === "-") {
        tokens = tokens.slice(0, tokens.length - 1);
        return strip(tokens);
      }
    case "string":
      let tokenBeforeTheLastToken = tokens[tokens.length - 2];
      if ((tokenBeforeTheLastToken == null ? void 0 : tokenBeforeTheLastToken.type) === "delimiter") {
        tokens = tokens.slice(0, tokens.length - 1);
        return strip(tokens);
      } else if ((tokenBeforeTheLastToken == null ? void 0 : tokenBeforeTheLastToken.type) === "brace" && tokenBeforeTheLastToken.value === "{") {
        tokens = tokens.slice(0, tokens.length - 1);
        return strip(tokens);
      }
      break;
    case "delimiter":
      tokens = tokens.slice(0, tokens.length - 1);
      return strip(tokens);
      break;
  }
  return tokens;
};
var unstrip = (tokens) => {
  let tail = [];
  tokens.map((token) => {
    if (token.type === "brace") {
      if (token.value === "{") {
        tail.push("}");
      } else {
        tail.splice(tail.lastIndexOf("}"), 1);
      }
    }
    if (token.type === "paren") {
      if (token.value === "[") {
        tail.push("]");
      } else {
        tail.splice(tail.lastIndexOf("]"), 1);
      }
    }
  });
  if (tail.length > 0) {
    tail.reverse().map((item) => {
      if (item === "}") {
        tokens.push({
          type: "brace",
          value: "}"
        });
      } else if (item === "]") {
        tokens.push({
          type: "paren",
          value: "]"
        });
      }
    });
  }
  return tokens;
};
var generate = (tokens) => {
  let output = "";
  tokens.map((token) => {
    switch (token.type) {
      case "string":
        output += '"' + token.value + '"';
        break;
      default:
        output += token.value;
        break;
    }
  });
  return output;
};
var partialParse = (input) => JSON.parse(generate(unstrip(strip(tokenize(input)))));

// node_modules/@anthropic-ai/sdk/lib/PromptCachingBetaMessageStream.mjs
var __classPrivateFieldSet7 = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var __classPrivateFieldGet7 = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var _PromptCachingBetaMessageStream_instances;
var _PromptCachingBetaMessageStream_currentMessageSnapshot;
var _PromptCachingBetaMessageStream_connectedPromise;
var _PromptCachingBetaMessageStream_resolveConnectedPromise;
var _PromptCachingBetaMessageStream_rejectConnectedPromise;
var _PromptCachingBetaMessageStream_endPromise;
var _PromptCachingBetaMessageStream_resolveEndPromise;
var _PromptCachingBetaMessageStream_rejectEndPromise;
var _PromptCachingBetaMessageStream_listeners;
var _PromptCachingBetaMessageStream_ended;
var _PromptCachingBetaMessageStream_errored;
var _PromptCachingBetaMessageStream_aborted;
var _PromptCachingBetaMessageStream_catchingPromiseCreated;
var _PromptCachingBetaMessageStream_getFinalMessage;
var _PromptCachingBetaMessageStream_getFinalText;
var _PromptCachingBetaMessageStream_handleError;
var _PromptCachingBetaMessageStream_beginRequest;
var _PromptCachingBetaMessageStream_addStreamEvent;
var _PromptCachingBetaMessageStream_endRequest;
var _PromptCachingBetaMessageStream_accumulateMessage;
var JSON_BUF_PROPERTY = "__json_buf";
var PromptCachingBetaMessageStream = class {
  constructor() {
    _PromptCachingBetaMessageStream_instances.add(this);
    this.messages = [];
    this.receivedMessages = [];
    _PromptCachingBetaMessageStream_currentMessageSnapshot.set(this, void 0);
    this.controller = new AbortController();
    _PromptCachingBetaMessageStream_connectedPromise.set(this, void 0);
    _PromptCachingBetaMessageStream_resolveConnectedPromise.set(this, () => {
    });
    _PromptCachingBetaMessageStream_rejectConnectedPromise.set(this, () => {
    });
    _PromptCachingBetaMessageStream_endPromise.set(this, void 0);
    _PromptCachingBetaMessageStream_resolveEndPromise.set(this, () => {
    });
    _PromptCachingBetaMessageStream_rejectEndPromise.set(this, () => {
    });
    _PromptCachingBetaMessageStream_listeners.set(this, {});
    _PromptCachingBetaMessageStream_ended.set(this, false);
    _PromptCachingBetaMessageStream_errored.set(this, false);
    _PromptCachingBetaMessageStream_aborted.set(this, false);
    _PromptCachingBetaMessageStream_catchingPromiseCreated.set(this, false);
    _PromptCachingBetaMessageStream_handleError.set(this, (error) => {
      __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_errored, true, "f");
      if (error instanceof Error && error.name === "AbortError") {
        error = new APIUserAbortError3();
      }
      if (error instanceof APIUserAbortError3) {
        __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_aborted, true, "f");
        return this._emit("abort", error);
      }
      if (error instanceof AnthropicError) {
        return this._emit("error", error);
      }
      if (error instanceof Error) {
        const anthropicError = new AnthropicError(error.message);
        anthropicError.cause = error;
        return this._emit("error", anthropicError);
      }
      return this._emit("error", new AnthropicError(String(error)));
    });
    __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_connectedPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_resolveConnectedPromise, resolve, "f");
      __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_rejectConnectedPromise, reject, "f");
    }), "f");
    __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_endPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_resolveEndPromise, resolve, "f");
      __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_rejectEndPromise, reject, "f");
    }), "f");
    __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_connectedPromise, "f").catch(() => {
    });
    __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_endPromise, "f").catch(() => {
    });
  }
  /**
   * Intended for use on the frontend, consuming a stream produced with
   * `.toReadableStream()` on the backend.
   *
   * Note that messages sent to the model do not appear in `.on('message')`
   * in this context.
   */
  static fromReadableStream(stream) {
    const runner = new PromptCachingBetaMessageStream();
    runner._run(() => runner._fromReadableStream(stream));
    return runner;
  }
  static createMessage(messages2, params, options) {
    const runner = new PromptCachingBetaMessageStream();
    for (const message of params.messages) {
      runner._addPromptCachingBetaMessageParam(message);
    }
    runner._run(() => runner._createPromptCachingBetaMessage(messages2, { ...params, stream: true }, { ...options, headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "stream" } }));
    return runner;
  }
  _run(executor) {
    executor().then(() => {
      this._emitFinal();
      this._emit("end");
    }, __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_handleError, "f"));
  }
  _addPromptCachingBetaMessageParam(message) {
    this.messages.push(message);
  }
  _addPromptCachingBetaMessage(message, emit = true) {
    this.receivedMessages.push(message);
    if (emit) {
      this._emit("message", message);
    }
  }
  async _createPromptCachingBetaMessage(messages2, params, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_beginRequest).call(this);
    const stream = await messages2.create({ ...params, stream: true }, { ...options, signal: this.controller.signal });
    this._connected();
    for await (const event of stream) {
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_addStreamEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError3();
    }
    __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_endRequest).call(this);
  }
  _connected() {
    if (this.ended)
      return;
    __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_resolveConnectedPromise, "f").call(this);
    this._emit("connect");
  }
  get ended() {
    return __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_ended, "f");
  }
  get errored() {
    return __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_errored, "f");
  }
  get aborted() {
    return __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_aborted, "f");
  }
  abort() {
    this.controller.abort();
  }
  /**
   * Adds the listener function to the end of the listeners array for the event.
   * No checks are made to see if the listener has already been added. Multiple calls passing
   * the same combination of event and listener will result in the listener being added, and
   * called, multiple times.
   * @returns this PromptCachingBetaMessageStream, so that calls can be chained
   */
  on(event, listener) {
    const listeners = __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_listeners, "f")[event] || (__classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_listeners, "f")[event] = []);
    listeners.push({ listener });
    return this;
  }
  /**
   * Removes the specified listener from the listener array for the event.
   * off() will remove, at most, one instance of a listener from the listener array. If any single
   * listener has been added multiple times to the listener array for the specified event, then
   * off() must be called multiple times to remove each instance.
   * @returns this PromptCachingBetaMessageStream, so that calls can be chained
   */
  off(event, listener) {
    const listeners = __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_listeners, "f")[event];
    if (!listeners)
      return this;
    const index = listeners.findIndex((l) => l.listener === listener);
    if (index >= 0)
      listeners.splice(index, 1);
    return this;
  }
  /**
   * Adds a one-time listener function for the event. The next time the event is triggered,
   * this listener is removed and then invoked.
   * @returns this PromptCachingBetaMessageStream, so that calls can be chained
   */
  once(event, listener) {
    const listeners = __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_listeners, "f")[event] || (__classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_listeners, "f")[event] = []);
    listeners.push({ listener, once: true });
    return this;
  }
  /**
   * This is similar to `.once()`, but returns a Promise that resolves the next time
   * the event is triggered, instead of calling a listener callback.
   * @returns a Promise that resolves the next time given event is triggered,
   * or rejects if an error is emitted.  (If you request the 'error' event,
   * returns a promise that resolves with the error).
   *
   * Example:
   *
   *   const message = await stream.emitted('message') // rejects if the stream errors
   */
  emitted(event) {
    return new Promise((resolve, reject) => {
      __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_catchingPromiseCreated, true, "f");
      if (event !== "error")
        this.once("error", reject);
      this.once(event, resolve);
    });
  }
  async done() {
    __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_catchingPromiseCreated, true, "f");
    await __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_endPromise, "f");
  }
  get currentMessage() {
    return __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_currentMessageSnapshot, "f");
  }
  /**
   * @returns a promise that resolves with the the final assistant PromptCachingBetaMessage response,
   * or rejects if an error occurred or the stream ended prematurely without producing a PromptCachingBetaMessage.
   */
  async finalMessage() {
    await this.done();
    return __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_getFinalMessage).call(this);
  }
  /**
   * @returns a promise that resolves with the the final assistant PromptCachingBetaMessage's text response, concatenated
   * together if there are more than one text blocks.
   * Rejects if an error occurred or the stream ended prematurely without producing a PromptCachingBetaMessage.
   */
  async finalText() {
    await this.done();
    return __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_getFinalText).call(this);
  }
  _emit(event, ...args) {
    if (__classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_ended, "f"))
      return;
    if (event === "end") {
      __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_ended, true, "f");
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_resolveEndPromise, "f").call(this);
    }
    const listeners = __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_listeners, "f")[event];
    if (listeners) {
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_listeners, "f")[event] = listeners.filter((l) => !l.once);
      listeners.forEach(({ listener }) => listener(...args));
    }
    if (event === "abort") {
      const error = args[0];
      if (!__classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_rejectEndPromise, "f").call(this, error);
      this._emit("end");
      return;
    }
    if (event === "error") {
      const error = args[0];
      if (!__classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_rejectEndPromise, "f").call(this, error);
      this._emit("end");
    }
  }
  _emitFinal() {
    const finalPromptCachingBetaMessage = this.receivedMessages.at(-1);
    if (finalPromptCachingBetaMessage) {
      this._emit("finalPromptCachingBetaMessage", __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_getFinalMessage).call(this));
    }
  }
  async _fromReadableStream(readableStream, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_beginRequest).call(this);
    this._connected();
    const stream = Stream2.fromReadableStream(readableStream, this.controller);
    for await (const event of stream) {
      __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_addStreamEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError3();
    }
    __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_endRequest).call(this);
  }
  [(_PromptCachingBetaMessageStream_currentMessageSnapshot = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_connectedPromise = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_resolveConnectedPromise = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_rejectConnectedPromise = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_endPromise = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_resolveEndPromise = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_rejectEndPromise = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_listeners = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_ended = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_errored = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_aborted = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_catchingPromiseCreated = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_handleError = /* @__PURE__ */ new WeakMap(), _PromptCachingBetaMessageStream_instances = /* @__PURE__ */ new WeakSet(), _PromptCachingBetaMessageStream_getFinalMessage = function _PromptCachingBetaMessageStream_getFinalMessage2() {
    if (this.receivedMessages.length === 0) {
      throw new AnthropicError("stream ended without producing a PromptCachingBetaMessage with role=assistant");
    }
    return this.receivedMessages.at(-1);
  }, _PromptCachingBetaMessageStream_getFinalText = function _PromptCachingBetaMessageStream_getFinalText2() {
    if (this.receivedMessages.length === 0) {
      throw new AnthropicError("stream ended without producing a PromptCachingBetaMessage with role=assistant");
    }
    const textBlocks = this.receivedMessages.at(-1).content.filter((block) => block.type === "text").map((block) => block.text);
    if (textBlocks.length === 0) {
      throw new AnthropicError("stream ended without producing a content block with type=text");
    }
    return textBlocks.join(" ");
  }, _PromptCachingBetaMessageStream_beginRequest = function _PromptCachingBetaMessageStream_beginRequest2() {
    if (this.ended)
      return;
    __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_currentMessageSnapshot, void 0, "f");
  }, _PromptCachingBetaMessageStream_addStreamEvent = function _PromptCachingBetaMessageStream_addStreamEvent2(event) {
    if (this.ended)
      return;
    const messageSnapshot = __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_instances, "m", _PromptCachingBetaMessageStream_accumulateMessage).call(this, event);
    this._emit("streamEvent", event, messageSnapshot);
    switch (event.type) {
      case "content_block_delta": {
        const content = messageSnapshot.content.at(-1);
        if (event.delta.type === "text_delta" && content.type === "text") {
          this._emit("text", event.delta.text, content.text || "");
        } else if (event.delta.type === "input_json_delta" && content.type === "tool_use") {
          if (content.input) {
            this._emit("inputJson", event.delta.partial_json, content.input);
          }
        }
        break;
      }
      case "message_stop": {
        this._addPromptCachingBetaMessageParam(messageSnapshot);
        this._addPromptCachingBetaMessage(messageSnapshot, true);
        break;
      }
      case "content_block_stop": {
        this._emit("contentBlock", messageSnapshot.content.at(-1));
        break;
      }
      case "message_start": {
        __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_currentMessageSnapshot, messageSnapshot, "f");
        break;
      }
      case "content_block_start":
      case "message_delta":
        break;
    }
  }, _PromptCachingBetaMessageStream_endRequest = function _PromptCachingBetaMessageStream_endRequest2() {
    if (this.ended) {
      throw new AnthropicError(`stream has ended, this shouldn't happen`);
    }
    const snapshot = __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_currentMessageSnapshot, "f");
    if (!snapshot) {
      throw new AnthropicError(`request ended without sending any chunks`);
    }
    __classPrivateFieldSet7(this, _PromptCachingBetaMessageStream_currentMessageSnapshot, void 0, "f");
    return snapshot;
  }, _PromptCachingBetaMessageStream_accumulateMessage = function _PromptCachingBetaMessageStream_accumulateMessage2(event) {
    let snapshot = __classPrivateFieldGet7(this, _PromptCachingBetaMessageStream_currentMessageSnapshot, "f");
    if (event.type === "message_start") {
      if (snapshot) {
        throw new AnthropicError(`Unexpected event order, got ${event.type} before receiving "message_stop"`);
      }
      return event.message;
    }
    if (!snapshot) {
      throw new AnthropicError(`Unexpected event order, got ${event.type} before "message_start"`);
    }
    switch (event.type) {
      case "message_stop":
        return snapshot;
      case "message_delta":
        snapshot.stop_reason = event.delta.stop_reason;
        snapshot.stop_sequence = event.delta.stop_sequence;
        snapshot.usage.output_tokens = event.usage.output_tokens;
        return snapshot;
      case "content_block_start":
        snapshot.content.push(event.content_block);
        return snapshot;
      case "content_block_delta": {
        const snapshotContent = snapshot.content.at(event.index);
        if ((snapshotContent == null ? void 0 : snapshotContent.type) === "text" && event.delta.type === "text_delta") {
          snapshotContent.text += event.delta.text;
        } else if ((snapshotContent == null ? void 0 : snapshotContent.type) === "tool_use" && event.delta.type === "input_json_delta") {
          let jsonBuf = snapshotContent[JSON_BUF_PROPERTY] || "";
          jsonBuf += event.delta.partial_json;
          Object.defineProperty(snapshotContent, JSON_BUF_PROPERTY, {
            value: jsonBuf,
            enumerable: false,
            writable: true
          });
          if (jsonBuf) {
            snapshotContent.input = partialParse(jsonBuf);
          }
        }
        return snapshot;
      }
      case "content_block_stop":
        return snapshot;
    }
  }, Symbol.asyncIterator)]() {
    const pushQueue = [];
    const readQueue = [];
    let done = false;
    this.on("streamEvent", (event) => {
      const reader = readQueue.shift();
      if (reader) {
        reader.resolve(event);
      } else {
        pushQueue.push(event);
      }
    });
    this.on("end", () => {
      done = true;
      for (const reader of readQueue) {
        reader.resolve(void 0);
      }
      readQueue.length = 0;
    });
    this.on("abort", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    this.on("error", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    return {
      next: async () => {
        if (!pushQueue.length) {
          if (done) {
            return { value: void 0, done: true };
          }
          return new Promise((resolve, reject) => readQueue.push({ resolve, reject })).then((chunk2) => chunk2 ? { value: chunk2, done: false } : { value: void 0, done: true });
        }
        const chunk = pushQueue.shift();
        return { value: chunk, done: false };
      },
      return: async () => {
        this.abort();
        return { value: void 0, done: true };
      }
    };
  }
  toReadableStream() {
    const stream = new Stream2(this[Symbol.asyncIterator].bind(this), this.controller);
    return stream.toReadableStream();
  }
};

// node_modules/@anthropic-ai/sdk/resources/beta/prompt-caching/messages.mjs
var Messages2 = class extends APIResource2 {
  create(body, options) {
    var _a3, _b;
    return this._client.post("/v1/messages?beta=prompt_caching", {
      body,
      timeout: (_a3 = this._client._options.timeout) != null ? _a3 : 6e5,
      ...options,
      headers: { "anthropic-beta": "prompt-caching-2024-07-31", ...options == null ? void 0 : options.headers },
      stream: (_b = body.stream) != null ? _b : false
    });
  }
  /**
   * Create a Message stream
   */
  stream(body, options) {
    return PromptCachingBetaMessageStream.createMessage(this, body, options);
  }
};
(function(Messages4) {
})(Messages2 || (Messages2 = {}));

// node_modules/@anthropic-ai/sdk/resources/beta/prompt-caching/prompt-caching.mjs
var PromptCaching = class extends APIResource2 {
  constructor() {
    super(...arguments);
    this.messages = new Messages2(this._client);
  }
};
(function(PromptCaching2) {
  PromptCaching2.Messages = Messages2;
})(PromptCaching || (PromptCaching = {}));

// node_modules/@anthropic-ai/sdk/resources/beta/beta.mjs
var Beta2 = class extends APIResource2 {
  constructor() {
    super(...arguments);
    this.promptCaching = new PromptCaching(this._client);
  }
};
(function(Beta3) {
  Beta3.PromptCaching = PromptCaching;
})(Beta2 || (Beta2 = {}));

// node_modules/@anthropic-ai/sdk/resources/completions.mjs
var Completions4 = class extends APIResource2 {
  create(body, options) {
    var _a3, _b;
    return this._client.post("/v1/complete", {
      body,
      timeout: (_a3 = this._client._options.timeout) != null ? _a3 : 6e5,
      ...options,
      stream: (_b = body.stream) != null ? _b : false
    });
  }
};
(function(Completions5) {
})(Completions4 || (Completions4 = {}));

// node_modules/@anthropic-ai/sdk/lib/MessageStream.mjs
var __classPrivateFieldSet8 = function(receiver, state, value, kind3, f) {
  if (kind3 === "m")
    throw new TypeError("Private method is not writable");
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a setter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot write private member to an object whose class did not declare it");
  return kind3 === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
};
var __classPrivateFieldGet8 = function(receiver, state, kind3, f) {
  if (kind3 === "a" && !f)
    throw new TypeError("Private accessor was defined without a getter");
  if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver))
    throw new TypeError("Cannot read private member from an object whose class did not declare it");
  return kind3 === "m" ? f : kind3 === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
};
var _MessageStream_instances;
var _MessageStream_currentMessageSnapshot;
var _MessageStream_connectedPromise;
var _MessageStream_resolveConnectedPromise;
var _MessageStream_rejectConnectedPromise;
var _MessageStream_endPromise;
var _MessageStream_resolveEndPromise;
var _MessageStream_rejectEndPromise;
var _MessageStream_listeners;
var _MessageStream_ended;
var _MessageStream_errored;
var _MessageStream_aborted;
var _MessageStream_catchingPromiseCreated;
var _MessageStream_getFinalMessage;
var _MessageStream_getFinalText;
var _MessageStream_handleError;
var _MessageStream_beginRequest;
var _MessageStream_addStreamEvent;
var _MessageStream_endRequest;
var _MessageStream_accumulateMessage;
var JSON_BUF_PROPERTY2 = "__json_buf";
var MessageStream = class {
  constructor() {
    _MessageStream_instances.add(this);
    this.messages = [];
    this.receivedMessages = [];
    _MessageStream_currentMessageSnapshot.set(this, void 0);
    this.controller = new AbortController();
    _MessageStream_connectedPromise.set(this, void 0);
    _MessageStream_resolveConnectedPromise.set(this, () => {
    });
    _MessageStream_rejectConnectedPromise.set(this, () => {
    });
    _MessageStream_endPromise.set(this, void 0);
    _MessageStream_resolveEndPromise.set(this, () => {
    });
    _MessageStream_rejectEndPromise.set(this, () => {
    });
    _MessageStream_listeners.set(this, {});
    _MessageStream_ended.set(this, false);
    _MessageStream_errored.set(this, false);
    _MessageStream_aborted.set(this, false);
    _MessageStream_catchingPromiseCreated.set(this, false);
    _MessageStream_handleError.set(this, (error) => {
      __classPrivateFieldSet8(this, _MessageStream_errored, true, "f");
      if (error instanceof Error && error.name === "AbortError") {
        error = new APIUserAbortError3();
      }
      if (error instanceof APIUserAbortError3) {
        __classPrivateFieldSet8(this, _MessageStream_aborted, true, "f");
        return this._emit("abort", error);
      }
      if (error instanceof AnthropicError) {
        return this._emit("error", error);
      }
      if (error instanceof Error) {
        const anthropicError = new AnthropicError(error.message);
        anthropicError.cause = error;
        return this._emit("error", anthropicError);
      }
      return this._emit("error", new AnthropicError(String(error)));
    });
    __classPrivateFieldSet8(this, _MessageStream_connectedPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet8(this, _MessageStream_resolveConnectedPromise, resolve, "f");
      __classPrivateFieldSet8(this, _MessageStream_rejectConnectedPromise, reject, "f");
    }), "f");
    __classPrivateFieldSet8(this, _MessageStream_endPromise, new Promise((resolve, reject) => {
      __classPrivateFieldSet8(this, _MessageStream_resolveEndPromise, resolve, "f");
      __classPrivateFieldSet8(this, _MessageStream_rejectEndPromise, reject, "f");
    }), "f");
    __classPrivateFieldGet8(this, _MessageStream_connectedPromise, "f").catch(() => {
    });
    __classPrivateFieldGet8(this, _MessageStream_endPromise, "f").catch(() => {
    });
  }
  /**
   * Intended for use on the frontend, consuming a stream produced with
   * `.toReadableStream()` on the backend.
   *
   * Note that messages sent to the model do not appear in `.on('message')`
   * in this context.
   */
  static fromReadableStream(stream) {
    const runner = new MessageStream();
    runner._run(() => runner._fromReadableStream(stream));
    return runner;
  }
  static createMessage(messages2, params, options) {
    const runner = new MessageStream();
    for (const message of params.messages) {
      runner._addMessageParam(message);
    }
    runner._run(() => runner._createMessage(messages2, { ...params, stream: true }, { ...options, headers: { ...options == null ? void 0 : options.headers, "X-Stainless-Helper-Method": "stream" } }));
    return runner;
  }
  _run(executor) {
    executor().then(() => {
      this._emitFinal();
      this._emit("end");
    }, __classPrivateFieldGet8(this, _MessageStream_handleError, "f"));
  }
  _addMessageParam(message) {
    this.messages.push(message);
  }
  _addMessage(message, emit = true) {
    this.receivedMessages.push(message);
    if (emit) {
      this._emit("message", message);
    }
  }
  async _createMessage(messages2, params, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_beginRequest).call(this);
    const stream = await messages2.create({ ...params, stream: true }, { ...options, signal: this.controller.signal });
    this._connected();
    for await (const event of stream) {
      __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_addStreamEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError3();
    }
    __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_endRequest).call(this);
  }
  _connected() {
    if (this.ended)
      return;
    __classPrivateFieldGet8(this, _MessageStream_resolveConnectedPromise, "f").call(this);
    this._emit("connect");
  }
  get ended() {
    return __classPrivateFieldGet8(this, _MessageStream_ended, "f");
  }
  get errored() {
    return __classPrivateFieldGet8(this, _MessageStream_errored, "f");
  }
  get aborted() {
    return __classPrivateFieldGet8(this, _MessageStream_aborted, "f");
  }
  abort() {
    this.controller.abort();
  }
  /**
   * Adds the listener function to the end of the listeners array for the event.
   * No checks are made to see if the listener has already been added. Multiple calls passing
   * the same combination of event and listener will result in the listener being added, and
   * called, multiple times.
   * @returns this MessageStream, so that calls can be chained
   */
  on(event, listener) {
    const listeners = __classPrivateFieldGet8(this, _MessageStream_listeners, "f")[event] || (__classPrivateFieldGet8(this, _MessageStream_listeners, "f")[event] = []);
    listeners.push({ listener });
    return this;
  }
  /**
   * Removes the specified listener from the listener array for the event.
   * off() will remove, at most, one instance of a listener from the listener array. If any single
   * listener has been added multiple times to the listener array for the specified event, then
   * off() must be called multiple times to remove each instance.
   * @returns this MessageStream, so that calls can be chained
   */
  off(event, listener) {
    const listeners = __classPrivateFieldGet8(this, _MessageStream_listeners, "f")[event];
    if (!listeners)
      return this;
    const index = listeners.findIndex((l) => l.listener === listener);
    if (index >= 0)
      listeners.splice(index, 1);
    return this;
  }
  /**
   * Adds a one-time listener function for the event. The next time the event is triggered,
   * this listener is removed and then invoked.
   * @returns this MessageStream, so that calls can be chained
   */
  once(event, listener) {
    const listeners = __classPrivateFieldGet8(this, _MessageStream_listeners, "f")[event] || (__classPrivateFieldGet8(this, _MessageStream_listeners, "f")[event] = []);
    listeners.push({ listener, once: true });
    return this;
  }
  /**
   * This is similar to `.once()`, but returns a Promise that resolves the next time
   * the event is triggered, instead of calling a listener callback.
   * @returns a Promise that resolves the next time given event is triggered,
   * or rejects if an error is emitted.  (If you request the 'error' event,
   * returns a promise that resolves with the error).
   *
   * Example:
   *
   *   const message = await stream.emitted('message') // rejects if the stream errors
   */
  emitted(event) {
    return new Promise((resolve, reject) => {
      __classPrivateFieldSet8(this, _MessageStream_catchingPromiseCreated, true, "f");
      if (event !== "error")
        this.once("error", reject);
      this.once(event, resolve);
    });
  }
  async done() {
    __classPrivateFieldSet8(this, _MessageStream_catchingPromiseCreated, true, "f");
    await __classPrivateFieldGet8(this, _MessageStream_endPromise, "f");
  }
  get currentMessage() {
    return __classPrivateFieldGet8(this, _MessageStream_currentMessageSnapshot, "f");
  }
  /**
   * @returns a promise that resolves with the the final assistant Message response,
   * or rejects if an error occurred or the stream ended prematurely without producing a Message.
   */
  async finalMessage() {
    await this.done();
    return __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_getFinalMessage).call(this);
  }
  /**
   * @returns a promise that resolves with the the final assistant Message's text response, concatenated
   * together if there are more than one text blocks.
   * Rejects if an error occurred or the stream ended prematurely without producing a Message.
   */
  async finalText() {
    await this.done();
    return __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_getFinalText).call(this);
  }
  _emit(event, ...args) {
    if (__classPrivateFieldGet8(this, _MessageStream_ended, "f"))
      return;
    if (event === "end") {
      __classPrivateFieldSet8(this, _MessageStream_ended, true, "f");
      __classPrivateFieldGet8(this, _MessageStream_resolveEndPromise, "f").call(this);
    }
    const listeners = __classPrivateFieldGet8(this, _MessageStream_listeners, "f")[event];
    if (listeners) {
      __classPrivateFieldGet8(this, _MessageStream_listeners, "f")[event] = listeners.filter((l) => !l.once);
      listeners.forEach(({ listener }) => listener(...args));
    }
    if (event === "abort") {
      const error = args[0];
      if (!__classPrivateFieldGet8(this, _MessageStream_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet8(this, _MessageStream_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet8(this, _MessageStream_rejectEndPromise, "f").call(this, error);
      this._emit("end");
      return;
    }
    if (event === "error") {
      const error = args[0];
      if (!__classPrivateFieldGet8(this, _MessageStream_catchingPromiseCreated, "f") && !(listeners == null ? void 0 : listeners.length)) {
        Promise.reject(error);
      }
      __classPrivateFieldGet8(this, _MessageStream_rejectConnectedPromise, "f").call(this, error);
      __classPrivateFieldGet8(this, _MessageStream_rejectEndPromise, "f").call(this, error);
      this._emit("end");
    }
  }
  _emitFinal() {
    const finalMessage = this.receivedMessages.at(-1);
    if (finalMessage) {
      this._emit("finalMessage", __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_getFinalMessage).call(this));
    }
  }
  async _fromReadableStream(readableStream, options) {
    var _a3;
    const signal = options == null ? void 0 : options.signal;
    if (signal) {
      if (signal.aborted)
        this.controller.abort();
      signal.addEventListener("abort", () => this.controller.abort());
    }
    __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_beginRequest).call(this);
    this._connected();
    const stream = Stream2.fromReadableStream(readableStream, this.controller);
    for await (const event of stream) {
      __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_addStreamEvent).call(this, event);
    }
    if ((_a3 = stream.controller.signal) == null ? void 0 : _a3.aborted) {
      throw new APIUserAbortError3();
    }
    __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_endRequest).call(this);
  }
  [(_MessageStream_currentMessageSnapshot = /* @__PURE__ */ new WeakMap(), _MessageStream_connectedPromise = /* @__PURE__ */ new WeakMap(), _MessageStream_resolveConnectedPromise = /* @__PURE__ */ new WeakMap(), _MessageStream_rejectConnectedPromise = /* @__PURE__ */ new WeakMap(), _MessageStream_endPromise = /* @__PURE__ */ new WeakMap(), _MessageStream_resolveEndPromise = /* @__PURE__ */ new WeakMap(), _MessageStream_rejectEndPromise = /* @__PURE__ */ new WeakMap(), _MessageStream_listeners = /* @__PURE__ */ new WeakMap(), _MessageStream_ended = /* @__PURE__ */ new WeakMap(), _MessageStream_errored = /* @__PURE__ */ new WeakMap(), _MessageStream_aborted = /* @__PURE__ */ new WeakMap(), _MessageStream_catchingPromiseCreated = /* @__PURE__ */ new WeakMap(), _MessageStream_handleError = /* @__PURE__ */ new WeakMap(), _MessageStream_instances = /* @__PURE__ */ new WeakSet(), _MessageStream_getFinalMessage = function _MessageStream_getFinalMessage2() {
    if (this.receivedMessages.length === 0) {
      throw new AnthropicError("stream ended without producing a Message with role=assistant");
    }
    return this.receivedMessages.at(-1);
  }, _MessageStream_getFinalText = function _MessageStream_getFinalText2() {
    if (this.receivedMessages.length === 0) {
      throw new AnthropicError("stream ended without producing a Message with role=assistant");
    }
    const textBlocks = this.receivedMessages.at(-1).content.filter((block) => block.type === "text").map((block) => block.text);
    if (textBlocks.length === 0) {
      throw new AnthropicError("stream ended without producing a content block with type=text");
    }
    return textBlocks.join(" ");
  }, _MessageStream_beginRequest = function _MessageStream_beginRequest2() {
    if (this.ended)
      return;
    __classPrivateFieldSet8(this, _MessageStream_currentMessageSnapshot, void 0, "f");
  }, _MessageStream_addStreamEvent = function _MessageStream_addStreamEvent2(event) {
    if (this.ended)
      return;
    const messageSnapshot = __classPrivateFieldGet8(this, _MessageStream_instances, "m", _MessageStream_accumulateMessage).call(this, event);
    this._emit("streamEvent", event, messageSnapshot);
    switch (event.type) {
      case "content_block_delta": {
        const content = messageSnapshot.content.at(-1);
        if (event.delta.type === "text_delta" && content.type === "text") {
          this._emit("text", event.delta.text, content.text || "");
        } else if (event.delta.type === "input_json_delta" && content.type === "tool_use") {
          if (content.input) {
            this._emit("inputJson", event.delta.partial_json, content.input);
          }
        }
        break;
      }
      case "message_stop": {
        this._addMessageParam(messageSnapshot);
        this._addMessage(messageSnapshot, true);
        break;
      }
      case "content_block_stop": {
        this._emit("contentBlock", messageSnapshot.content.at(-1));
        break;
      }
      case "message_start": {
        __classPrivateFieldSet8(this, _MessageStream_currentMessageSnapshot, messageSnapshot, "f");
        break;
      }
      case "content_block_start":
      case "message_delta":
        break;
    }
  }, _MessageStream_endRequest = function _MessageStream_endRequest2() {
    if (this.ended) {
      throw new AnthropicError(`stream has ended, this shouldn't happen`);
    }
    const snapshot = __classPrivateFieldGet8(this, _MessageStream_currentMessageSnapshot, "f");
    if (!snapshot) {
      throw new AnthropicError(`request ended without sending any chunks`);
    }
    __classPrivateFieldSet8(this, _MessageStream_currentMessageSnapshot, void 0, "f");
    return snapshot;
  }, _MessageStream_accumulateMessage = function _MessageStream_accumulateMessage2(event) {
    let snapshot = __classPrivateFieldGet8(this, _MessageStream_currentMessageSnapshot, "f");
    if (event.type === "message_start") {
      if (snapshot) {
        throw new AnthropicError(`Unexpected event order, got ${event.type} before receiving "message_stop"`);
      }
      return event.message;
    }
    if (!snapshot) {
      throw new AnthropicError(`Unexpected event order, got ${event.type} before "message_start"`);
    }
    switch (event.type) {
      case "message_stop":
        return snapshot;
      case "message_delta":
        snapshot.stop_reason = event.delta.stop_reason;
        snapshot.stop_sequence = event.delta.stop_sequence;
        snapshot.usage.output_tokens = event.usage.output_tokens;
        return snapshot;
      case "content_block_start":
        snapshot.content.push(event.content_block);
        return snapshot;
      case "content_block_delta": {
        const snapshotContent = snapshot.content.at(event.index);
        if ((snapshotContent == null ? void 0 : snapshotContent.type) === "text" && event.delta.type === "text_delta") {
          snapshotContent.text += event.delta.text;
        } else if ((snapshotContent == null ? void 0 : snapshotContent.type) === "tool_use" && event.delta.type === "input_json_delta") {
          let jsonBuf = snapshotContent[JSON_BUF_PROPERTY2] || "";
          jsonBuf += event.delta.partial_json;
          Object.defineProperty(snapshotContent, JSON_BUF_PROPERTY2, {
            value: jsonBuf,
            enumerable: false,
            writable: true
          });
          if (jsonBuf) {
            snapshotContent.input = partialParse(jsonBuf);
          }
        }
        return snapshot;
      }
      case "content_block_stop":
        return snapshot;
    }
  }, Symbol.asyncIterator)]() {
    const pushQueue = [];
    const readQueue = [];
    let done = false;
    this.on("streamEvent", (event) => {
      const reader = readQueue.shift();
      if (reader) {
        reader.resolve(event);
      } else {
        pushQueue.push(event);
      }
    });
    this.on("end", () => {
      done = true;
      for (const reader of readQueue) {
        reader.resolve(void 0);
      }
      readQueue.length = 0;
    });
    this.on("abort", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    this.on("error", (err) => {
      done = true;
      for (const reader of readQueue) {
        reader.reject(err);
      }
      readQueue.length = 0;
    });
    return {
      next: async () => {
        if (!pushQueue.length) {
          if (done) {
            return { value: void 0, done: true };
          }
          return new Promise((resolve, reject) => readQueue.push({ resolve, reject })).then((chunk2) => chunk2 ? { value: chunk2, done: false } : { value: void 0, done: true });
        }
        const chunk = pushQueue.shift();
        return { value: chunk, done: false };
      },
      return: async () => {
        this.abort();
        return { value: void 0, done: true };
      }
    };
  }
  toReadableStream() {
    const stream = new Stream2(this[Symbol.asyncIterator].bind(this), this.controller);
    return stream.toReadableStream();
  }
};

// node_modules/@anthropic-ai/sdk/resources/messages.mjs
var Messages3 = class extends APIResource2 {
  create(body, options) {
    var _a3, _b;
    if (body.model in DEPRECATED_MODELS) {
      console.warn(`The model '${body.model}' is deprecated and will reach end-of-life on ${DEPRECATED_MODELS[body.model]}
Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.`);
    }
    return this._client.post("/v1/messages", {
      body,
      timeout: (_a3 = this._client._options.timeout) != null ? _a3 : 6e5,
      ...options,
      stream: (_b = body.stream) != null ? _b : false
    });
  }
  /**
   * Create a Message stream
   */
  stream(body, options) {
    return MessageStream.createMessage(this, body, options);
  }
};
var DEPRECATED_MODELS = {
  "claude-1.3": "November 6th, 2024",
  "claude-1.3-100k": "November 6th, 2024",
  "claude-instant-1.1": "November 6th, 2024",
  "claude-instant-1.1-100k": "November 6th, 2024",
  "claude-instant-1.2": "November 6th, 2024"
};
(function(Messages4) {
})(Messages3 || (Messages3 = {}));

// node_modules/@anthropic-ai/sdk/index.mjs
var _a2;
var Anthropic = class extends APIClient2 {
  /**
   * API Client for interfacing with the Anthropic API.
   *
   * @param {string | null | undefined} [opts.apiKey=process.env['ANTHROPIC_API_KEY'] ?? null]
   * @param {string | null | undefined} [opts.authToken=process.env['ANTHROPIC_AUTH_TOKEN'] ?? null]
   * @param {string} [opts.baseURL=process.env['ANTHROPIC_BASE_URL'] ?? https://api.anthropic.com] - Override the default base URL for the API.
   * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.
   * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.
   * @param {Core.Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.
   * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.
   * @param {Core.Headers} opts.defaultHeaders - Default headers to include with every request to the API.
   * @param {Core.DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.
   * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.
   */
  constructor({ baseURL = readEnv2("ANTHROPIC_BASE_URL"), apiKey = ((_a3) => (_a3 = readEnv2("ANTHROPIC_API_KEY")) != null ? _a3 : null)(), authToken = ((_b) => (_b = readEnv2("ANTHROPIC_AUTH_TOKEN")) != null ? _b : null)(), ...opts } = {}) {
    var _a4;
    const options = {
      apiKey,
      authToken,
      ...opts,
      baseURL: baseURL || `https://api.anthropic.com`
    };
    if (!options.dangerouslyAllowBrowser && isRunningInBrowser2()) {
      throw new AnthropicError("It looks like you're running in a browser-like environment.\n\nThis is disabled by default, as it risks exposing your secret API credentials to attackers.\nIf you understand the risks and have appropriate mitigations in place,\nyou can set the `dangerouslyAllowBrowser` option to `true`, e.g.,\n\nnew Anthropic({ apiKey, dangerouslyAllowBrowser: true });\n\nTODO: link!\n");
    }
    super({
      baseURL: options.baseURL,
      timeout: (_a4 = options.timeout) != null ? _a4 : 6e5,
      httpAgent: options.httpAgent,
      maxRetries: options.maxRetries,
      fetch: options.fetch
    });
    this.completions = new Completions4(this);
    this.messages = new Messages3(this);
    this.beta = new Beta2(this);
    this._options = options;
    this.apiKey = apiKey;
    this.authToken = authToken;
  }
  defaultQuery() {
    return this._options.defaultQuery;
  }
  defaultHeaders(opts) {
    return {
      ...super.defaultHeaders(opts),
      ...this._options.dangerouslyAllowBrowser ? { "anthropic-dangerous-direct-browser-access": "true" } : void 0,
      "anthropic-version": "2023-06-01",
      ...this._options.defaultHeaders
    };
  }
  validateHeaders(headers, customHeaders) {
    if (this.apiKey && headers["x-api-key"]) {
      return;
    }
    if (customHeaders["x-api-key"] === null) {
      return;
    }
    if (this.authToken && headers["authorization"]) {
      return;
    }
    if (customHeaders["authorization"] === null) {
      return;
    }
    throw new Error('Could not resolve authentication method. Expected either apiKey or authToken to be set. Or for one of the "X-Api-Key" or "Authorization" headers to be explicitly omitted');
  }
  authHeaders(opts) {
    const apiKeyAuth = this.apiKeyAuth(opts);
    const bearerAuth = this.bearerAuth(opts);
    if (apiKeyAuth != null && !isEmptyObj2(apiKeyAuth)) {
      return apiKeyAuth;
    }
    if (bearerAuth != null && !isEmptyObj2(bearerAuth)) {
      return bearerAuth;
    }
    return {};
  }
  apiKeyAuth(opts) {
    if (this.apiKey == null) {
      return {};
    }
    return { "X-Api-Key": this.apiKey };
  }
  bearerAuth(opts) {
    if (this.authToken == null) {
      return {};
    }
    return { Authorization: `Bearer ${this.authToken}` };
  }
};
_a2 = Anthropic;
Anthropic.Anthropic = _a2;
Anthropic.HUMAN_PROMPT = "\n\nHuman:";
Anthropic.AI_PROMPT = "\n\nAssistant:";
Anthropic.DEFAULT_TIMEOUT = 6e5;
Anthropic.AnthropicError = AnthropicError;
Anthropic.APIError = APIError3;
Anthropic.APIConnectionError = APIConnectionError3;
Anthropic.APIConnectionTimeoutError = APIConnectionTimeoutError3;
Anthropic.APIUserAbortError = APIUserAbortError3;
Anthropic.NotFoundError = NotFoundError3;
Anthropic.ConflictError = ConflictError3;
Anthropic.RateLimitError = RateLimitError3;
Anthropic.BadRequestError = BadRequestError3;
Anthropic.AuthenticationError = AuthenticationError3;
Anthropic.InternalServerError = InternalServerError3;
Anthropic.PermissionDeniedError = PermissionDeniedError3;
Anthropic.UnprocessableEntityError = UnprocessableEntityError3;
Anthropic.toFile = toFile3;
Anthropic.fileFromPath = fileFromPath2;
var { HUMAN_PROMPT, AI_PROMPT } = Anthropic;
var { AnthropicError: AnthropicError2, APIError: APIError4, APIConnectionError: APIConnectionError4, APIConnectionTimeoutError: APIConnectionTimeoutError4, APIUserAbortError: APIUserAbortError4, NotFoundError: NotFoundError4, ConflictError: ConflictError4, RateLimitError: RateLimitError4, BadRequestError: BadRequestError4, AuthenticationError: AuthenticationError4, InternalServerError: InternalServerError4, PermissionDeniedError: PermissionDeniedError4, UnprocessableEntityError: UnprocessableEntityError4 } = error_exports2;
(function(Anthropic2) {
  Anthropic2.Completions = Completions4;
  Anthropic2.Messages = Messages3;
  Anthropic2.Beta = Beta2;
})(Anthropic || (Anthropic = {}));
var sdk_default = Anthropic;

// src/Plugin/Components/SingletonNotice.ts
var import_obsidian = require("obsidian");
var _SingletonNotice = class {
  static show(message, duration = 4e3) {
    if (_SingletonNotice.activeNotice)
      return;
    _SingletonNotice.activeNotice = new import_obsidian.Notice(message, duration);
    setTimeout(() => {
      _SingletonNotice.activeNotice = null;
    }, duration);
  }
};
var SingletonNotice = _SingletonNotice;
SingletonNotice.activeNotice = null;

// node_modules/@google/generative-ai/dist/index.mjs
var POSSIBLE_ROLES = ["user", "model", "function", "system"];
var HarmCategory;
(function(HarmCategory2) {
  HarmCategory2["HARM_CATEGORY_UNSPECIFIED"] = "HARM_CATEGORY_UNSPECIFIED";
  HarmCategory2["HARM_CATEGORY_HATE_SPEECH"] = "HARM_CATEGORY_HATE_SPEECH";
  HarmCategory2["HARM_CATEGORY_SEXUALLY_EXPLICIT"] = "HARM_CATEGORY_SEXUALLY_EXPLICIT";
  HarmCategory2["HARM_CATEGORY_HARASSMENT"] = "HARM_CATEGORY_HARASSMENT";
  HarmCategory2["HARM_CATEGORY_DANGEROUS_CONTENT"] = "HARM_CATEGORY_DANGEROUS_CONTENT";
})(HarmCategory || (HarmCategory = {}));
var HarmBlockThreshold;
(function(HarmBlockThreshold2) {
  HarmBlockThreshold2["HARM_BLOCK_THRESHOLD_UNSPECIFIED"] = "HARM_BLOCK_THRESHOLD_UNSPECIFIED";
  HarmBlockThreshold2["BLOCK_LOW_AND_ABOVE"] = "BLOCK_LOW_AND_ABOVE";
  HarmBlockThreshold2["BLOCK_MEDIUM_AND_ABOVE"] = "BLOCK_MEDIUM_AND_ABOVE";
  HarmBlockThreshold2["BLOCK_ONLY_HIGH"] = "BLOCK_ONLY_HIGH";
  HarmBlockThreshold2["BLOCK_NONE"] = "BLOCK_NONE";
})(HarmBlockThreshold || (HarmBlockThreshold = {}));
var HarmProbability;
(function(HarmProbability2) {
  HarmProbability2["HARM_PROBABILITY_UNSPECIFIED"] = "HARM_PROBABILITY_UNSPECIFIED";
  HarmProbability2["NEGLIGIBLE"] = "NEGLIGIBLE";
  HarmProbability2["LOW"] = "LOW";
  HarmProbability2["MEDIUM"] = "MEDIUM";
  HarmProbability2["HIGH"] = "HIGH";
})(HarmProbability || (HarmProbability = {}));
var BlockReason;
(function(BlockReason2) {
  BlockReason2["BLOCKED_REASON_UNSPECIFIED"] = "BLOCKED_REASON_UNSPECIFIED";
  BlockReason2["SAFETY"] = "SAFETY";
  BlockReason2["OTHER"] = "OTHER";
})(BlockReason || (BlockReason = {}));
var FinishReason;
(function(FinishReason2) {
  FinishReason2["FINISH_REASON_UNSPECIFIED"] = "FINISH_REASON_UNSPECIFIED";
  FinishReason2["STOP"] = "STOP";
  FinishReason2["MAX_TOKENS"] = "MAX_TOKENS";
  FinishReason2["SAFETY"] = "SAFETY";
  FinishReason2["RECITATION"] = "RECITATION";
  FinishReason2["OTHER"] = "OTHER";
})(FinishReason || (FinishReason = {}));
var TaskType;
(function(TaskType2) {
  TaskType2["TASK_TYPE_UNSPECIFIED"] = "TASK_TYPE_UNSPECIFIED";
  TaskType2["RETRIEVAL_QUERY"] = "RETRIEVAL_QUERY";
  TaskType2["RETRIEVAL_DOCUMENT"] = "RETRIEVAL_DOCUMENT";
  TaskType2["SEMANTIC_SIMILARITY"] = "SEMANTIC_SIMILARITY";
  TaskType2["CLASSIFICATION"] = "CLASSIFICATION";
  TaskType2["CLUSTERING"] = "CLUSTERING";
})(TaskType || (TaskType = {}));
var FunctionCallingMode;
(function(FunctionCallingMode2) {
  FunctionCallingMode2["MODE_UNSPECIFIED"] = "MODE_UNSPECIFIED";
  FunctionCallingMode2["AUTO"] = "AUTO";
  FunctionCallingMode2["ANY"] = "ANY";
  FunctionCallingMode2["NONE"] = "NONE";
})(FunctionCallingMode || (FunctionCallingMode = {}));
var FunctionDeclarationSchemaType;
(function(FunctionDeclarationSchemaType2) {
  FunctionDeclarationSchemaType2["STRING"] = "STRING";
  FunctionDeclarationSchemaType2["NUMBER"] = "NUMBER";
  FunctionDeclarationSchemaType2["INTEGER"] = "INTEGER";
  FunctionDeclarationSchemaType2["BOOLEAN"] = "BOOLEAN";
  FunctionDeclarationSchemaType2["ARRAY"] = "ARRAY";
  FunctionDeclarationSchemaType2["OBJECT"] = "OBJECT";
})(FunctionDeclarationSchemaType || (FunctionDeclarationSchemaType = {}));
var GoogleGenerativeAIError = class extends Error {
  constructor(message) {
    super(`[GoogleGenerativeAI Error]: ${message}`);
  }
};
var GoogleGenerativeAIResponseError = class extends GoogleGenerativeAIError {
  constructor(message, response) {
    super(message);
    this.response = response;
  }
};
var DEFAULT_BASE_URL = "https://generativelanguage.googleapis.com";
var DEFAULT_API_VERSION = "v1beta";
var PACKAGE_VERSION = "0.8.0";
var PACKAGE_LOG_HEADER = "genai-js";
var Task;
(function(Task2) {
  Task2["GENERATE_CONTENT"] = "generateContent";
  Task2["STREAM_GENERATE_CONTENT"] = "streamGenerateContent";
  Task2["COUNT_TOKENS"] = "countTokens";
  Task2["EMBED_CONTENT"] = "embedContent";
  Task2["BATCH_EMBED_CONTENTS"] = "batchEmbedContents";
})(Task || (Task = {}));
var RequestUrl = class {
  constructor(model, task, apiKey, stream, requestOptions) {
    this.model = model;
    this.task = task;
    this.apiKey = apiKey;
    this.stream = stream;
    this.requestOptions = requestOptions;
  }
  toString() {
    var _a3, _b;
    const apiVersion = ((_a3 = this.requestOptions) === null || _a3 === void 0 ? void 0 : _a3.apiVersion) || DEFAULT_API_VERSION;
    const baseUrl = ((_b = this.requestOptions) === null || _b === void 0 ? void 0 : _b.baseUrl) || DEFAULT_BASE_URL;
    let url = `${baseUrl}/${apiVersion}/${this.model}:${this.task}`;
    if (this.stream) {
      url += "?alt=sse";
    }
    return url;
  }
};
function getClientHeaders(requestOptions) {
  const clientHeaders = [];
  if (requestOptions === null || requestOptions === void 0 ? void 0 : requestOptions.apiClient) {
    clientHeaders.push(requestOptions.apiClient);
  }
  clientHeaders.push(`${PACKAGE_LOG_HEADER}/${PACKAGE_VERSION}`);
  return clientHeaders.join(" ");
}
async function getHeaders(url) {
  const headers = new Headers();
  headers.append("Content-Type", "application/json");
  headers.append("x-goog-api-client", getClientHeaders(url.requestOptions));
  headers.append("x-goog-api-key", url.apiKey);
  return headers;
}
async function constructRequest(model, task, apiKey, stream, body, requestOptions) {
  const url = new RequestUrl(model, task, apiKey, stream, requestOptions);
  return {
    url: url.toString(),
    fetchOptions: Object.assign(Object.assign({}, buildFetchOptions(requestOptions)), { method: "POST", headers: await getHeaders(url), body })
  };
}
async function makeRequest(model, task, apiKey, stream, body, requestOptions) {
  return _makeRequestInternal(model, task, apiKey, stream, body, requestOptions, fetch);
}
async function _makeRequestInternal(model, task, apiKey, stream, body, requestOptions, fetchFn = fetch) {
  const url = new RequestUrl(model, task, apiKey, stream, requestOptions);
  let response;
  try {
    const request = await constructRequest(model, task, apiKey, stream, body, requestOptions);
    response = await fetchFn(request.url, request.fetchOptions);
    if (!response.ok) {
      let message = "";
      try {
        const json = await response.json();
        message = json.error.message;
        if (json.error.details) {
          message += ` ${JSON.stringify(json.error.details)}`;
        }
      } catch (e) {
      }
      throw new Error(`[${response.status} ${response.statusText}] ${message}`);
    }
  } catch (e) {
    const err = new GoogleGenerativeAIError(`Error fetching from ${url.toString()}: ${e.message}`);
    err.stack = e.stack;
    throw err;
  }
  return response;
}
function buildFetchOptions(requestOptions) {
  const fetchOptions = {};
  if ((requestOptions === null || requestOptions === void 0 ? void 0 : requestOptions.timeout) >= 0) {
    const abortController = new AbortController();
    const signal = abortController.signal;
    setTimeout(() => abortController.abort(), requestOptions.timeout);
    fetchOptions.signal = signal;
  }
  return fetchOptions;
}
function addHelpers(response) {
  response.text = () => {
    if (response.candidates && response.candidates.length > 0) {
      if (response.candidates.length > 1) {
        console.warn(`This response had ${response.candidates.length} candidates. Returning text from the first candidate only. Access response.candidates directly to use the other candidates.`);
      }
      if (hadBadFinishReason(response.candidates[0])) {
        throw new GoogleGenerativeAIResponseError(`${formatBlockErrorMessage(response)}`, response);
      }
      return getText(response);
    } else if (response.promptFeedback) {
      throw new GoogleGenerativeAIResponseError(`Text not available. ${formatBlockErrorMessage(response)}`, response);
    }
    return "";
  };
  response.functionCall = () => {
    if (response.candidates && response.candidates.length > 0) {
      if (response.candidates.length > 1) {
        console.warn(`This response had ${response.candidates.length} candidates. Returning function calls from the first candidate only. Access response.candidates directly to use the other candidates.`);
      }
      if (hadBadFinishReason(response.candidates[0])) {
        throw new GoogleGenerativeAIResponseError(`${formatBlockErrorMessage(response)}`, response);
      }
      console.warn(`response.functionCall() is deprecated. Use response.functionCalls() instead.`);
      return getFunctionCalls(response)[0];
    } else if (response.promptFeedback) {
      throw new GoogleGenerativeAIResponseError(`Function call not available. ${formatBlockErrorMessage(response)}`, response);
    }
    return void 0;
  };
  response.functionCalls = () => {
    if (response.candidates && response.candidates.length > 0) {
      if (response.candidates.length > 1) {
        console.warn(`This response had ${response.candidates.length} candidates. Returning function calls from the first candidate only. Access response.candidates directly to use the other candidates.`);
      }
      if (hadBadFinishReason(response.candidates[0])) {
        throw new GoogleGenerativeAIResponseError(`${formatBlockErrorMessage(response)}`, response);
      }
      return getFunctionCalls(response);
    } else if (response.promptFeedback) {
      throw new GoogleGenerativeAIResponseError(`Function call not available. ${formatBlockErrorMessage(response)}`, response);
    }
    return void 0;
  };
  return response;
}
function getText(response) {
  var _a3, _b, _c, _d;
  if ((_d = (_c = (_b = (_a3 = response.candidates) === null || _a3 === void 0 ? void 0 : _a3[0].content) === null || _b === void 0 ? void 0 : _b.parts) === null || _c === void 0 ? void 0 : _c[0]) === null || _d === void 0 ? void 0 : _d.text) {
    return response.candidates[0].content.parts.map(({ text }) => text).join("");
  } else {
    return "";
  }
}
function getFunctionCalls(response) {
  var _a3, _b, _c, _d;
  const functionCalls = [];
  if ((_b = (_a3 = response.candidates) === null || _a3 === void 0 ? void 0 : _a3[0].content) === null || _b === void 0 ? void 0 : _b.parts) {
    for (const part of (_d = (_c = response.candidates) === null || _c === void 0 ? void 0 : _c[0].content) === null || _d === void 0 ? void 0 : _d.parts) {
      if (part.functionCall) {
        functionCalls.push(part.functionCall);
      }
    }
  }
  if (functionCalls.length > 0) {
    return functionCalls;
  } else {
    return void 0;
  }
}
var badFinishReasons = [FinishReason.RECITATION, FinishReason.SAFETY];
function hadBadFinishReason(candidate) {
  return !!candidate.finishReason && badFinishReasons.includes(candidate.finishReason);
}
function formatBlockErrorMessage(response) {
  var _a3, _b, _c;
  let message = "";
  if ((!response.candidates || response.candidates.length === 0) && response.promptFeedback) {
    message += "Response was blocked";
    if ((_a3 = response.promptFeedback) === null || _a3 === void 0 ? void 0 : _a3.blockReason) {
      message += ` due to ${response.promptFeedback.blockReason}`;
    }
    if ((_b = response.promptFeedback) === null || _b === void 0 ? void 0 : _b.blockReasonMessage) {
      message += `: ${response.promptFeedback.blockReasonMessage}`;
    }
  } else if ((_c = response.candidates) === null || _c === void 0 ? void 0 : _c[0]) {
    const firstCandidate = response.candidates[0];
    if (hadBadFinishReason(firstCandidate)) {
      message += `Candidate was blocked due to ${firstCandidate.finishReason}`;
      if (firstCandidate.finishMessage) {
        message += `: ${firstCandidate.finishMessage}`;
      }
    }
  }
  return message;
}
function __await(v) {
  return this instanceof __await ? (this.v = v, this) : new __await(v);
}
function __asyncGenerator(thisArg, _arguments, generator) {
  if (!Symbol.asyncIterator)
    throw new TypeError("Symbol.asyncIterator is not defined.");
  var g = generator.apply(thisArg, _arguments || []), i, q = [];
  return i = {}, verb("next"), verb("throw"), verb("return"), i[Symbol.asyncIterator] = function() {
    return this;
  }, i;
  function verb(n) {
    if (g[n])
      i[n] = function(v) {
        return new Promise(function(a, b) {
          q.push([n, v, a, b]) > 1 || resume(n, v);
        });
      };
  }
  function resume(n, v) {
    try {
      step(g[n](v));
    } catch (e) {
      settle(q[0][3], e);
    }
  }
  function step(r) {
    r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r);
  }
  function fulfill(value) {
    resume("next", value);
  }
  function reject(value) {
    resume("throw", value);
  }
  function settle(f, v) {
    if (f(v), q.shift(), q.length)
      resume(q[0][0], q[0][1]);
  }
}
var responseLineRE = /^data\: (.*)(?:\n\n|\r\r|\r\n\r\n)/;
function processStream(response) {
  const inputStream = response.body.pipeThrough(new TextDecoderStream("utf8", { fatal: true }));
  const responseStream = getResponseStream(inputStream);
  const [stream1, stream2] = responseStream.tee();
  return {
    stream: generateResponseSequence(stream1),
    response: getResponsePromise(stream2)
  };
}
async function getResponsePromise(stream) {
  const allResponses = [];
  const reader = stream.getReader();
  while (true) {
    const { done, value } = await reader.read();
    if (done) {
      return addHelpers(aggregateResponses(allResponses));
    }
    allResponses.push(value);
  }
}
function generateResponseSequence(stream) {
  return __asyncGenerator(this, arguments, function* generateResponseSequence_1() {
    const reader = stream.getReader();
    while (true) {
      const { value, done } = yield __await(reader.read());
      if (done) {
        break;
      }
      yield yield __await(addHelpers(value));
    }
  });
}
function getResponseStream(inputStream) {
  const reader = inputStream.getReader();
  const stream = new ReadableStream({
    start(controller) {
      let currentText = "";
      return pump();
      function pump() {
        return reader.read().then(({ value, done }) => {
          if (done) {
            if (currentText.trim()) {
              controller.error(new GoogleGenerativeAIError("Failed to parse stream"));
              return;
            }
            controller.close();
            return;
          }
          currentText += value;
          let match = currentText.match(responseLineRE);
          let parsedResponse;
          while (match) {
            try {
              parsedResponse = JSON.parse(match[1]);
            } catch (e) {
              controller.error(new GoogleGenerativeAIError(`Error parsing JSON response: "${match[1]}"`));
              return;
            }
            controller.enqueue(parsedResponse);
            currentText = currentText.substring(match[0].length);
            match = currentText.match(responseLineRE);
          }
          return pump();
        });
      }
    }
  });
  return stream;
}
function aggregateResponses(responses) {
  const lastResponse = responses[responses.length - 1];
  const aggregatedResponse = {
    promptFeedback: lastResponse === null || lastResponse === void 0 ? void 0 : lastResponse.promptFeedback
  };
  for (const response of responses) {
    if (response.candidates) {
      for (const candidate of response.candidates) {
        const i = candidate.index;
        if (!aggregatedResponse.candidates) {
          aggregatedResponse.candidates = [];
        }
        if (!aggregatedResponse.candidates[i]) {
          aggregatedResponse.candidates[i] = {
            index: candidate.index
          };
        }
        aggregatedResponse.candidates[i].citationMetadata = candidate.citationMetadata;
        aggregatedResponse.candidates[i].finishReason = candidate.finishReason;
        aggregatedResponse.candidates[i].finishMessage = candidate.finishMessage;
        aggregatedResponse.candidates[i].safetyRatings = candidate.safetyRatings;
        if (candidate.content && candidate.content.parts) {
          if (!aggregatedResponse.candidates[i].content) {
            aggregatedResponse.candidates[i].content = {
              role: candidate.content.role || "user",
              parts: []
            };
          }
          const newPart = {};
          for (const part of candidate.content.parts) {
            if (part.text) {
              newPart.text = part.text;
            }
            if (part.functionCall) {
              newPart.functionCall = part.functionCall;
            }
            if (Object.keys(newPart).length === 0) {
              newPart.text = "";
            }
            aggregatedResponse.candidates[i].content.parts.push(newPart);
          }
        }
      }
    }
  }
  return aggregatedResponse;
}
async function generateContentStream(apiKey, model, params, requestOptions) {
  const response = await makeRequest(
    model,
    Task.STREAM_GENERATE_CONTENT,
    apiKey,
    /* stream */
    true,
    JSON.stringify(params),
    requestOptions
  );
  return processStream(response);
}
async function generateContent(apiKey, model, params, requestOptions) {
  const response = await makeRequest(
    model,
    Task.GENERATE_CONTENT,
    apiKey,
    /* stream */
    false,
    JSON.stringify(params),
    requestOptions
  );
  const responseJson = await response.json();
  const enhancedResponse = addHelpers(responseJson);
  return {
    response: enhancedResponse
  };
}
function formatNewContent(request) {
  let newParts = [];
  if (typeof request === "string") {
    newParts = [{ text: request }];
  } else {
    for (const partOrString of request) {
      if (typeof partOrString === "string") {
        newParts.push({ text: partOrString });
      } else {
        newParts.push(partOrString);
      }
    }
  }
  return assignRoleToPartsAndValidateSendMessageRequest(newParts);
}
function assignRoleToPartsAndValidateSendMessageRequest(parts) {
  const userContent = { role: "user", parts: [] };
  const functionContent = { role: "function", parts: [] };
  let hasUserContent = false;
  let hasFunctionContent = false;
  for (const part of parts) {
    if ("functionResponse" in part) {
      functionContent.parts.push(part);
      hasFunctionContent = true;
    } else {
      userContent.parts.push(part);
      hasUserContent = true;
    }
  }
  if (hasUserContent && hasFunctionContent) {
    throw new GoogleGenerativeAIError("Within a single message, FunctionResponse cannot be mixed with other type of part in the request for sending chat message.");
  }
  if (!hasUserContent && !hasFunctionContent) {
    throw new GoogleGenerativeAIError("No content is provided for sending chat message.");
  }
  if (hasUserContent) {
    return userContent;
  }
  return functionContent;
}
function formatGenerateContentInput(params) {
  if (params.contents) {
    return params;
  } else {
    const content = formatNewContent(params);
    return { contents: [content] };
  }
}
function formatEmbedContentInput(params) {
  if (typeof params === "string" || Array.isArray(params)) {
    const content = formatNewContent(params);
    return { content };
  }
  return params;
}
var VALID_PART_FIELDS = [
  "text",
  "inlineData",
  "functionCall",
  "functionResponse"
];
var VALID_PARTS_PER_ROLE = {
  user: ["text", "inlineData"],
  function: ["functionResponse"],
  model: ["text", "functionCall"],
  // System instructions shouldn't be in history anyway.
  system: ["text"]
};
var VALID_PREVIOUS_CONTENT_ROLES = {
  user: ["model"],
  function: ["model"],
  model: ["user", "function"],
  // System instructions shouldn't be in history.
  system: []
};
function validateChatHistory(history) {
  let prevContent;
  for (const currContent of history) {
    const { role, parts } = currContent;
    if (!prevContent && role !== "user") {
      throw new GoogleGenerativeAIError(`First content should be with role 'user', got ${role}`);
    }
    if (!POSSIBLE_ROLES.includes(role)) {
      throw new GoogleGenerativeAIError(`Each item should include role field. Got ${role} but valid roles are: ${JSON.stringify(POSSIBLE_ROLES)}`);
    }
    if (!Array.isArray(parts)) {
      throw new GoogleGenerativeAIError("Content should have 'parts' property with an array of Parts");
    }
    if (parts.length === 0) {
      throw new GoogleGenerativeAIError("Each Content should have at least one part");
    }
    const countFields = {
      text: 0,
      inlineData: 0,
      functionCall: 0,
      functionResponse: 0,
      fileData: 0
    };
    for (const part of parts) {
      for (const key of VALID_PART_FIELDS) {
        if (key in part) {
          countFields[key] += 1;
        }
      }
    }
    const validParts = VALID_PARTS_PER_ROLE[role];
    for (const key of VALID_PART_FIELDS) {
      if (!validParts.includes(key) && countFields[key] > 0) {
        throw new GoogleGenerativeAIError(`Content with role '${role}' can't contain '${key}' part`);
      }
    }
    if (prevContent) {
      const validPreviousContentRoles = VALID_PREVIOUS_CONTENT_ROLES[role];
      if (!validPreviousContentRoles.includes(prevContent.role)) {
        throw new GoogleGenerativeAIError(`Content with role '${role}' can't follow '${prevContent.role}'. Valid previous roles: ${JSON.stringify(VALID_PREVIOUS_CONTENT_ROLES)}`);
      }
    }
    prevContent = currContent;
  }
}
var SILENT_ERROR = "SILENT_ERROR";
var ChatSession = class {
  constructor(apiKey, model, params, requestOptions) {
    this.model = model;
    this.params = params;
    this.requestOptions = requestOptions;
    this._history = [];
    this._sendPromise = Promise.resolve();
    this._apiKey = apiKey;
    if (params === null || params === void 0 ? void 0 : params.history) {
      validateChatHistory(params.history);
      this._history = params.history;
    }
  }
  /**
   * Gets the chat history so far. Blocked prompts are not added to history.
   * Blocked candidates are not added to history, nor are the prompts that
   * generated them.
   */
  async getHistory() {
    await this._sendPromise;
    return this._history;
  }
  /**
   * Sends a chat message and receives a non-streaming
   * {@link GenerateContentResult}
   */
  async sendMessage(request) {
    var _a3, _b, _c, _d, _e;
    await this._sendPromise;
    const newContent = formatNewContent(request);
    const generateContentRequest = {
      safetySettings: (_a3 = this.params) === null || _a3 === void 0 ? void 0 : _a3.safetySettings,
      generationConfig: (_b = this.params) === null || _b === void 0 ? void 0 : _b.generationConfig,
      tools: (_c = this.params) === null || _c === void 0 ? void 0 : _c.tools,
      toolConfig: (_d = this.params) === null || _d === void 0 ? void 0 : _d.toolConfig,
      systemInstruction: (_e = this.params) === null || _e === void 0 ? void 0 : _e.systemInstruction,
      contents: [...this._history, newContent]
    };
    let finalResult;
    this._sendPromise = this._sendPromise.then(() => generateContent(this._apiKey, this.model, generateContentRequest, this.requestOptions)).then((result) => {
      var _a4;
      if (result.response.candidates && result.response.candidates.length > 0) {
        this._history.push(newContent);
        const responseContent = Object.assign({
          parts: [],
          // Response seems to come back without a role set.
          role: "model"
        }, (_a4 = result.response.candidates) === null || _a4 === void 0 ? void 0 : _a4[0].content);
        this._history.push(responseContent);
      } else {
        const blockErrorMessage = formatBlockErrorMessage(result.response);
        if (blockErrorMessage) {
          console.warn(`sendMessage() was unsuccessful. ${blockErrorMessage}. Inspect response object for details.`);
        }
      }
      finalResult = result;
    });
    await this._sendPromise;
    return finalResult;
  }
  /**
   * Sends a chat message and receives the response as a
   * {@link GenerateContentStreamResult} containing an iterable stream
   * and a response promise.
   */
  async sendMessageStream(request) {
    var _a3, _b, _c, _d, _e;
    await this._sendPromise;
    const newContent = formatNewContent(request);
    const generateContentRequest = {
      safetySettings: (_a3 = this.params) === null || _a3 === void 0 ? void 0 : _a3.safetySettings,
      generationConfig: (_b = this.params) === null || _b === void 0 ? void 0 : _b.generationConfig,
      tools: (_c = this.params) === null || _c === void 0 ? void 0 : _c.tools,
      toolConfig: (_d = this.params) === null || _d === void 0 ? void 0 : _d.toolConfig,
      systemInstruction: (_e = this.params) === null || _e === void 0 ? void 0 : _e.systemInstruction,
      contents: [...this._history, newContent]
    };
    const streamPromise = generateContentStream(this._apiKey, this.model, generateContentRequest, this.requestOptions);
    this._sendPromise = this._sendPromise.then(() => streamPromise).catch((_ignored) => {
      throw new Error(SILENT_ERROR);
    }).then((streamResult) => streamResult.response).then((response) => {
      if (response.candidates && response.candidates.length > 0) {
        this._history.push(newContent);
        const responseContent = Object.assign({}, response.candidates[0].content);
        if (!responseContent.role) {
          responseContent.role = "model";
        }
        this._history.push(responseContent);
      } else {
        const blockErrorMessage = formatBlockErrorMessage(response);
        if (blockErrorMessage) {
          console.warn(`sendMessageStream() was unsuccessful. ${blockErrorMessage}. Inspect response object for details.`);
        }
      }
    }).catch((e) => {
      if (e.message !== SILENT_ERROR) {
        console.error(e);
      }
    });
    return streamPromise;
  }
};
async function countTokens(apiKey, model, params, requestOptions) {
  const response = await makeRequest(model, Task.COUNT_TOKENS, apiKey, false, JSON.stringify(Object.assign(Object.assign({}, params), { model })), requestOptions);
  return response.json();
}
async function embedContent(apiKey, model, params, requestOptions) {
  const response = await makeRequest(model, Task.EMBED_CONTENT, apiKey, false, JSON.stringify(params), requestOptions);
  return response.json();
}
async function batchEmbedContents(apiKey, model, params, requestOptions) {
  const requestsWithModel = params.requests.map((request) => {
    return Object.assign(Object.assign({}, request), { model });
  });
  const response = await makeRequest(model, Task.BATCH_EMBED_CONTENTS, apiKey, false, JSON.stringify({ requests: requestsWithModel }), requestOptions);
  return response.json();
}
var GenerativeModel = class {
  constructor(apiKey, modelParams, requestOptions) {
    this.apiKey = apiKey;
    if (modelParams.model.includes("/")) {
      this.model = modelParams.model;
    } else {
      this.model = `models/${modelParams.model}`;
    }
    this.generationConfig = modelParams.generationConfig || {};
    this.safetySettings = modelParams.safetySettings || [];
    this.tools = modelParams.tools;
    this.toolConfig = modelParams.toolConfig;
    this.systemInstruction = modelParams.systemInstruction;
    this.requestOptions = requestOptions || {};
  }
  /**
   * Makes a single non-streaming call to the model
   * and returns an object containing a single {@link GenerateContentResponse}.
   */
  async generateContent(request) {
    const formattedParams = formatGenerateContentInput(request);
    return generateContent(this.apiKey, this.model, Object.assign({ generationConfig: this.generationConfig, safetySettings: this.safetySettings, tools: this.tools, toolConfig: this.toolConfig, systemInstruction: this.systemInstruction }, formattedParams), this.requestOptions);
  }
  /**
   * Makes a single streaming call to the model
   * and returns an object containing an iterable stream that iterates
   * over all chunks in the streaming response as well as
   * a promise that returns the final aggregated response.
   */
  async generateContentStream(request) {
    const formattedParams = formatGenerateContentInput(request);
    return generateContentStream(this.apiKey, this.model, Object.assign({ generationConfig: this.generationConfig, safetySettings: this.safetySettings, tools: this.tools, toolConfig: this.toolConfig, systemInstruction: this.systemInstruction }, formattedParams), this.requestOptions);
  }
  /**
   * Gets a new {@link ChatSession} instance which can be used for
   * multi-turn chats.
   */
  startChat(startChatParams) {
    return new ChatSession(this.apiKey, this.model, Object.assign({ generationConfig: this.generationConfig, safetySettings: this.safetySettings, tools: this.tools, toolConfig: this.toolConfig, systemInstruction: this.systemInstruction }, startChatParams), this.requestOptions);
  }
  /**
   * Counts the tokens in the provided request.
   */
  async countTokens(request) {
    const formattedParams = formatGenerateContentInput(request);
    return countTokens(this.apiKey, this.model, formattedParams, this.requestOptions);
  }
  /**
   * Embeds the provided content.
   */
  async embedContent(request) {
    const formattedParams = formatEmbedContentInput(request);
    return embedContent(this.apiKey, this.model, formattedParams, this.requestOptions);
  }
  /**
   * Embeds an array of {@link EmbedContentRequest}s.
   */
  async batchEmbedContents(batchEmbedContentRequest) {
    return batchEmbedContents(this.apiKey, this.model, batchEmbedContentRequest, this.requestOptions);
  }
};
var GoogleGenerativeAI = class {
  constructor(apiKey) {
    this.apiKey = apiKey;
  }
  /**
   * Gets a {@link GenerativeModel} instance for the provided model name.
   */
  getGenerativeModel(modelParams, requestOptions) {
    if (!modelParams.model) {
      throw new GoogleGenerativeAIError(`Must provide a model name. Example: genai.getGenerativeModel({ model: 'my-model-name' })`);
    }
    return new GenerativeModel(this.apiKey, modelParams, requestOptions);
  }
};

// src/utils/utils.ts
function getGpt4AllPath(plugin) {
  const platform = plugin.os.platform();
  const homedir = plugin.os.homedir();
  if (platform === "win32") {
    return `${homedir}\\AppData\\Local\\nomic.ai\\GPT4All`;
  } else if (platform === "linux") {
    return `${homedir}/gpt4all`;
  } else {
    return `${homedir}/Library/Application Support/nomic.ai/GPT4All`;
  }
}
function upperCaseFirst(input) {
  if (input.length === 0)
    return input;
  return input.charAt(0).toUpperCase() + input.slice(1);
}
async function messageGPT4AllServer(params, url) {
  const request = {
    url: `http://localhost:4891${url}`,
    method: "POST",
    body: JSON.stringify({
      model: params.model,
      messages: params.messages,
      max_tokens: params.tokens,
      temperature: params.temperature
    })
  };
  const response = await (0, import_obsidian2.requestUrl)(request).then((res) => res.json);
  return response.choices[0].message;
}
async function getApiKeyValidity(providerKeyPair) {
  try {
    const { key, provider } = providerKeyPair;
    if (provider === openAI) {
      const openaiClient = new openai_default({
        apiKey: key,
        dangerouslyAllowBrowser: true
      });
      await openaiClient.models.list();
      return { provider, valid: true };
    } else if (provider === claude) {
      const client = new sdk_default({
        apiKey: key,
        dangerouslyAllowBrowser: true
      });
      await client.messages.create({
        model: claudeSonnetJuneModel,
        max_tokens: 1,
        messages: [{ role: "user", content: "Reply 'a'" }]
      });
      return { provider, valid: true };
    } else if (provider === gemini) {
      const client = new GoogleGenerativeAI(key);
      const model = client.getGenerativeModel({
        model: geminiModel,
        generationConfig: {
          candidateCount: 1,
          maxOutputTokens: 1
        }
      });
      await model.generateContent("Reply 'a'");
      return { provider, valid: true };
    }
  } catch (error) {
    if (error.status === 401) {
      console.error(`Invalid API key for ${providerKeyPair.provider}.`);
      SingletonNotice.show(
        `Invalid API key for ${upperCaseFirst(
          providerKeyPair.provider
        )}.`
      );
    } else {
      console.log("An error occurred:", error.message);
    }
    return false;
  }
}
async function geminiMessage(params, Gemini_API_KEY) {
  const { model, topP, messages: messages2, tokens, temperature } = params;
  const genAI = new GoogleGenerativeAI(Gemini_API_KEY);
  const client = genAI.getGenerativeModel({
    model,
    generationConfig: {
      candidateCount: 1,
      maxOutputTokens: tokens,
      temperature,
      topP: topP != null ? topP : void 0
    }
  });
  const contents = messages2.map((message) => {
    const role = message.role === "user" ? "user" : "model";
    return {
      role,
      parts: [{ text: message.content }]
      // Convert content to Part[]
    };
  });
  const generateContentRequest = { contents };
  const stream = await client.generateContentStream(generateContentRequest);
  return stream;
}
async function claudeMessage(params, Claude_API_KEY) {
  const client = new sdk_default({
    apiKey: Claude_API_KEY,
    dangerouslyAllowBrowser: true
  });
  const { model, messages: messages2, tokens, temperature } = params;
  const stream = client.messages.stream({
    model,
    messages: messages2,
    max_tokens: tokens,
    temperature,
    stream: true
  });
  return stream;
}
async function openAIMessage(params, OpenAI_API_Key, endpoint, endpointType) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  if (endpointType === chat) {
    const { model, messages: messages2, tokens, temperature } = params;
    const stream = await openai.chat.completions.create(
      {
        model,
        messages: messages2,
        max_tokens: tokens,
        temperature,
        stream: true
      },
      { path: endpoint }
    );
    return stream;
  }
  if (endpointType === "images") {
    const {
      prompt,
      model,
      messages: messages2,
      quality,
      size,
      style,
      numberOfImages
    } = params;
    const image = await openai.images.generate({
      model,
      prompt,
      size,
      quality,
      n: numberOfImages,
      style
    });
    let imageURLs = [];
    image.data.map((image2) => {
      return imageURLs.push(image2.url);
    });
    return imageURLs;
  }
}
async function assistantsMessage(OpenAI_API_Key, messages2, assistant_id) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  const thread = await openai.beta.threads.create({
    messages: messages2
  });
  const stream = openai.beta.threads.runs.stream(thread.id, {
    assistant_id
  });
  return stream;
}
function getViewInfo(plugin, viewType) {
  if (viewType === "modal") {
    return {
      assistant: plugin.settings.modalSettings.assistant,
      assistantId: plugin.settings.modalSettings.assistantId,
      imageSettings: plugin.settings.modalSettings.imageSettings,
      chatSettings: plugin.settings.modalSettings.chatSettings,
      model: plugin.settings.modalSettings.model,
      modelName: plugin.settings.modalSettings.modelName,
      modelType: plugin.settings.modalSettings.modelType,
      historyIndex: plugin.settings.modalSettings.historyIndex,
      modelEndpoint: plugin.settings.modalSettings.modelEndpoint,
      endpointURL: plugin.settings.modalSettings.endpointURL
    };
  }
  if (viewType === "widget") {
    return {
      assistant: plugin.settings.widgetSettings.assistant,
      assistantId: plugin.settings.widgetSettings.assistantId,
      imageSettings: plugin.settings.widgetSettings.imageSettings,
      chatSettings: plugin.settings.widgetSettings.chatSettings,
      model: plugin.settings.widgetSettings.model,
      modelName: plugin.settings.widgetSettings.modelName,
      modelType: plugin.settings.widgetSettings.modelType,
      historyIndex: plugin.settings.widgetSettings.historyIndex,
      modelEndpoint: plugin.settings.widgetSettings.modelEndpoint,
      endpointURL: plugin.settings.widgetSettings.endpointURL
    };
  }
  if (viewType === "floating-action-button") {
    return {
      assistant: plugin.settings.fabSettings.assistant,
      assistantId: plugin.settings.fabSettings.assistantId,
      imageSettings: plugin.settings.fabSettings.imageSettings,
      chatSettings: plugin.settings.fabSettings.chatSettings,
      model: plugin.settings.fabSettings.model,
      modelName: plugin.settings.fabSettings.modelName,
      modelType: plugin.settings.fabSettings.modelType,
      historyIndex: plugin.settings.fabSettings.historyIndex,
      modelEndpoint: plugin.settings.fabSettings.modelEndpoint,
      endpointURL: plugin.settings.fabSettings.endpointURL
    };
  }
  return {
    assistant: false,
    assistantId: "",
    imageSettings: {
      numberOfImages: 0,
      response_format: "url",
      size: "1024x1024",
      style: "natural",
      quality: "standard"
    },
    chatSettings: { maxTokens: 0, temperature: 0 },
    model: "",
    modelName: "",
    modelType: "",
    historyIndex: -1,
    modelEndpoint: "",
    endpointURL: ""
  };
}
function changeDefaultModel(model, plugin) {
  plugin.settings.defaultModel = model;
  const modelName = modelNames[model];
  plugin.settings.modalSettings.model = model;
  plugin.settings.modalSettings.modelName = modelName;
  plugin.settings.modalSettings.modelType = models[modelName].type;
  plugin.settings.modalSettings.endpointURL = models[modelName].url;
  plugin.settings.modalSettings.modelEndpoint = models[modelName].endpoint;
  plugin.settings.widgetSettings.model = model;
  plugin.settings.widgetSettings.modelName = modelName;
  plugin.settings.widgetSettings.modelType = models[modelName].type;
  plugin.settings.widgetSettings.endpointURL = models[modelName].url;
  plugin.settings.widgetSettings.modelEndpoint = models[modelName].endpoint;
  plugin.saveSettings();
}
function setHistoryIndex(plugin, viewType, length) {
  const settings = {
    modal: "modalSettings",
    widget: "widgetSettings",
    "floating-action-button": "fabSettings"
  };
  const settingType = settings[viewType];
  if (!length) {
    plugin.settings[settingType].historyIndex = -1;
    plugin.saveSettings();
    return;
  }
  plugin.settings[settingType].historyIndex = length - 1;
  plugin.saveSettings();
}
function setView(plugin, viewType) {
  plugin.settings.currentView = viewType;
  plugin.saveSettings();
}
function getSettingType(viewType) {
  const settings = {
    modal: "modalSettings",
    widget: "widgetSettings",
    "floating-action-button": "fabSettings"
  };
  const settingType = settings[viewType];
  return settingType;
}
async function createAssistant(assistantObj, OpenAI_API_Key) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  const assistant2 = await openai.beta.assistants.create(assistantObj);
  return assistant2;
}
function getAssistant(plugin, assistant_id) {
  return plugin.settings.assistants.find(
    (assistant2) => assistant2.id === assistant_id
  );
}
async function listAssistants(OpenAI_API_Key) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  const myAssistants = await openai.beta.assistants.list();
  return myAssistants.data;
}
async function generateAssistantsList(settings) {
  const assisitantsFromOpenAI = await listAssistants(settings.openAIAPIKey);
  const processedAssisitants = assisitantsFromOpenAI.map(
    (assistant2) => ({
      ...assistant2,
      modelType: assistant2
    })
  );
  settings.assistants = processedAssisitants;
}
async function deleteAssistant(OpenAI_API_Key, assistant_id) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  await openai.beta.assistants.del(assistant_id);
}
async function listVectors(OpenAI_API_Key) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  const vectorStores = await openai.beta.vectorStores.list();
  return vectorStores.data;
}
async function deleteVector(OpenAI_API_Key, vector_id) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  await openai.beta.vectorStores.del(vector_id);
}
async function createVectorAndUpdate(files, assistant2, OpenAI_API_Key, fileSystem) {
  const openai = new openai_default({
    apiKey: OpenAI_API_Key,
    dangerouslyAllowBrowser: true
  });
  const file_ids = await Promise.all(
    files.map(async (filePath) => {
      const stream = await fileSystem.createReadStream(filePath);
      const reader = stream.getReader();
      const chunks = [];
      while (true) {
        const { done, value } = await reader.read();
        if (done)
          break;
        chunks.push(value);
      }
      const fileContent = new Uint8Array(chunks.flat());
      const fileToUpload = await toFile2(
        new Blob([fileContent]),
        filePath
      );
      const file = await openai.files.create({
        file: fileToUpload,
        purpose: "assistants"
      });
      return file.id;
    })
  );
  let vectorStore = await openai.beta.vectorStores.create({
    name: "Assistant Files"
  });
  await openai.beta.vectorStores.fileBatches.create(vectorStore.id, {
    file_ids
  });
  await openai.beta.assistants.update(assistant2.id, {
    tool_resources: { file_search: { vector_store_ids: [vectorStore.id] } }
  });
  return vectorStore.id;
}

// src/Plugin/Components/AssistantsContainer.ts
var AssistantsContainer = class {
  constructor(plugin, viewType) {
    this.plugin = plugin;
    this.viewType = viewType;
  }
  validateFields(fields) {
    const invalidFields = [];
    for (const [fieldName, value] of Object.entries(fields)) {
      if (!value) {
        invalidFields.push(fieldName);
      }
    }
    return invalidFields;
  }
  async generateAssistantsContainer(parentContainer) {
    const optionDropdown = new import_obsidian3.Setting(parentContainer).setName("Assistants options").setDesc("What do you want to do?").addDropdown((dropdown) => {
      dropdown.addOption("", "---Assistant options---");
      dropdown.addOption("asst_create", "Create an assistant");
      dropdown.addOption("asst_update", "Update an assistant");
      dropdown.addOption("asst_delete", "Delete an assistant");
      dropdown.addOption("", "---Vector storage options---");
      dropdown.addOption("vect_create", "Create vector storage");
      dropdown.addOption("vect_update", "Update vector storage");
      dropdown.addOption("vect_delete", "Delete vector storage");
      dropdown.onChange((change) => {
        this.resetContainer(parentContainer);
        switch (change) {
          case "asst_create":
            this.createAssistant(parentContainer);
            return;
          case "asst_update":
            this.updateAssistant(parentContainer);
            return;
          case "asst_delete":
            this.deleteAssistant(parentContainer);
            return;
          case "vect_create":
            this.createVector(parentContainer);
            return;
          case "vect_update":
            this.updateVector(parentContainer);
            return;
          case "vect_delete":
            this.deleteVector(parentContainer);
            return;
        }
      });
    });
  }
  // NOTE -> for both the create assistant flow we should dump the this.createAssistant name & other fields
  // after a successful submission event.
  createAssistant(parentContainer) {
    const file_ids = this.createSearch(
      parentContainer,
      assistant,
      true
    );
    this.filesSetting = file_ids;
    file_ids.settingEl.setAttr("style", "display:none");
    const buttonDiv = parentContainer.createDiv();
    buttonDiv.addClass(
      "llm-flex",
      "assistants-create-button-div",
      "setting-item"
    );
    const submitButton = new import_obsidian3.ButtonComponent(buttonDiv);
    submitButton.buttonEl.addClass("mod-cta", "llm-assistants-button");
    submitButton.buttonEl.textContent = "Create assistant";
    submitButton.onClick(async (e) => {
      var _a3, _b;
      const requiredFields = {
        "Name": this.createAssistantName,
        "Model": this.createAssistantModel
      };
      const invalidFields = this.validateFields(requiredFields);
      if (invalidFields.length > 0) {
        SingletonNotice.show(`Please fill out the following fields: ${invalidFields.join(", ")}`);
        return;
      }
      SingletonNotice.show("Creating assistant...");
      e.preventDefault();
      const assistantFiles = (_a3 = this.assistantFilesToAdd) == null ? void 0 : _a3.map((file) => {
        if (import_obsidian3.Platform.isMobile) {
          return file;
        } else {
          const slashToUse = this.plugin.os.platform() === "win32" ? "\\" : "/";
          const basePath = this.plugin.app.vault.adapter.basePath;
          return `${basePath}${slashToUse}${file}`;
        }
      });
      const hasFiles = ((_b = this.assistantFilesToAdd) == null ? void 0 : _b.length) >= 1;
      const assistantObj = {
        name: this.createAssistantName,
        instructions: this.createAssistantIntructions,
        model: this.createAssistantModel,
        tools: hasFiles ? [{ type: this.createAssistantToolType }] : null
      };
      const assistant2 = await createAssistant(
        assistantObj,
        this.plugin.settings.openAIAPIKey
      );
      if (hasFiles) {
        const vector_store_id = await createVectorAndUpdate(
          assistantFiles,
          assistant2,
          this.plugin.settings.openAIAPIKey,
          this.plugin.fileSystem
        );
        this.plugin.assistants.push({
          ...assistant2,
          modelType: assistant,
          tool_resources: {
            file_search: { vector_store_ids: [vector_store_id] }
          }
        });
      } else {
        this.plugin.assistants.push({
          ...assistant2,
          modelType: assistant
        });
      }
      if (assistant2) {
        new import_obsidian3.Notice("Assistant created successfully");
      }
      this.resetContainer(parentContainer);
    });
  }
  async updateAssistant(parentContainer) {
    const assistantsList = await listAssistants(
      this.plugin.settings.openAIAPIKey
    );
    let chosenAssistant;
    const assistants = new import_obsidian3.Setting(parentContainer).setName("Assistants").setDesc("Which assistant do you want to update?").addDropdown((dropdown) => {
      dropdown.addOption("", "---Select an assistant---");
      assistantsList.map((assistant2) => {
        dropdown.addOption(assistant2.id, assistant2.name);
        dropdown.onChange((change) => {
          chosenAssistant = assistantsList.find(
            (assistant3) => assistant3.id === change
          );
          this.resetContainer(this.updateSettings, false);
          this.generateGenericSettings(
            this.updateSettings,
            "update",
            chosenAssistant
          );
          this.generateUpdateAssistants(
            this.updateSettings,
            chosenAssistant
          );
        });
      });
    });
    const updateSettings = parentContainer.createEl("div");
    updateSettings.addClass("llm-update-settings");
    this.updateSettings = updateSettings;
    this.generateGenericSettings(this.updateSettings, "update");
    this.generateUpdateAssistants(this.updateSettings);
    const buttonDiv = parentContainer.createDiv();
    buttonDiv.addClass("llm-flex", "update-button-div", "setting-item");
    const submitButton = new import_obsidian3.ButtonComponent(buttonDiv);
    submitButton.buttonEl.addClass("mod-cta", "llm-assistants-button");
    submitButton.buttonEl.textContent = "Update assistant";
    submitButton.onClick((event) => {
      event.preventDefault();
      const assistantObj = {
        name: this.updateAssistantName,
        instructions: this.updateAssistantIntructions,
        model: this.updateAssistantModel,
        tools: [{ type: this.updateAssistantToolType }],
        topP: this.updateAssistantTopP,
        temperature: this.updateAssistantTemperature
      };
    });
  }
  deleteAssistant(parentContainer) {
    const assistants = this.plugin.settings.assistants;
    if (assistants.length < 1) {
      parentContainer.createEl("div", {
        text: "No assistants found",
        cls: "assistants-empty-state"
      });
    }
    assistants.map((assistant2, index) => {
      const item = parentContainer.createDiv();
      const text = item.createEl("p", {
        text: assistant2.name
      });
      const buttonsDiv = item.createDiv();
      buttonsDiv.addClass("history-buttons-div", "llm-flex");
      const deleteHistory = new import_obsidian3.ButtonComponent(buttonsDiv);
      deleteHistory.buttonEl.setAttr("style", "visibility: hidden");
      item.className = "setting-item";
      item.setAttr("contenteditable", "false");
      item.addClass("llm-history-item", "llm-flex");
      deleteHistory.buttonEl.addClass(
        "llm-delete-history-button",
        "mod-warning"
      );
      deleteHistory.buttonEl.id = "llm-delete-history-button";
      item.addEventListener("mouseenter", () => {
        if (text.contentEditable == "false" || text.contentEditable == "inherit") {
          deleteHistory.buttonEl.setAttr(
            "style",
            "visibility: visible"
          );
        }
      });
      item.addEventListener("mouseleave", () => {
        if (text.contentEditable == "false" || text.contentEditable == "inherit") {
          deleteHistory.buttonEl.setAttr(
            "style",
            "visibility: hidden"
          );
        }
      });
      deleteHistory.setIcon("trash");
      deleteHistory.onClick((e) => {
        e.stopPropagation();
        deleteAssistant(
          this.plugin.settings.openAIAPIKey,
          assistant2.id
        );
        this.resetContainer(parentContainer);
        let updatedAssistants = this.plugin.settings.assistants.filter(
          (item2, idx) => idx !== index
        );
        this.plugin.settings.assistants = updatedAssistants;
        this.plugin.saveSettings();
      });
    });
  }
  createSearch(parentContainer, assistantOption, needsReturn) {
    let filePathArray = [];
    const files = this.plugin.app.vault.getFiles();
    this.generateGenericSettings(parentContainer, "create");
    const file_ids = new import_obsidian3.Setting(parentContainer).setName("Search");
    let filesDiv = parentContainer.createEl("div");
    filesDiv.addClass("setting-item", "llm-vector-dropdown");
    let header = filesDiv.createEl("div");
    header.addClass("setting-item-info");
    let searchDiv = filesDiv.createEl("div");
    searchDiv.addClass("setting-item-control", "llm-vector-files");
    file_ids.addSearch((search) => {
      search.onChange((change) => {
        searchDiv.empty();
        if (change === "") {
          searchDiv.empty();
          return;
        }
        const options = files.filter(
          (file) => file.basename.toLowerCase().includes(change.toLowerCase())
        );
        options.map((option) => {
          const item = searchDiv.createEl("span", {
            text: option.name,
            cls: "llm-vector-file"
          });
          if (filePathArray.includes(option.path))
            item.addClass("llm-file-added");
          item.onClickEvent((click) => {
            if (filePathArray.includes(option.path)) {
              item.removeClass("llm-file-added");
              filePathArray = filePathArray.filter(
                (file_path) => file_path !== option.path
              );
            } else {
              item.addClass("llm-file-added");
              filePathArray = [...filePathArray, option.path];
            }
            assistantOption === assistant ? this.assistantFilesToAdd = filePathArray : this.vectorFilesToAdd = filePathArray;
          });
        });
      });
    });
    if (needsReturn)
      return file_ids;
  }
  createVector(parentContainer) {
    let vectorName = "";
    const name = new import_obsidian3.Setting(parentContainer).setName("Vector storage name").setDesc("The name for your new vector storage").addText((text) => {
      text.onChange((change) => {
      });
    });
  }
  updateVector(parentContainer) {
  }
  async deleteVector(parentContainer) {
    const vectorStores = await listVectors(
      this.plugin.settings.openAIAPIKey
    );
    vectorStores.map((vectorStore, index) => {
      const item = parentContainer.createDiv();
      const text = item.createEl("p", {
        text: vectorStore.name
      });
      const buttonsDiv = item.createDiv();
      buttonsDiv.addClass("history-buttons-div", "llm-flex");
      const deleteHistory = new import_obsidian3.ButtonComponent(buttonsDiv);
      deleteHistory.buttonEl.setAttr("style", "visibility: hidden");
      item.className = "setting-item";
      item.setAttr("contenteditable", "false");
      item.addClass("llm-history-item", "llm-flex");
      deleteHistory.buttonEl.addClass(
        "llm-delete-history-button",
        "mod-warning"
      );
      deleteHistory.buttonEl.id = "llm-delete-history-button";
      item.addEventListener("mouseenter", () => {
        if (text.contentEditable == "false" || text.contentEditable == "inherit") {
          deleteHistory.buttonEl.setAttr(
            "style",
            "visibility: visible"
          );
        }
      });
      item.addEventListener("mouseleave", () => {
        if (text.contentEditable == "false" || text.contentEditable == "inherit") {
          deleteHistory.buttonEl.setAttr(
            "style",
            "visibility: hidden"
          );
        }
      });
      deleteHistory.setIcon("trash");
      deleteHistory.onClick((e) => {
        e.stopPropagation();
        deleteVector(this.plugin.settings.openAIAPIKey, vectorStore.id);
        this.resetContainer(parentContainer);
      });
    });
  }
  generateGenericSettings(parentContainer, option, assistant2) {
    const assistantName = new import_obsidian3.Setting(parentContainer).setName("Assistant name").setDesc("The name to be attributed to the new assistant").addText((text) => {
      if (assistant2)
        text.setValue(assistant2.name);
      text.inputEl.type = "text";
      text.onChange((change) => {
        option === "create" ? this.createAssistantName = change : this.updateAssistantName = change;
      });
    });
    const assistantIntructions = new import_obsidian3.Setting(parentContainer).setName("Assistant instructions").setDesc("The system instructions for the assistant to follow.").addText((text) => {
      if (assistant2)
        text.setValue(assistant2.instructions);
      text.inputEl.type = "text";
      text.onChange((change) => {
        option === "create" ? this.createAssistantIntructions = change : this.updateAssistantIntructions = change;
      });
    });
    const assistantModel = new import_obsidian3.Setting(parentContainer).setName("Assistant model").setDesc("Which LLM you want your assistant to use").addDropdown((dropdown) => {
      if (assistant2)
        dropdown.setValue(assistant2.model);
      dropdown.addOption("", "---Select model---");
      let keys = Object.keys(openAIModels);
      for (let model of keys) {
        dropdown.addOption(models[model].model, model);
      }
      dropdown.onChange((change) => {
        option === "create" ? this.createAssistantModel = change : this.updateAssistantModel = change;
      });
    });
    const assistantToolType = new import_obsidian3.Setting(parentContainer).setName("Assistant tool type").setDesc("File search or code review").addDropdown((dropdown) => {
      if (assistant2)
        dropdown.setValue(assistant2.tools[0].type);
      dropdown.addOption("", "---Tool type---");
      dropdown.addOption("file_search", "File Search");
      dropdown.onChange((change) => {
        if (option === "create") {
          this.createAssistantToolType = change;
          change === "file_search" ? this.filesSetting.settingEl.setAttr(
            "style",
            "display:flex"
          ) : this.filesSetting.settingEl.setAttr(
            "style",
            "display:none"
          );
        } else
          this.updateAssistantToolType = change;
      });
    });
  }
  generateUpdateAssistants(parentContainer, assistant2) {
    const tool_resources = new import_obsidian3.Setting(parentContainer).setName("Tool resources").setDesc(
      "A set of resources that are used by the assistant's tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."
    ).addToggle((toggle) => {
      const trDiv = parentContainer.createEl("div");
      toggle.onChange((change) => {
        if (change) {
          const vector_store_ids = new import_obsidian3.Setting(trDiv).setName("Vector store").setDesc(
            "The new vector store id to attach to ths assistant"
          ).addDropdown((dropdown) => {
            dropdown.addOption(
              "",
              "---Select vector store---"
            );
            dropdown.addOption("vectorStoreId", "ID");
            dropdown.onChange((change2) => {
              this.updateAssistantVectorStoreID = change2;
            });
          });
        }
        if (!change) {
          trDiv.empty();
        }
      });
    });
    new import_obsidian3.Setting(parentContainer).setName("Temperature").setDesc(
      "Defaults to 1. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
    ).addText((text) => {
      if (assistant2)
        text.setValue(`${assistant2.temperature}`);
      text.inputEl.type = "number";
      text.onChange((change) => {
        this.updateAssistantTemperature = parseFloat(change);
      });
    });
    new import_obsidian3.Setting(parentContainer).setName("Top p").setDesc(
      "Defaults to 1. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
    ).addText((text) => {
      if (assistant2)
        text.setValue(`${assistant2.top_p}`);
      text.inputEl.type = "number";
      text.onChange((change) => {
        this.updateAssistantTopP = parseFloat(change);
      });
    });
  }
  resetContainer(parentContainer, total = true) {
    parentContainer.empty();
    if (total)
      this.generateAssistantsContainer(parentContainer);
  }
};

// src/Plugin/Components/ChatContainer.ts
var import_obsidian5 = require("obsidian");

// src/Plugin/Errors/errors.ts
var import_obsidian4 = require("obsidian");
function settingsErrorHandling(params) {
  const settings = Object.keys(params);
  const errors = [];
  settings.map((setting) => {
    if (params[setting] === "quality")
      return;
    if (!params[setting]) {
      errors.push(`Request must include ${setting.toUpperCase()}`);
    }
  });
  return errors;
}
function errorMessages(error, params) {
  if (error.message === "Incorrect Settings") {
    settingsErrorHandling(params).forEach((wrongSetting) => {
      new import_obsidian4.Notice(wrongSetting);
    });
  }
  if (error.message === "Failed to fetch") {
    new import_obsidian4.Notice(
      "You must have GPT4All open with the API Server enabled"
    );
  }
  if (error.message === "No API Key") {
    new import_obsidian4.Notice("You must have an API key to access OpenAI models");
  }
  if (error.message === "GPT4All streaming") {
    new import_obsidian4.Notice("GPT4All is already working on another request. Please wait until that request is done to submit another prompt.");
  }
}

// src/utils/classNames.ts
var classNames = {
  modal: {
    "messages-div": "llm-modal-messages-div",
    "title-border": "llm-modal-title-border",
    "prompt-container": "llm-modal-prompt-container",
    "text-area": "llm-modal-chat-prompt-text-area",
    button: "llm-modal-send-button",
    "chat-message": "llm-modal-chat-message"
  },
  widget: {
    "messages-div": "llm-widget-messages-div",
    "title-border": "llm-widget-title-border",
    "prompt-container": "llm-widget-prompt-container",
    "text-area": "llm-widget-chat-prompt-text-area",
    button: "llm-widget-send-button",
    "chat-message": "llm-widget-chat-message"
  },
  "floating-action-button": {
    "messages-div": "fab-messages-div",
    "title-border": "fab-title-border",
    "prompt-container": "fab-prompt-container",
    "text-area": "fab-chat-prompt-text-area",
    button: "fab-send-button",
    "chat-message": "fab-chat-message"
  }
};

// src/Plugin/Components/AssistantLogo.ts
function assistantLogo() {
  const uniqueId = `paint0_linear_${Math.random().toString(36).substring(2, 9)}`;
  const svgNamespace = "http://www.w3.org/2000/svg";
  const svg = document.createElementNS(svgNamespace, "svg");
  svg.setAttribute("width", "24");
  svg.setAttribute("height", "24");
  svg.setAttribute("viewBox", "0 0 24 24");
  svg.setAttribute("fill", "none");
  const circle = document.createElementNS(svgNamespace, "circle");
  circle.setAttribute("cx", "12");
  circle.setAttribute("cy", "12");
  circle.setAttribute("r", "12");
  circle.setAttribute("fill", `url(#${uniqueId})`);
  svg.appendChild(circle);
  const defs = document.createElementNS(svgNamespace, "defs");
  const linearGradient = document.createElementNS(svgNamespace, "linearGradient");
  linearGradient.setAttribute("id", uniqueId);
  linearGradient.setAttribute("x1", "12");
  linearGradient.setAttribute("y1", "-0.000414185");
  linearGradient.setAttribute("x2", "12");
  linearGradient.setAttribute("y2", "24");
  linearGradient.setAttribute("gradientUnits", "userSpaceOnUse");
  const stop1 = document.createElementNS(svgNamespace, "stop");
  stop1.setAttribute("stop-color", "var(--color-accent)");
  linearGradient.appendChild(stop1);
  const stop2 = document.createElementNS(svgNamespace, "stop");
  stop2.setAttribute("offset", "1");
  stop2.setAttribute("stop-color", "var(--color-accent)");
  stop2.setAttribute("stop-opacity", "0.1");
  linearGradient.appendChild(stop2);
  defs.appendChild(linearGradient);
  svg.appendChild(defs);
  return svg;
}

// src/assets/LLMgal.svg
var LLMgal_default = '<svg width="97" height="71" viewBox="0 0 97 71" fill="none" xmlns="http://www.w3.org/2000/svg">\r\n<path fill-rule="evenodd" clip-rule="evenodd" d="M42.0185 0.5H44.6111H47.2037H49.7963H62.7593H65.3519V3.09259H62.7593H49.7963H47.2037H44.6111H42.0185V0.5ZM36.8333 5.68518V3.09259H39.4259H42.0185L42.0185 5.68518H39.4259H36.8333ZM34.2408 8.27778V5.68518H36.8333L36.8333 8.27778H34.2408ZM31.6482 10.8704V8.27778H34.2408V10.8704H31.6482ZM29.0556 13.463V10.8704H31.6482V13.463H29.0556ZM3.12964 18.6481V16.0556H5.72223H8.31483H10.9074H23.8704H26.463V13.463L29.0556 13.463V23.8333H26.463V18.6481H23.8704H10.9074H8.31483H5.72223H3.12964ZM3.12964 21.2407H0.537048V18.6481H3.12964V21.2407ZM5.72223 23.8333L3.12964 23.8333V21.2407H5.72223V23.8333ZM8.31483 26.4259H5.72223V23.8333H8.31483V26.4259ZM13.5 29.0185H10.9074H8.31483V26.4259H10.9074H13.5V29.0185ZM31.6482 34.2037V31.6111H23.8704V29.0185H16.0926H13.5L13.5 31.6111H16.0926H21.2778V34.2037H18.6852V36.7963L10.9074 36.7963H8.31483V39.3889H10.9074V41.9815L13.5 41.9815V44.5741H16.0926H18.6852V47.1667V49.7593L13.5 49.7593V47.1667H5.72223V49.7593L8.31483 49.7593V52.3519H10.9074V54.9444L13.5 54.9444V57.537H16.0926V60.1296H23.8704L23.8704 57.537H26.463V60.1296H29.0556V62.7222V65.3148H34.2408V67.9074H39.4259L44.6111 67.9074V70.5H47.2037H54.9815V67.9074L60.1667 67.9074V65.3148H62.7593L62.7593 62.7222H65.3519V65.3148H67.9445H78.3148V62.7222H80.9074V60.1296H78.3148V57.537H75.7222L75.7222 54.9444H73.1296V52.3518V44.5741H83.5H93.8704L93.8704 41.9815H96.463V39.3889V36.7963H93.8704V34.2037H88.6852V31.6111H86.0926H83.5V29.0185H80.9074V26.4259H78.3148V23.8333H75.7222L75.7222 10.8704H73.1296V8.27778H70.537V5.68518H67.9445V3.09259H65.3519V5.68518H67.9445L67.9445 8.27778H70.537V10.8704H73.1296V31.6111H75.7222L75.7222 26.4259H78.3148V29.0185H80.9074V31.6111H83.5V34.2037H86.0926H88.6852V36.7963L93.8704 36.7963V39.3889V41.9815L83.5 41.9815H73.1296V39.3889H62.7593L62.7593 36.7963H57.5741H49.7963H44.6111V34.2037H36.8333H31.6482ZM31.6482 47.1667V44.5741H34.2408V36.7963L31.6482 36.7963V34.2037H29.0556V41.9815L26.463 41.9815V34.2037H23.8704H21.2778V36.7963V39.3889H13.5L13.5 41.9815H16.0926H18.6852V44.5741H21.2778V47.1667H23.8704V49.7593H21.2778H18.6852V52.3519H13.5L13.5 49.7593H10.9074V52.3519H13.5L13.5 54.9444H16.0926V57.537H23.8704V54.9444L26.463 54.9444V52.3519V49.7593L29.0556 49.7593V52.3519V54.9444V57.537V60.1296H31.6482V62.7222L36.8333 62.7222V65.3148H39.4259H44.6111V67.9074H47.2037L52.3889 67.9074V65.3148H60.1667V62.7222L62.7593 62.7222V60.1296V57.537H65.3519V54.9444V52.3518L62.7593 52.3519V49.7593L62.7593 47.1667H65.3519V41.9815L62.7593 41.9815V44.5741V47.1667H60.1667V44.5741V41.9815V39.3889H57.5741H54.9815L54.9815 49.7593H52.3889V39.3889H49.7963H47.2037V41.9815V44.5741V47.1667H44.6111V52.3519H42.0185H39.4259H36.8333V49.7593H39.4259H42.0185L42.0185 47.1667V44.5741H44.6111V41.9815V39.3889V36.7963H42.0185V41.9815L42.0185 44.5741H39.4259V41.9815V36.7963H36.8333V44.5741L36.8333 49.7593H34.2408V47.1667H31.6482ZM31.6482 47.1667V49.7593H29.0556V47.1667H31.6482ZM36.8333 52.3519L36.8333 54.9444H34.2408V52.3519H36.8333ZM44.6111 52.3519H47.2037V54.9444H44.6111V52.3519ZM54.9815 54.9444V52.3519V49.7593H57.5741V52.3519H60.1667H62.7593L62.7593 54.9444H60.1667H57.5741H54.9815ZM54.9815 54.9444L54.9815 57.537H52.3889V54.9444L54.9815 54.9444ZM23.8704 47.1667L23.8704 41.9815H26.463V47.1667L23.8704 47.1667ZM73.1296 54.9444L70.537 54.9444V52.3518V41.9815H67.9445V52.3518V54.9444V57.537L67.9445 60.1296H65.3519V62.7222L67.9445 62.7222L78.3148 62.7222V60.1296H75.7222V57.537H73.1296V54.9444Z" fill="black"/>\r\n</svg>\r\n';

// src/Plugin/Components/ChatContainer.ts
var ChatContainer = class {
  constructor(plugin, viewType, messageStore) {
    this.plugin = plugin;
    this.viewType = viewType;
    this.messageStore = messageStore;
    this.messageStore.subscribe(this.updateMessages.bind(this));
  }
  updateMessages(message) {
    const currentIndex = this.plugin.settings.currentIndex;
    const fabIndex = this.plugin.settings.fabSettings.historyIndex;
    const widgetIndex = this.plugin.settings.widgetSettings.historyIndex;
    if (currentIndex > -1) {
      message = this.plugin.settings.promptHistory[currentIndex].messages;
    }
    if (this.viewType === this.plugin.settings.currentView) {
      this.resetChat();
      this.generateIMLikeMessages(message);
      return;
    }
    if (this.viewType === "floating-action-button" && fabIndex === currentIndex && currentIndex > -1) {
      this.resetChat();
      this.generateIMLikeMessages(message);
      return;
    }
    if (this.viewType === "widget" && widgetIndex === currentIndex && currentIndex > -1) {
      this.resetChat();
      this.generateIMLikeMessages(message);
      return;
    }
  }
  getMessages() {
    return this.messageStore.getMessages();
  }
  getParams(endpoint, model, modelType) {
    const settingType = getSettingType(this.viewType);
    const messagesForParams = this.getMessages();
    if (modelType === gemini) {
      const params = {
        // QUESTION -> Do we really want to send prompt when we are sending messages?
        prompt: this.prompt,
        // QUESTION -> how many messages do we really want to send?
        messages: messagesForParams,
        model,
        temperature: this.plugin.settings[settingType].chatSettings.temperature,
        tokens: this.plugin.settings[settingType].chatSettings.maxTokens,
        ...this.plugin.settings[settingType].chatSettings.gemini
      };
      return params;
    }
    if (modelType === assistant) {
      const params = {
        prompt: this.prompt,
        messages: messagesForParams,
        model
      };
      return params;
    }
    if (endpoint === "images") {
      const params = {
        prompt: this.prompt,
        messages: messagesForParams,
        model,
        ...this.plugin.settings[settingType].imageSettings
      };
      return params;
    }
    if (endpoint === chat) {
      if (modelType === GPT4All) {
        const params2 = {
          prompt: this.prompt,
          messages: messagesForParams,
          model,
          temperature: this.plugin.settings[settingType].chatSettings.temperature,
          tokens: this.plugin.settings[settingType].chatSettings.maxTokens,
          ...this.plugin.settings[settingType].chatSettings.GPT4All
        };
        return params2;
      }
      const params = {
        prompt: this.prompt,
        messages: messagesForParams,
        model,
        temperature: this.plugin.settings[settingType].chatSettings.temperature,
        tokens: this.plugin.settings[settingType].chatSettings.maxTokens,
        ...this.plugin.settings[settingType].chatSettings.openAI
      };
      return params;
    }
    if (endpoint === messages) {
      const params = {
        prompt: this.prompt,
        // The Claude API accepts the most recent user message
        // as well as an optional most recent assistant message.
        // This initial approach only sends the most recent user message.
        messages: messagesForParams.slice(-1),
        model,
        temperature: this.plugin.settings[settingType].chatSettings.temperature,
        tokens: this.plugin.settings[settingType].chatSettings.maxTokens
      };
      return params;
    }
  }
  async regenerateOutput() {
    const currentIndex = this.plugin.settings.currentIndex;
    const messages2 = this.plugin.settings.promptHistory[currentIndex].messages;
    this.messageStore.setMessages(messages2);
    this.removeLastMessageAndHistoryMessage();
    this.handleGenerate();
  }
  async handleGenerate() {
    var _a3, _b;
    this.previewText = "";
    const {
      model,
      endpointURL,
      modelEndpoint,
      modelType,
      assistantId,
      modelName
    } = getViewInfo(this.plugin, this.viewType);
    let shouldHaveAPIKey = modelType !== GPT4All;
    const messagesForParams = this.getMessages();
    if (shouldHaveAPIKey) {
      const API_KEY = this.plugin.settings.openAIAPIKey || this.plugin.settings.claudeAPIKey || this.plugin.settings.geminiAPIKey;
      if (!API_KEY) {
        throw new Error("No API key");
      }
    }
    const params = this.getParams(modelEndpoint, model, modelType);
    if (modelEndpoint === assistant) {
      const stream = await assistantsMessage(
        this.plugin.settings.openAIAPIKey,
        messagesForParams,
        assistantId
      );
      stream.on("textCreated", () => this.setDiv(true));
      stream.on("textDelta", (textDelta, snapshot) => {
        var _a4;
        if ((_a4 = textDelta.value) == null ? void 0 : _a4.includes("\u3010"))
          return;
        this.previewText += textDelta.value;
        this.streamingDiv.textContent = this.previewText;
        this.historyMessages.scroll(0, 9999);
      });
      return new Promise((resolve) => {
        stream.on("end", () => {
          this.streamingDiv.empty();
          import_obsidian5.MarkdownRenderer.render(
            this.plugin.app,
            this.previewText,
            this.streamingDiv,
            "",
            this.plugin
          );
          this.historyMessages.scroll(0, 9999);
          this.messageStore.addMessage({
            role: assistant,
            content: this.previewText
          });
          const message_context = {
            ...params,
            messages: this.getMessages(),
            assistant_id: assistantId,
            modelName
          };
          this.historyPush(message_context);
          resolve(true);
        });
      });
    }
    if (model === geminiModel) {
      const stream = await geminiMessage(
        params,
        this.plugin.settings.geminiAPIKey
      );
      this.setDiv(true);
      try {
        for await (const chunk of stream.stream) {
          this.previewText += chunk.text() || "";
          this.streamingDiv.textContent = this.previewText;
          this.historyMessages.scroll(0, 9999);
        }
      } catch (err) {
        console.error(err);
        return false;
      }
      this.streamingDiv.empty();
      import_obsidian5.MarkdownRenderer.render(
        this.plugin.app,
        this.previewText,
        this.streamingDiv,
        "",
        this.plugin
      );
      const copyButton = this.streamingDiv.querySelectorAll(
        ".copy-code-button"
      );
      copyButton.forEach((item) => {
        item.setAttribute("style", "display: none");
      });
      this.messageStore.addMessage({
        role: assistant,
        content: this.previewText
      });
      const message_context = {
        ...params,
        messages: this.getMessages()
      };
      this.historyPush(message_context);
      return true;
    }
    if (modelEndpoint === messages) {
      const stream = await claudeMessage(
        params,
        this.plugin.settings.claudeAPIKey
      );
      this.setDiv(true);
      stream.on("text", (text) => {
        this.previewText += text || "";
        this.streamingDiv.textContent = this.previewText;
        this.historyMessages.scroll(0, 9999);
      });
      this.streamingDiv.empty();
      import_obsidian5.MarkdownRenderer.render(
        this.plugin.app,
        this.previewText,
        this.streamingDiv,
        "",
        this.plugin
      );
      const copyButton = this.streamingDiv.querySelectorAll(
        ".copy-code-button"
      );
      copyButton.forEach((item) => {
        item.setAttribute("style", "display: none");
      });
      this.messageStore.addMessage({
        role: assistant,
        content: this.previewText
      });
      const message_context = {
        ...params,
        messages: this.getMessages()
      };
      this.historyPush(message_context);
      return true;
    }
    if (modelType === GPT4All) {
      this.plugin.settings.GPT4AllStreaming = true;
      this.setDiv(false);
      messageGPT4AllServer(params, endpointURL).then(
        (response) => {
          this.streamingDiv.textContent = response.content;
          this.messageStore.addMessage(response);
          this.previewText = response.content;
          this.historyPush(params);
        }
      );
    } else if (modelEndpoint === chat) {
      const stream = await openAIMessage(
        params,
        this.plugin.settings.openAIAPIKey,
        endpointURL,
        modelEndpoint
      );
      this.setDiv(true);
      for await (const chunk of stream) {
        this.previewText += ((_b = (_a3 = chunk.choices[0]) == null ? void 0 : _a3.delta) == null ? void 0 : _b.content) || "";
        this.streamingDiv.textContent = this.previewText;
        this.historyMessages.scroll(0, 9999);
      }
      this.streamingDiv.empty();
      import_obsidian5.MarkdownRenderer.render(
        this.plugin.app,
        this.previewText,
        this.streamingDiv,
        "",
        this.plugin
      );
      const copyButton = this.streamingDiv.querySelectorAll(
        ".copy-code-button"
      );
      copyButton.forEach((item) => {
        item.setAttribute("style", "display: none");
      });
      this.messageStore.addMessage({
        role: assistant,
        content: this.previewText
      });
      const message_context = {
        ...params,
        messages: this.messageStore.getMessages()
      };
      this.historyPush(message_context);
      return true;
    }
    return true;
  }
  async handleGenerateClick(header, sendButton) {
    header.disableButtons();
    sendButton.setDisabled(true);
    const {
      model,
      modelName,
      modelType,
      endpointURL,
      modelEndpoint,
      historyIndex
    } = getViewInfo(this.plugin, this.viewType);
    if (historyIndex > -1) {
      const messages2 = this.plugin.settings.promptHistory[historyIndex].messages;
      this.messageStore.setMessages(messages2);
    }
    const refreshButton = this.historyMessages.querySelector(
      ".llm-refresh-output"
    );
    refreshButton == null ? void 0 : refreshButton.remove();
    if (this.historyMessages.children.length < 1) {
      header.setHeader(modelName, this.prompt);
    }
    this.messageStore.addMessage({ role: "user", content: this.prompt });
    const params = this.getParams(modelEndpoint, model, modelType);
    try {
      this.previewText = "";
      if (modelEndpoint !== "images") {
        await this.handleGenerate();
      }
      if (modelEndpoint === "images") {
        this.setDiv(false);
        await openAIMessage(
          params,
          this.plugin.settings.openAIAPIKey,
          endpointURL,
          modelEndpoint
        ).then((response) => {
          this.streamingDiv.empty();
          let content = "";
          response.map((url) => {
            content += `![created with prompt ${this.prompt}](${url})`;
          });
          this.messageStore.addMessage({
            role: assistant,
            content
          });
          this.appendImage(response);
          this.historyPush({
            ...params,
            messages: this.getMessages()
          });
        });
      }
      header.enableButtons();
      sendButton.setDisabled(false);
      const buttonsContainer = this.loadingDivContainer.querySelector(
        ".llm-assistant-buttons"
      );
      buttonsContainer == null ? void 0 : buttonsContainer.removeClass("llm-hide");
    } catch (error) {
      header.enableButtons();
      sendButton.setDisabled(false);
      this.plugin.settings.GPT4AllStreaming = false;
      this.prompt = "";
      errorMessages(error, params);
      if (this.getMessages().length > 0) {
        setTimeout(() => {
          this.removeMessage(header, modelName);
        }, 1e3);
      }
    }
  }
  historyPush(params) {
    const { modelName, historyIndex, modelEndpoint, assistantId } = getViewInfo(this.plugin, this.viewType);
    if (historyIndex > -1) {
      this.plugin.history.overwriteHistory(
        this.getMessages(),
        historyIndex
      );
      return;
    }
    if (modelEndpoint === chat || modelEndpoint === gemini || modelEndpoint === messages) {
      this.plugin.history.push({
        ...params,
        modelName
      });
    }
    if (modelEndpoint === "images") {
      this.plugin.history.push({
        ...params,
        modelName
      });
    }
    if (modelEndpoint === assistant) {
      this.plugin.history.push({
        ...params,
        modelName,
        assistant_id: assistantId
      });
    }
    const length = this.plugin.settings.promptHistory.length;
    setHistoryIndex(this.plugin, this.viewType, length);
    this.plugin.saveSettings();
    this.prompt = "";
  }
  auto_height(elem, parentElement) {
    elem.inputEl.setAttribute("style", "height: 50px");
    const height = elem.inputEl.scrollHeight - 5;
    if (!(height > parseInt(window.getComputedStyle(elem.inputEl).height)))
      return;
    elem.inputEl.setAttribute("style", `height: ${height}px`);
    elem.inputEl.setAttribute("style", `overflow: hidden`);
    parentElement.scrollTo(0, 9999);
  }
  displayNoChatView(parentElement) {
    parentElement.addClass("llm-justify-content-center");
    parentElement.addClass("center-llmgal");
    const llmGal = parentElement.createDiv();
    llmGal.addClass("llm-icon-wrapper");
    llmGal.addClass("llm-icon-new-chat");
    const parser = new DOMParser();
    const svgDoc = parser.parseFromString(LLMgal_default, "image/svg+xml");
    const svgElement = svgDoc.documentElement;
    llmGal.appendChild(svgElement);
  }
  async generateChatContainer(parentElement, header) {
    this.messageStore.setMessages([]);
    this.historyMessages = parentElement.createDiv();
    this.historyMessages.className = classNames[this.viewType]["messages-div"];
    if (this.getMessages().length === 0) {
      this.displayNoChatView(this.historyMessages);
    }
    const promptContainer = parentElement.createDiv();
    const promptField = new import_obsidian5.TextAreaComponent(promptContainer);
    const sendButton = new import_obsidian5.ButtonComponent(promptContainer);
    if (this.viewType === "floating-action-button") {
      promptContainer.addClass("llm-flex");
    }
    promptContainer.addClass(classNames[this.viewType]["prompt-container"]);
    promptField.inputEl.className = classNames[this.viewType]["text-area"];
    promptField.inputEl.id = "chat-prompt-text-area";
    promptContainer.addEventListener("input", () => {
      this.auto_height(promptField, parentElement);
    });
    sendButton.buttonEl.addClass(
      classNames[this.viewType].button,
      "llm-send-button"
    );
    sendButton.setIcon("up-arrow-with-tail");
    sendButton.setTooltip("Send prompt");
    promptField.setPlaceholder("Send a message...");
    promptField.onChange((change) => {
      this.prompt = change;
      promptField.setValue(change);
    });
    promptField.inputEl.addEventListener("keydown", (event) => {
      if (sendButton.disabled === true)
        return;
      if (event.code == "Enter") {
        event.preventDefault();
        this.handleGenerateClick(header, sendButton);
        promptField.inputEl.setText("");
        promptField.setValue("");
      }
    });
    sendButton.onClick(() => {
      this.handleGenerateClick(header, sendButton);
      promptField.inputEl.setText("");
      promptField.setValue("");
    });
  }
  setMessages(replaceChatHistory = false) {
    const { historyIndex } = getViewInfo(this.plugin, this.viewType);
    if (replaceChatHistory) {
      let history = this.plugin.settings.promptHistory;
      this.messageStore.setMessages(history[historyIndex].messages);
    }
    if (!replaceChatHistory) {
      this.messageStore.addMessage({
        role: "user",
        content: this.prompt
      });
    }
  }
  resetMessages() {
    this.messageStore.setMessages([]);
  }
  setDiv(streaming) {
    const parent = this.historyMessages.createDiv();
    parent.addClass("llm-flex");
    const assistant2 = parent.createEl("div", { cls: "llm-assistant-logo" });
    assistant2.appendChild(assistantLogo());
    this.loadingDivContainer = parent.createDiv();
    this.streamingDiv = this.loadingDivContainer.createDiv();
    const buttonsContainer = this.loadingDivContainer.createEl("div", {
      cls: "llm-assistant-buttons llm-hide"
    });
    const copyToClipboardButton = new import_obsidian5.ButtonComponent(buttonsContainer);
    copyToClipboardButton.setIcon("files");
    const refreshButton = new import_obsidian5.ButtonComponent(buttonsContainer);
    refreshButton.setIcon("refresh-cw");
    copyToClipboardButton.buttonEl.addClass("llm-add-text");
    refreshButton.buttonEl.addClass("llm-refresh-output");
    if (streaming) {
      this.streamingDiv.empty();
    } else {
      const dots = this.streamingDiv.createEl("span");
      for (let i = 0; i < 3; i++) {
        const dot = dots.createEl("span", { cls: "streaming-dot" });
        dot.textContent = ".";
      }
    }
    this.streamingDiv.addClass("im-like-message");
    this.loadingDivContainer.addClass(
      "llm-flex-end",
      "im-like-message-container",
      "llm-flex"
    );
    copyToClipboardButton.onClick(async () => {
      await navigator.clipboard.writeText(this.previewText);
      new import_obsidian5.Notice("Text copied to clipboard");
    });
    refreshButton.onClick(async () => {
      new import_obsidian5.Notice("Regenerating response...");
      this.regenerateOutput();
    });
  }
  appendImage(imageURLs) {
    imageURLs.map((url) => {
      const img = this.streamingDiv.createEl("img");
      img.src = url;
      img.alt = `image generated with ${this.prompt}`;
    });
  }
  createMessage(content, index, finalMessage, assistant2 = false) {
    const imLikeMessageContainer = this.historyMessages.createDiv();
    const imLikeMessage = imLikeMessageContainer.createDiv();
    const copyToClipboardButton = new import_obsidian5.ButtonComponent(
      imLikeMessageContainer
    );
    copyToClipboardButton.setIcon("files");
    if (assistant2) {
      const parent = imLikeMessage.createDiv();
      parent.addClass("llm-flex-reverse");
      const assistantMessage = parent.createDiv();
      assistantMessage.addClass("llm-flex-column");
      imLikeMessage.addClass("llm-flex");
      const assistant3 = parent.createEl("div", {
        cls: "llm-assistant-logo"
      });
      assistant3.appendChild(assistantLogo());
      import_obsidian5.MarkdownRenderer.render(
        this.plugin.app,
        content,
        assistantMessage,
        "",
        this.plugin
      );
    } else {
      import_obsidian5.MarkdownRenderer.render(
        this.plugin.app,
        content,
        imLikeMessage,
        "",
        this.plugin
      );
    }
    const copyButton = imLikeMessage.querySelectorAll(
      ".copy-code-button"
    );
    copyButton.forEach((item) => {
      item.setAttribute("style", "display: none");
    });
    imLikeMessageContainer.addClass(
      "im-like-message-container",
      "llm-flex"
    );
    copyToClipboardButton.buttonEl.addClass(
      "add-text",
      "llm-hide",
      "mt-auto"
    );
    imLikeMessage.addClass(
      "im-like-message",
      classNames[this.viewType]["chat-message"]
    );
    if (index % 2 === 0) {
      imLikeMessageContainer.addClass("llm-flex-start", "llm-flex");
    } else {
      imLikeMessageContainer.addClass("llm-flex-end", "llm-flex");
    }
    imLikeMessageContainer.addEventListener("mouseenter", () => {
      copyToClipboardButton.buttonEl.removeClass("llm-hide");
    });
    imLikeMessageContainer.addEventListener("mouseleave", () => {
      copyToClipboardButton.buttonEl.addClass("llm-hide");
    });
    copyToClipboardButton.setTooltip("Copy to clipboard");
    copyToClipboardButton.onClick(async () => {
      await navigator.clipboard.writeText(content);
      new import_obsidian5.Notice("Text copied to clipboard");
    });
    if (finalMessage) {
      const refreshButton = new import_obsidian5.ButtonComponent(imLikeMessageContainer);
      refreshButton.setIcon("refresh-cw");
      refreshButton.buttonEl.addClass("llm-refresh-output", "llm-hide");
      imLikeMessageContainer.addEventListener("mouseenter", () => {
        refreshButton.buttonEl.removeClass("llm-hide");
      });
      imLikeMessageContainer.addEventListener("mouseleave", () => {
        refreshButton.buttonEl.addClass("llm-hide");
      });
      refreshButton.onClick(async () => {
        new import_obsidian5.Notice("Regenerating response...");
        this.regenerateOutput();
      });
    }
  }
  generateIMLikeMessages(messages2) {
    let finalMessage = false;
    messages2.map(({ role, content }, index) => {
      if (index === messages2.length - 1)
        finalMessage = true;
      if (role === "assistant") {
        this.createMessage(content, index, finalMessage, true);
        return;
      }
      this.createMessage(content, index, finalMessage);
    });
    this.historyMessages.scroll(0, 9999);
  }
  appendNewMessage(message) {
    const length = this.historyMessages.childNodes.length;
    const { content } = message;
    this.createMessage(content, length, false);
  }
  removeLastMessageAndHistoryMessage() {
    var _a3;
    const messages2 = this.messageStore.getMessages();
    messages2.pop();
    this.messageStore.setMessages(messages2);
    (_a3 = this.historyMessages.lastElementChild) == null ? void 0 : _a3.remove();
  }
  removeMessage(header, modelName) {
    this.removeLastMessageAndHistoryMessage();
    if (this.historyMessages.children.length < 1) {
      header.setHeader(modelName, "LLM plugin");
    }
  }
  resetChat() {
    this.historyMessages.empty();
    this.historyMessages.removeClass("center-llmgal");
  }
  newChat() {
    this.historyMessages.empty();
    this.displayNoChatView(this.historyMessages);
  }
};

// src/Plugin/Components/Header.ts
var import_obsidian6 = require("obsidian");
var Header = class {
  constructor(plugin, viewType) {
    this.plugin = plugin;
    this.viewType = viewType;
  }
  setHeader(modelName, title) {
    if (title) {
      this.titleEl.textContent = title;
    }
    this.modelEl.textContent = modelName;
  }
  resetHistoryButton() {
    this.chatHistoryButton.buttonEl.removeClass("is-active");
  }
  clickHandler(button, toggles) {
    if (button.buttonEl.classList.contains("is-active")) {
      button.buttonEl.removeClass("is-active");
    } else {
      if (!button.buttonEl.classList.contains("new-chat-button")) {
        button.buttonEl.addClass("is-active");
      }
      toggles.map((el) => {
        el.buttonEl.removeClass("is-active");
      });
    }
  }
  disableButtons() {
    this.chatHistoryButton.setDisabled(true);
    this.newChatButton.setDisabled(true);
    this.settingsButton.setDisabled(true);
    this.assistantsButton.setDisabled(true);
  }
  enableButtons() {
    this.chatHistoryButton.setDisabled(false);
    this.newChatButton.setDisabled(false);
    this.settingsButton.setDisabled(false);
    this.assistantsButton.setDisabled(false);
  }
  generateHeader(parentElement, chatContainerDiv, chatHistoryContainerDiv, settingsContainerDiv, assistantContainerDiv, chatContainer, historyContainer, settingsContainer, assistantsContainer) {
    const { modelName } = getViewInfo(this.plugin, this.viewType);
    const titleDiv = createDiv();
    const leftButtonDiv = titleDiv.createDiv();
    const titleContainer = titleDiv.createDiv();
    this.titleEl = titleContainer.createDiv();
    this.titleEl.addClass(`${this.viewType}-llm-title`);
    const rightButtonsDiv = titleDiv.createDiv();
    titleDiv.addClass("llm-title-div", "llm-flex");
    this.titleEl.textContent = "LLM";
    this.modelEl = titleContainer.createDiv();
    this.modelEl.addClass("llm-model-name");
    this.modelEl.textContent = modelName;
    this.chatHistoryButton = new import_obsidian6.ButtonComponent(leftButtonDiv);
    this.chatHistoryButton.setTooltip("Chats");
    this.chatHistoryButton.onClick(() => {
      historyContainer.resetHistory(chatHistoryContainerDiv);
      historyContainer.generateHistoryContainer(
        chatHistoryContainerDiv,
        this.plugin.settings.promptHistory,
        chatContainerDiv,
        chatContainer,
        this
      );
      this.clickHandler(this.chatHistoryButton, [
        this.settingsButton,
        this.assistantsButton
      ]);
      if (!chatHistoryContainerDiv.isShown()) {
        chatHistoryContainerDiv.show();
        settingsContainerDiv.hide();
        chatContainerDiv.hide();
        assistantContainerDiv.hide();
      } else {
        chatContainerDiv.show();
        chatHistoryContainerDiv.hide();
      }
    });
    this.assistantsButton = new import_obsidian6.ButtonComponent(rightButtonsDiv);
    this.assistantsButton.setTooltip("Assistants");
    assistantsContainer.generateAssistantsContainer(assistantContainerDiv);
    this.assistantsButton.onClick(() => {
      this.clickHandler(this.assistantsButton, [
        this.settingsButton,
        this.chatHistoryButton
      ]);
      if (!assistantContainerDiv.isShown()) {
        assistantContainerDiv.show();
        settingsContainerDiv.hide();
        chatContainerDiv.hide();
        chatHistoryContainerDiv.hide();
      } else {
        chatContainerDiv.show();
        assistantContainerDiv.hide();
      }
    });
    if (this.viewType === "floating-action-button") {
      this.newChatButton = new import_obsidian6.ButtonComponent(leftButtonDiv);
      this.settingsButton = new import_obsidian6.ButtonComponent(rightButtonsDiv);
    } else {
      this.newChatButton = new import_obsidian6.ButtonComponent(rightButtonsDiv);
      this.settingsButton = new import_obsidian6.ButtonComponent(leftButtonDiv);
    }
    this.settingsButton.setTooltip("Chat settings");
    this.settingsButton.onClick(() => {
      settingsContainer.resetSettings(settingsContainerDiv);
      settingsContainer.generateSettingsContainer(
        settingsContainerDiv,
        this
      );
      this.clickHandler(this.settingsButton, [
        this.chatHistoryButton,
        this.assistantsButton
      ]);
      if (!settingsContainerDiv.isShown()) {
        settingsContainerDiv.show();
        chatContainerDiv.hide();
        chatHistoryContainerDiv.hide();
        assistantContainerDiv.hide();
      } else {
        chatContainerDiv.show();
        settingsContainerDiv.hide();
      }
    });
    this.newChatButton.setTooltip("New chat");
    this.newChatButton.onClick(() => {
      const { modelName: modelName2 } = getViewInfo(this.plugin, this.viewType);
      this.clickHandler(this.newChatButton, [
        this.settingsButton,
        this.chatHistoryButton,
        this.assistantsButton
      ]);
      this.setHeader(modelName2, "New chat");
      chatContainerDiv.show();
      settingsContainerDiv.hide();
      chatHistoryContainerDiv.hide();
      assistantContainerDiv.hide();
      chatContainer.newChat();
      chatContainer.resetMessages();
      setHistoryIndex(this.plugin, this.viewType);
    });
    leftButtonDiv.addClass("llm-left-buttons-div", "llm-flex");
    rightButtonsDiv.addClass("llm-right-buttons-div", "llm-flex");
    titleContainer.addClass("llm-title", "llm-flex");
    this.chatHistoryButton.buttonEl.addClass(
      "clickable-icon",
      "chat-history"
    );
    this.settingsButton.buttonEl.addClass(
      "clickable-icon",
      "settings-button"
    );
    this.newChatButton.buttonEl.addClass(
      "clickable-icon",
      "new-chat-button"
    );
    this.assistantsButton.buttonEl.addClass("clickable-icon", "assistants");
    this.chatHistoryButton.setIcon("menu");
    this.settingsButton.setIcon("sliders-horizontal");
    this.newChatButton.setIcon("plus");
    this.assistantsButton.setIcon("bot");
    parentElement.prepend(titleDiv);
  }
};

// src/Plugin/Components/HistoryContainer.ts
var import_obsidian7 = require("obsidian");
var HistoryContainer = class {
  constructor(plugin, viewType) {
    this.plugin = plugin;
    this.viewType = viewType;
  }
  getChatContainerClassPrefix() {
    if (this.viewType === "floating-action-button") {
      return "fab";
    } else if (this.viewType === "widget") {
      return this.viewType;
    } else if (this.viewType === "modal") {
      return this.viewType;
    }
  }
  displayNoHistoryView(parentElement) {
    parentElement.addClass("llm-justify-content-center");
    const llmGal = parentElement.createDiv();
    llmGal.addClass("llm-icon-wrapper");
    llmGal.addClass("llm-icon-new-history");
    const parser = new DOMParser();
    const svgDoc = parser.parseFromString(LLMgal_default, "image/svg+xml");
    const svgElement = svgDoc.documentElement;
    llmGal.appendChild(svgElement);
    const cta = llmGal.createEl("div", {
      attr: {
        class: "empty-history-cta llm-font-size-medium llm-justify-content-center"
      },
      text: "Looking kind of empty. Start chatting and conversations will appear here."
    });
    cta.addClass("text-align-center");
    const createChatButton = new import_obsidian7.ButtonComponent(cta);
    createChatButton.setButtonText("New chat");
    createChatButton.setClass("llm-empty-history-button");
    createChatButton.setClass("mod-cta");
    createChatButton.onClick(() => {
      parentElement.hide();
      const activeHistoryButton = document.querySelector(
        ".chat-history.is-active"
      );
      activeHistoryButton == null ? void 0 : activeHistoryButton.classList.remove("is-active");
      const prefix = this.getChatContainerClassPrefix();
      const chatContainer = document.querySelector(
        `[class*="${prefix}-chat-container"]`
      );
      chatContainer.show();
      parentElement.classList.remove("llm-justify-content-center");
    });
  }
  generateHistoryContainer(parentElement, history, containerToShow, chat2, Header2) {
    if (!history.length) {
      this.displayNoHistoryView(parentElement);
      return;
    }
    const settingType = getSettingType(this.viewType);
    this.model = this.plugin.settings[settingType].model;
    this.modelName = this.plugin.settings[settingType].modelName;
    this.modelType = this.plugin.settings[settingType].modelType;
    this.modelType = this.plugin.settings[settingType].modelType;
    this.historyIndex = this.plugin.settings[settingType].historyIndex;
    const eventListener = () => {
      var _a3;
      chat2.resetChat();
      parentElement.hide();
      containerToShow.show();
      chat2.setMessages(true);
      const messages2 = chat2.getMessages();
      chat2.generateIMLikeMessages(messages2);
      (_a3 = containerToShow.querySelector(".messages-div")) == null ? void 0 : _a3.scroll(0, 9999);
      const index = this.historyIndex;
      this.plugin.settings.currentIndex = index;
      const header = this.plugin.settings.promptHistory[index].prompt;
      const modelName = this.plugin.settings.promptHistory[index].modelName;
      const model = this.plugin.settings.promptHistory[index].model;
      this.plugin.settings[settingType].modelName = modelName;
      if (!model.includes("asst")) {
        this.plugin.settings[settingType].model = models[modelName].model;
        this.plugin.settings[settingType].modelType = models[modelName].type;
        this.plugin.settings[settingType].modelEndpoint = models[modelName].endpoint;
        this.plugin.settings[settingType].endpointURL = models[modelName].url;
      } else {
        this.plugin.settings[settingType].model = this.plugin.settings.promptHistory[index].model;
        this.plugin.settings[settingType].modelName = this.plugin.settings.promptHistory[index].modelName;
        this.plugin.settings[settingType].modelType = assistant;
        this.plugin.settings[settingType].modelEndpoint = assistant;
        this.plugin.settings[settingType].endpointURL = "";
      }
      this.plugin.saveSettings();
      Header2.setHeader(modelName, header);
      Header2.resetHistoryButton();
    };
    eventListener.bind(this);
    const disableHistory = (collection, index, enabled) => {
      var _a3, _b;
      for (let i = 0; i < collection.length; i++) {
        if (i !== index && !enabled) {
          (_a3 = collection.item(i)) == null ? void 0 : _a3.addClass("llm-no-pointer");
        } else {
          (_b = collection.item(i)) == null ? void 0 : _b.removeClass("llm-no-pointer");
        }
      }
    };
    const toggleContentEditable = (element, toggle) => {
      element.setAttr("contenteditable", toggle);
    };
    history.map((historyItem, index) => {
      var _a3;
      const item = parentElement.createDiv();
      const text = item.createEl("p");
      const displayHTML = (historyItem == null ? void 0 : historyItem.prompt) || ((_a3 = historyItem == null ? void 0 : historyItem.messages[0]) == null ? void 0 : _a3.content);
      text.textContent = displayHTML;
      const buttonsDiv = item.createDiv();
      buttonsDiv.addClass("history-buttons-div", "llm-flex");
      const editPrompt = new import_obsidian7.ButtonComponent(buttonsDiv);
      const savePrompt = new import_obsidian7.ButtonComponent(buttonsDiv);
      const deleteHistory = new import_obsidian7.ButtonComponent(buttonsDiv);
      savePrompt.buttonEl.setAttr(
        "style",
        "display: none; visibility: hidden"
      );
      editPrompt.buttonEl.setAttr("style", "visibility: hidden");
      deleteHistory.buttonEl.setAttr("style", "visibility: hidden");
      item.className = "setting-item";
      item.setAttr("contenteditable", "false");
      item.addClass("llm-history-item", "llm-flex");
      editPrompt.buttonEl.addClass("edit-prompt-button");
      savePrompt.buttonEl.addClass("save-prompt-button");
      editPrompt.setIcon("pencil");
      savePrompt.setIcon("save");
      deleteHistory.buttonEl.addClass(
        "llm-delete-history-button",
        "mod-warning"
      );
      deleteHistory.buttonEl.id = "llm-delete-history-button";
      item.addEventListener("click", () => {
        this.plugin.settings[settingType].historyIndex = index;
        this.historyIndex = index;
        this.plugin.saveSettings();
      });
      item.addEventListener("mouseenter", () => {
        if (text.contentEditable == "false" || text.contentEditable == "inherit") {
          editPrompt.buttonEl.setAttr("style", "visibility: visible");
          deleteHistory.buttonEl.setAttr(
            "style",
            "visibility: visible"
          );
        }
      });
      item.addEventListener("mouseleave", () => {
        if (text.contentEditable == "false" || text.contentEditable == "inherit") {
          editPrompt.buttonEl.setAttr("style", "visibility: hidden");
          deleteHistory.buttonEl.setAttr(
            "style",
            "visibility: hidden"
          );
        }
      });
      item.addEventListener("click", eventListener);
      deleteHistory.setIcon("trash");
      deleteHistory.onClick((e) => {
        e.stopPropagation();
        this.resetHistory(parentElement);
        let updatedHistory = this.plugin.settings.promptHistory.filter(
          (item2, idx) => idx !== index
        );
        this.plugin.settings.promptHistory = updatedHistory;
        this.plugin.saveSettings();
        this.generateHistoryContainer(
          parentElement,
          this.plugin.settings.promptHistory,
          containerToShow,
          chat2,
          Header2
        );
        chat2.resetChat();
        chat2.resetMessages();
        Header2.setHeader(this.modelName, "Local LLM plugin");
        this.plugin.settings[settingType].historyIndex = DEFAULT_SETTINGS[settingType].historyIndex;
        this.plugin.saveSettings();
      });
      editPrompt.onClick((e) => {
        e.stopPropagation();
        item.removeEventListener("click", eventListener);
        toggleContentEditable(text, true);
        text.focus();
        editPrompt.buttonEl.setAttr("style", "display: none");
        savePrompt.buttonEl.setAttr("style", "display: inline-flex");
        disableHistory(parentElement.children, index, false);
      });
      savePrompt.onClick((e) => {
        e.stopPropagation();
        if (item.textContent) {
          this.plugin.settings.promptHistory[index].prompt = item.textContent;
          this.plugin.saveSettings();
        } else {
          new import_obsidian7.Notice("Prompt length must be greater than 0");
          return;
        }
        item.addEventListener("click", eventListener);
        toggleContentEditable(text, false);
        editPrompt.buttonEl.setAttr("style", "display: inline-flex");
        savePrompt.buttonEl.setAttr("style", "display: none");
        disableHistory(parentElement.children, index, true);
      });
    });
  }
  resetHistory(parentContainer) {
    parentContainer.empty();
  }
};

// src/Plugin/Components/SettingsContainer.ts
var import_obsidian8 = require("obsidian");
var SettingsContainer = class {
  constructor(plugin, viewType) {
    this.plugin = plugin;
    this.viewType = viewType;
  }
  async generateSettingsContainer(parentContainer, Header2) {
    this.resetSettings(parentContainer);
    this.generateModels(parentContainer, Header2);
    this.generateModelSettings(parentContainer);
  }
  generateModels(parentContainer, Header2) {
    const settingType = getSettingType(this.viewType);
    const viewSettings = this.plugin.settings[settingType];
    new import_obsidian8.Setting(parentContainer).setName("Models").setDesc("The model you want to use to generate a chat response.").addDropdown((dropdown) => {
      dropdown.addOption("", "---Select assistant---");
      const assistants = this.plugin.settings.assistants;
      assistants.map((assistant2) => {
        dropdown.addOption(`${assistant2.id}`, `${assistant2.name}`);
      });
      dropdown.addOption("", "---Select model---");
      let keys = Object.keys(models);
      for (let model of keys) {
        if (models[model].type === GPT4All) {
          const gpt4AllPath = getGpt4AllPath(this.plugin);
          const fullPath = `${gpt4AllPath}/${models[model].model}`;
          const exists = this.plugin.fileSystem.existsSync(fullPath);
          if (exists) {
            dropdown.addOption(models[model].model, model);
          }
        } else {
          dropdown.addOption(models[model].model, model);
        }
      }
      dropdown.onChange((change) => {
        const { historyIndex } = getViewInfo(
          this.plugin,
          this.viewType
        );
        const index = historyIndex;
        if (change.includes("asst")) {
          viewSettings.assistant = true;
          this.plugin.saveSettings();
        } else {
          viewSettings.assistant = false;
          viewSettings.assistantId = "";
          this.plugin.saveSettings();
        }
        if (!viewSettings.assistant) {
          const modelName = modelNames[change];
          viewSettings.model = change;
          viewSettings.modelName = modelName;
          viewSettings.modelType = models[modelName].type;
          viewSettings.endpointURL = models[modelName].url;
          viewSettings.modelEndpoint = models[modelName].endpoint;
          if (index > -1) {
            this.plugin.settings.promptHistory[index].model = change;
            this.plugin.settings.promptHistory[index].modelName = modelName;
          }
          this.plugin.saveSettings();
          Header2.setHeader(modelName);
        }
        if (viewSettings.assistant) {
          viewSettings.assistantId = change;
          const assistant2 = getAssistant(
            this.plugin,
            viewSettings.assistantId
          );
          viewSettings.model = assistant2.id;
          viewSettings.modelName = assistant2.name;
          viewSettings.modelType = assistant2.modelType;
          viewSettings.endpointURL = "";
          viewSettings.modelEndpoint = assistant;
          if (index > -1) {
            this.plugin.settings.promptHistory[index].model = assistant2.model;
            this.plugin.settings.promptHistory[index].modelName = modelNames[assistant2.model];
          }
          this.plugin.saveSettings();
          Header2.setHeader(assistant2.name);
        }
        this.generateSettingsContainer(parentContainer, Header2);
      });
      dropdown.setValue(viewSettings.model);
    });
  }
  resetSettings(parentContainer) {
    parentContainer.empty();
  }
  generateModelSettings(parentContainer) {
    const settingType = getSettingType(this.viewType);
    const viewSettings = this.plugin.settings[settingType];
    const endpoint = viewSettings.modelEndpoint;
    const modelType = viewSettings.modelType;
    if (endpoint === "images") {
      this.generateImageSettings(parentContainer, viewSettings.model);
    }
    if (endpoint === "moderations") {
      this.generateModerationsSettings(parentContainer);
    }
    if (endpoint === chat || messages) {
      this.generateChatSettings(parentContainer, modelType);
    }
  }
  generateImageSettings(parentContainer, model) {
    const settingType = getSettingType(this.viewType);
    const viewSettings = this.plugin.settings[settingType];
    const imageSizes = {
      dallE2: ["256x256", "512x512", "1024x1024"],
      dallE3: ["1024x1024", "1792x1024", "1024x1792"]
    };
    const numberOfImages = new import_obsidian8.Setting(parentContainer).setName("Number of images").setDesc(
      "The number of images generated by the model. Must be between 1 and 10. For Dall-E 3, only 1 image can be generated."
    ).addText((text) => {
      text.setValue(`${viewSettings.imageSettings.numberOfImages}`);
      text.inputEl.type = "number";
      text.onChange((change) => {
        viewSettings.imageSettings.numberOfImages = parseInt(change);
        this.plugin.saveSettings();
      });
    });
    const responseFormat = new import_obsidian8.Setting(parentContainer).setName("Response format").setDesc(
      "The format in which the generated images are returned. Must be one of url or b64_json. URLs are only valid for 60 minutes after the image has been generated."
    ).addDropdown((dropdown) => {
      dropdown.addOption("", "Select a Format");
      dropdown.addOption("url", "URL");
      dropdown.addOption("b64_json", "Base64 JSON");
      dropdown.onChange((change) => {
        viewSettings.imageSettings.response_format = change;
        this.plugin.saveSettings();
      });
    });
    const imageSize = new import_obsidian8.Setting(parentContainer).setName("Image size").setDesc(
      "The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for dall-e-2. Must be one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3 models."
    ).addDropdown((dropdown) => {
      if (model === "dall-e-2") {
        dropdown.addOption("", "Dall-E 2 sizes");
        imageSizes["dallE2"].map((size) => {
          dropdown.addOption(size, size);
        });
      }
      if (model === "dall-e-3") {
        dropdown.addOption("", "Dall-E 3 sizes");
        imageSizes["dallE3"].map((size) => {
          dropdown.addOption(size, size);
        });
      }
      dropdown.onChange((change) => {
        viewSettings.imageSettings.size = change;
        this.plugin.saveSettings();
      });
    });
    if (model === "dall-e-3") {
      const imageStyle = new import_obsidian8.Setting(parentContainer).setName("Image style").setDesc(
        "Defaults to vivid. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for dall-e-3."
      ).addDropdown((dropdown) => {
        dropdown.addOption("", "Select style");
        dropdown.addOption("natural", "Natural");
        dropdown.addOption("vivid", "Vivid");
        dropdown.onChange((change) => {
          viewSettings.imageSettings.style = change;
          this.plugin.saveSettings();
        });
      });
      const quality = new import_obsidian8.Setting(parentContainer).setName("Quality").setDesc(
        "The quality of the image that will be generated. hd creates images with finer details and greater consistency across the image. This param is only supported for dall-e-3."
      ).addToggle((value) => {
        value.onChange(async (value2) => {
          if (value2) {
            viewSettings.imageSettings.quality = "hd";
          } else {
            viewSettings.imageSettings.quality = "standard";
          }
          this.plugin.saveSettings();
        });
      });
    }
  }
  generateChatSettings(parentContainer, modelType) {
    const settingType = getSettingType(this.viewType);
    const viewSettings = this.plugin.settings[settingType];
    new import_obsidian8.Setting(parentContainer).setName("Temperature").setDesc(
      modelType !== GPT4All ? "Defaults to 1. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both." : "Higher temperatures (eg., 1.2) increase randomness, resulting in more imaginative and diverse text. Lower temperatures (eg., 0.5) make the output more focused, predictable, and conservative. A safe range would be around 0.6 - 0.85"
    ).addText((text) => {
      text.setValue(`${viewSettings.chatSettings.temperature}`);
      text.inputEl.type = "number";
      text.onChange((change) => {
        viewSettings.chatSettings.temperature = parseFloat(change);
        this.plugin.saveSettings();
      });
    });
    new import_obsidian8.Setting(parentContainer).setName("Tokens").setDesc("The number of tokens used in the completion.").addText((text) => {
      text.setValue(`${viewSettings.chatSettings.maxTokens}`);
      text.inputEl.type = "number";
      text.onChange((change) => {
        viewSettings.chatSettings.maxTokens = parseInt(change);
        this.plugin.saveSettings();
      });
    });
    if (modelType === openAI) {
      const frequencyPenalty = new import_obsidian8.Setting(parentContainer).setName("Frequency penalty").setDesc(
        "Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
      ).addText((text) => {
        var _a3;
        text.setValue(
          `${(_a3 = viewSettings.chatSettings.openAI) == null ? void 0 : _a3.frequencyPenalty}`
        );
        text.inputEl.type = "number";
        text.onChange((change) => {
          viewSettings.chatSettings.openAI.frequencyPenalty = parseFloat(change);
          this.plugin.saveSettings();
        });
      });
      const logProbs = new import_obsidian8.Setting(parentContainer).setName("Log probs").setDesc(
        "Defaults to false. Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."
      ).addToggle((value) => {
        value.onChange((change) => {
          viewSettings.chatSettings.openAI.logProbs = change;
          this.plugin.saveSettings();
        });
      });
      const topLogProbs = new import_obsidian8.Setting(parentContainer).setName("Top log probs").setDesc(
        "An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."
      ).addText((text) => {
        var _a3;
        text.setValue(
          `${(_a3 = viewSettings.chatSettings.openAI) == null ? void 0 : _a3.topLogProbs}`
        );
        text.inputEl.type = "number";
        text.onChange((change) => {
          viewSettings.chatSettings.openAI.topLogProbs = parseFloat(change);
          this.plugin.saveSettings();
        });
      });
      const presencePenalty = new import_obsidian8.Setting(parentContainer).setName("Presence penalty").setDesc(
        "Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
      ).addText((text) => {
        var _a3;
        text.setValue(
          `${(_a3 = viewSettings.chatSettings.openAI) == null ? void 0 : _a3.presencePenalty}`
        );
        text.inputEl.type = "number";
        text.onChange((change) => {
          viewSettings.chatSettings.openAI.presencePenalty = parseFloat(change);
          this.plugin.saveSettings();
        });
      });
      const responseFormat = new import_obsidian8.Setting(parentContainer).setName("Response format").setDesc(
        `An object specifying the format that the model must output. Compatible with GPT-4 turbo and all GPT-3.5 turbo models newer than gpt-3.5-turbo-1106. Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.`
      ).addText((text) => {
        var _a3;
        text.setValue(
          `${(_a3 = viewSettings.chatSettings.openAI) == null ? void 0 : _a3.responseFormat}`
        );
        text.onChange((change) => {
          viewSettings.chatSettings.openAI.responseFormat = change;
          this.plugin.saveSettings();
        });
      });
      const topP = new import_obsidian8.Setting(parentContainer).setName("Top p").setDesc(
        "Defaults to 1. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
      ).addText((text) => {
        var _a3;
        text.setValue(`${(_a3 = viewSettings.chatSettings.openAI) == null ? void 0 : _a3.topP}`);
        text.inputEl.type = "number";
        text.onChange((change) => {
          viewSettings.chatSettings.openAI.topP = parseFloat(change);
          this.plugin.saveSettings();
        });
      });
    }
  }
  generateModerationsSettings(parentContainer) {
  }
};

// src/Plugin/FAB/FAB.ts
var import_obsidian9 = require("obsidian");
var ROOT_WORKSPACE_CLASS = ".mod-vertical.mod-root";
var FAB = class {
  constructor(plugin) {
    this.plugin = plugin;
  }
  generateFAB() {
    var _a3;
    const fabContainer = createDiv();
    fabContainer.addEventListener("mouseenter", () => {
      const { historyIndex } = getViewInfo(
        this.plugin,
        "floating-action-button"
      );
      setView(this.plugin, "floating-action-button");
      this.plugin.settings.currentIndex = historyIndex;
      this.plugin.saveSettings();
    });
    fabContainer.setAttribute("class", `floating-action-button`);
    fabContainer.setAttribute("id", "_floating-action-button");
    const viewArea = fabContainer.createDiv();
    viewArea.addClass("fab-view-area");
    viewArea.setAttr("style", "display: none");
    const header = new Header(this.plugin, "floating-action-button");
    const chatContainer = new ChatContainer(
      this.plugin,
      "floating-action-button",
      this.plugin.messageStore
    );
    const historyContainer = new HistoryContainer(
      this.plugin,
      "floating-action-button"
    );
    const settingsContainer = new SettingsContainer(
      this.plugin,
      "floating-action-button"
    );
    const assistantsContainer = new AssistantsContainer(
      this.plugin,
      "floating-action-button"
    );
    const lineBreak = viewArea.createDiv();
    const chatContainerDiv = viewArea.createDiv();
    const chatHistoryContainer = viewArea.createDiv();
    const settingsContainerDiv = viewArea.createDiv();
    const assistantsContainerDiv = viewArea.createDiv();
    header.generateHeader(
      viewArea,
      chatContainerDiv,
      chatHistoryContainer,
      settingsContainerDiv,
      assistantsContainerDiv,
      chatContainer,
      historyContainer,
      settingsContainer,
      assistantsContainer
    );
    let history = this.plugin.settings.promptHistory;
    settingsContainerDiv.setAttr("style", "display: none");
    settingsContainerDiv.addClass("fab-settings-container", "llm-flex");
    assistantsContainerDiv.setAttr("style", "display: none");
    assistantsContainerDiv.addClass("fab-assistants-container", "llm-flex");
    chatHistoryContainer.setAttr("style", "display: none");
    chatHistoryContainer.addClass("fab-chat-history-container", "llm-flex");
    lineBreak.className = classNames["floating-action-button"]["title-border"];
    chatContainerDiv.addClass("fab-chat-container", "llm-flex");
    chatContainer.generateChatContainer(chatContainerDiv, header);
    historyContainer.generateHistoryContainer(
      chatHistoryContainer,
      history,
      chatContainerDiv,
      chatContainer,
      header
    );
    settingsContainer.generateSettingsContainer(
      settingsContainerDiv,
      header
    );
    let button = new import_obsidian9.ButtonComponent(fabContainer);
    button.setIcon("bot-message-square").setClass("buttonItem").onClick(() => {
      if (!viewArea.isShown()) {
        viewArea.setAttr("style", "display: block");
      } else {
        viewArea.hide();
      }
    });
    (_a3 = document.body.querySelector(ROOT_WORKSPACE_CLASS)) == null ? void 0 : _a3.insertAdjacentElement("afterbegin", fabContainer);
  }
  removeFab() {
    const FAB2 = document.getElementById("_floating-action-button");
    if (FAB2) {
      FAB2.remove();
    }
  }
  regenerateFAB() {
    this.removeFab();
    this.generateFAB();
  }
};

// src/Plugin/Modal/ChatModal2.ts
var import_obsidian10 = require("obsidian");
var ChatModal2 = class extends import_obsidian10.Modal {
  constructor(plugin) {
    super(plugin.app);
    this.plugin = plugin;
  }
  onOpen() {
    this.modalEl.getElementsByClassName("modal-close-button")[0].setAttr("style", "display: none");
    const { contentEl } = this;
    const closeModal = () => {
      this.close();
    };
    const header = new Header(this.plugin, "modal");
    const chatContainer = new ChatContainer(
      this.plugin,
      "modal",
      this.plugin.messageStore
    );
    const historyContainer = new HistoryContainer(this.plugin, "modal");
    const settingsContainer = new SettingsContainer(this.plugin, "modal");
    const assistantsContainer = new AssistantsContainer(
      this.plugin,
      "modal"
    );
    const lineBreak = contentEl.createDiv();
    const chatContainerDiv = contentEl.createDiv();
    const chatHistoryContainer = contentEl.createDiv();
    const settingsContainerDiv = contentEl.createDiv();
    const assistantsContainerDiv = contentEl.createDiv();
    header.generateHeader(
      contentEl,
      chatContainerDiv,
      chatHistoryContainer,
      settingsContainerDiv,
      assistantsContainerDiv,
      chatContainer,
      historyContainer,
      settingsContainer,
      assistantsContainer
    );
    let history = this.plugin.settings.promptHistory;
    settingsContainerDiv.setAttr("style", "display: none");
    settingsContainerDiv.addClass("llm-modal-settings-container", "llm-flex");
    assistantsContainerDiv.setAttr("style", "display: none");
    assistantsContainerDiv.addClass("llm-modal-assistants-container", "llm-flex");
    chatHistoryContainer.setAttr("style", "display: none");
    chatHistoryContainer.addClass("llm-modal-chat-history-container", "llm-flex");
    lineBreak.className = classNames["modal"]["title-border"];
    chatContainerDiv.addClass("llm-modal-chat-container", "llm-flex");
    chatContainer.generateChatContainer(chatContainerDiv, header);
    historyContainer.generateHistoryContainer(
      chatHistoryContainer,
      history,
      chatContainerDiv,
      chatContainer,
      header
    );
    settingsContainer.generateSettingsContainer(
      settingsContainerDiv,
      header
    );
  }
};

// src/Plugin/Widget/Widget.ts
var import_obsidian11 = require("obsidian");
var TAB_VIEW_TYPE = "tab-view";
var WidgetView = class extends import_obsidian11.ItemView {
  constructor(leaf, plugin) {
    super(leaf);
    this.plugin = plugin;
  }
  getViewType() {
    return TAB_VIEW_TYPE;
  }
  getDisplayText() {
    return "LLM";
  }
  async onOpen() {
    this.icon = "bot-message-square";
    const container = this.containerEl.children[1];
    const history = this.plugin.settings.promptHistory;
    container.addEventListener("mouseenter", () => {
      const { historyIndex } = getViewInfo(
        this.plugin,
        "widget"
      );
      setView(this.plugin, "widget");
      this.plugin.settings.currentIndex = historyIndex;
      this.plugin.saveSettings();
    });
    container.empty();
    const header = new Header(this.plugin, "widget");
    const chatContainer = new ChatContainer(
      this.plugin,
      "widget",
      this.plugin.messageStore
    );
    const historyContainer = new HistoryContainer(this.plugin, "widget");
    const settingsContainer = new SettingsContainer(this.plugin, "widget");
    const assistantsContainer = new AssistantsContainer(
      this.plugin,
      "widget"
    );
    const lineBreak = container.createDiv();
    const chatContainerDiv = container.createDiv();
    const chatHistoryContainer = container.createDiv();
    const settingsContainerDiv = container.createDiv();
    const assistantContainerDiv = container.createDiv();
    settingsContainerDiv.setAttr("style", "display: none");
    settingsContainerDiv.addClass(
      "llm-widget-settings-container",
      "llm-flex"
    );
    assistantContainerDiv.setAttr("style", "display: none");
    assistantContainerDiv.addClass(
      "llm-widget-assistant-container",
      "llm-flex",
      "llm-widget-tab-assistants"
    );
    chatHistoryContainer.setAttr("style", "display: none");
    chatHistoryContainer.addClass(
      "llm-widget-chat-history-container",
      "llm-flex"
    );
    lineBreak.className = classNames["widget"]["title-border"];
    chatContainerDiv.addClass("llm-widget-chat-container", "llm-flex");
    header.generateHeader(
      container,
      chatContainerDiv,
      chatHistoryContainer,
      settingsContainerDiv,
      assistantContainerDiv,
      chatContainer,
      historyContainer,
      settingsContainer,
      assistantsContainer
    );
    chatContainer.generateChatContainer(chatContainerDiv, header);
    historyContainer.generateHistoryContainer(
      chatHistoryContainer,
      history,
      chatContainerDiv,
      chatContainer,
      header
    );
    settingsContainer.generateSettingsContainer(
      settingsContainerDiv,
      header
    );
    assistantsContainer.generateAssistantsContainer(settingsContainerDiv);
  }
  async onClose() {
  }
};

// src/Settings/SettingsView.ts
var import_obsidian12 = require("obsidian");

// src/assets/LLMguy.svg
var LLMguy_default = '<svg width="40" height="81" viewBox="0 0 40 81" fill="none" xmlns="http://www.w3.org/2000/svg">\r\n<path fill-rule="evenodd" clip-rule="evenodd" d="M26 0H14V1H13V2H12V3H11V5H10V7H9V8H8H7V7H4V8H1V9H0V10V13H1V14H2V15H3V16V17H5V18H6V26H7V33H8V34H9V35H10V36H11V37H9V38H7V39H6V40H5V41H4V58H3V60H4V61H5V62H6H7V63H8V64H9V65H10V71H9V73H6V74H4H3V75H2V76H1V78H2V79H14V80H17V79H20V80H23V81H32V80H33V78V77H28V76H24V75H23V74H22V67V66H24V65H25V64H26V66H27H28V67H32V66H33V65V63H34V58H33V51H32V47H33V43H32V41V40H31V39H29V38H28V37H27V36H28V35H29V34H30V33H31V29H32V23H36V22H39H40V17V16H39V15H38V14H36V13H32V12H30V5H29V3H28V2H27V1H26V0ZM15 1H25V2H27V4H28V6H29V17H30V13H31V14H35V15H37V16H38V17H39V21H36V22H25V21H19V20H15V19H13V18H12H9V17H6V16H4V15H3V14H2V12H1V10H2V9H5V8H6V9H8V11H7V13H8V11H9V9H10V7H11V5H12V4H13V2H15V1ZM15 20V21V22H14V23H13V20H15ZM24 29V30H21V29V22H19V21H17V22V23V28H16V29H15H13V28H12V27V24V23V19H8V18H7V26H8V32H9V33H10V34H11V35H13V36H17V37H11V38H9V39H8V40H6V42H5V60H6V61H7V59H8V62H9V63H10V64H11H12V65H16V66H22V65H24V63H25V58H26V61H28V62H33V58H32V51H31V46H32V43H31V41H30V40H29V39H28V38H27H26V37H22V36H26V35H27H28V34H29V33H30V29H31V23H26V28H25V23V22H24V25H23H22V29H24ZM24 29V28H25V29H24ZM22 37V38H17V37H22ZM24 76H23H22V75H21V67H17V76H18V78H20V79H23V80H32V78H27V77H24V76ZM14 79H16V67H15V66H12H11V72H10V73V74H6V75H4V76H3V78H14V79ZM13 27H14V28H15V27H16V23H15V24H13V27ZM29 65V66H31V65H32V63H27V65H29ZM22 22H23V24H22V22Z" fill="currentColor"/>\r\n</svg>\r\n';

// src/Settings/SettingsView.ts
var SettingsView = class extends import_obsidian12.PluginSettingTab {
  constructor(app, plugin, fab) {
    super(app, plugin);
    this.currentApiInput = null;
    this.apiKeyConfigs = {
      claude: {
        name: "Claude API key",
        desc: "Claude models require an API key for authentication.",
        key: "claudeAPIKey",
        generateUrl: "https://console.anthropic.com/settings/keys"
      },
      gemini: {
        name: "Gemini API key",
        desc: "Gemini models require an API key for authentication.",
        key: "geminiAPIKey",
        generateUrl: "https://aistudio.google.com/app/apikey"
      },
      openai: {
        name: "OpenAI API key",
        desc: "OpenAI models require an API key for authentication.",
        key: "openAIAPIKey",
        generateUrl: "https://platform.openai.com/api-keys"
      }
    };
    this.plugin = plugin;
    this.fab = fab;
  }
  display() {
    const { containerEl } = this;
    containerEl.empty();
    new import_obsidian12.Setting(containerEl).setName("Reset chat history").setDesc("This will delete previous prompts and chat contexts").addButton((button) => {
      button.setButtonText("Reset history");
      button.onClick(() => {
        this.plugin.history.reset();
      });
    });
    const apiKeySection = containerEl.createDiv();
    new import_obsidian12.Setting(apiKeySection).setName("Manage API keys").setDesc("Select which API key you want to view or modify").addDropdown((dropdown) => {
      dropdown.addOption("", "Select API to configure");
      Object.keys(this.apiKeyConfigs).forEach((key) => {
        dropdown.addOption(key, this.apiKeyConfigs[key].name);
      });
      dropdown.onChange((value) => {
        this.showApiKeyInput(value, apiKeySection);
      });
    });
    new import_obsidian12.Setting(containerEl).setClass("default-model-selector").setName("Set default model").setDesc("Sets the default LLM you want to use for the plugin").addDropdown((dropdown) => {
      let valueChanged = false;
      dropdown.addOption(
        modelNames[this.plugin.settings.defaultModel],
        "Select default model"
      );
      let keys = Object.keys(models);
      for (let model of keys) {
        if (models[model].type === GPT4All) {
          const gpt4AllPath = getGpt4AllPath(this.plugin);
          const fullPath = `${gpt4AllPath}/${models[model].model}`;
          const exists = this.plugin.fileSystem.existsSync(fullPath);
          if (exists) {
            dropdown.addOption(models[model].model, model);
          }
        } else {
          dropdown.addOption(models[model].model, model);
        }
      }
      dropdown.onChange((change) => {
        valueChanged = true;
        changeDefaultModel(change, this.plugin);
      });
      dropdown.selectEl.addEventListener("blur", () => {
        if (valueChanged) {
          this.plugin.saveSettings();
          valueChanged = false;
        }
      });
      dropdown.setValue(this.plugin.settings.modalSettings.model);
    });
    new import_obsidian12.Setting(containerEl).setName("Toggle FAB").setDesc("Toggles the LLM floating action button").addToggle((value) => {
      value.setValue(this.plugin.settings.showFAB).onChange(async (value2) => {
        this.fab.removeFab();
        this.plugin.settings.showFAB = value2;
        await this.plugin.saveSettings();
        if (value2) {
          this.fab.regenerateFAB();
        }
      });
    });
    new import_obsidian12.Setting(containerEl).setName("Donate").setDesc("Consider donating to support development.").addButton((button) => {
      button.setButtonText("Donate");
      button.onClick(() => {
        window.open("https://www.buymeacoffee.com/johnny1093");
      });
    });
    const llmGuy = containerEl.createDiv();
    llmGuy.addClass("llm-icon-wrapper");
    const parser = new DOMParser();
    const svgDoc = parser.parseFromString(LLMguy_default, "image/svg+xml");
    const svgElement = svgDoc.documentElement;
    llmGuy.appendChild(svgElement);
    const credits = llmGuy.createEl("div", {
      attr: { id: "llm-settings-credits" }
    });
    const creditsHeader = credits.createEl("p", {
      text: "LLM plugin",
      attr: { id: "llm-hero-credits" }
    });
    credits.appendChild(creditsHeader);
    const creditsNames = credits.createEl("p", {
      text: "By Johnny\u2728, Ryan Mahoney, and Evan Harris",
      attr: { class: "llm-hero-names llm-text-muted" }
    });
    credits.appendChild(creditsNames);
    const creditsVersion = credits.createEl("span", {
      text: `v${this.plugin.manifest.version}`,
      attr: { class: "llm-text-muted version" }
    });
    credits.appendChild(creditsVersion);
  }
  showApiKeyInput(type, containerEl) {
    const existingSettings = containerEl.querySelector(".api-key-input");
    if (existingSettings) {
      existingSettings.remove();
    }
    if (!type)
      return;
    const config = this.apiKeyConfigs[type];
    const settingContainer = containerEl.createDiv();
    settingContainer.addClass("api-key-input");
    new import_obsidian12.Setting(settingContainer).setName(config.name).setDesc(config.desc).addText((text) => {
      this.currentApiInput = text;
      text.setValue(this.plugin.settings[config.key]);
      text.onChange((value) => {
        if (value.trim().length) {
          this.plugin.settings[config.key] = value;
          this.plugin.saveSettings();
        }
      });
    }).addButton((button) => {
      button.setButtonText("Generate token");
      button.onClick(() => {
        window.open(config.generateUrl);
      });
    });
  }
};

// src/Plugin/Components/MessageStore.ts
var MessageStore = class {
  constructor() {
    this.messages = [];
    this.subscribers = [];
  }
  addMessage(message) {
    this.messages.push(message);
    this.notifySubscribers();
  }
  setMessages(messages2) {
    this.messages = messages2;
  }
  getMessages() {
    return [...this.messages];
  }
  subscribe(subscriber) {
    this.subscribers.push(subscriber);
  }
  unsubscribe(subscriber) {
    this.subscribers = this.subscribers.filter((sub) => sub !== subscriber);
  }
  notifySubscribers() {
    this.subscribers.forEach((sub) => sub(this.getMessages()));
  }
};

// src/services/OperatingSystem.ts
var MobileOperatingSystem = class {
  homedir() {
    return "";
  }
  platform() {
    return "";
  }
};
var DesktopOperatingSystem = class {
  constructor() {
    this.os = require("os");
  }
  homedir() {
    return this.os.homedir();
  }
  platform() {
    return this.os.platform();
  }
};

// src/services/FileSystem.ts
var DesktopFileSystem = class {
  constructor() {
    this.fs = require("fs");
  }
  existsSync(path) {
    return this.fs.existsSync(path);
  }
  async createReadStream(path) {
    return new Promise((resolve) => {
      const nodeStream = this.fs.createReadStream(path);
      resolve(new ReadableStream({
        start(controller) {
          nodeStream.on("data", (chunk) => controller.enqueue(chunk));
          nodeStream.on("end", () => controller.close());
          nodeStream.on("error", (err) => controller.error(err));
        }
      }));
    });
  }
};
var MobileFileSystem = class {
  constructor(plugin) {
    this.plugin = plugin;
  }
  existsSync(path) {
    return false;
  }
  async createReadStream(path) {
    const buffer = await this.plugin.app.vault.adapter.readBinary(path);
    return new ReadableStream({
      start(controller) {
        controller.enqueue(buffer);
        controller.close();
      }
    });
  }
};

// src/main.ts
var defaultSettings = {
  assistant: false,
  assistantId: "",
  model: "gpt-3.5-turbo",
  modelName: "ChatGPT-3.5 turbo",
  modelType: "openAI",
  modelEndpoint: chat,
  endpointURL: "/chat/completions",
  historyIndex: -1,
  imageSettings: {
    numberOfImages: 1,
    response_format: "url",
    size: "1024x1024",
    style: "vivid",
    quality: "standard"
  },
  chatSettings: {
    maxTokens: 300,
    temperature: 0.65,
    GPT4All: {},
    openAI: {
      frequencyPenalty: 0,
      logProbs: false,
      topLogProbs: null,
      presencePenalty: 0,
      responseFormat: "",
      topP: 1
    }
  }
};
var DEFAULT_SETTINGS = {
  currentIndex: -1,
  currentView: null,
  modalSettings: {
    ...defaultSettings
  },
  widgetSettings: {
    ...defaultSettings
  },
  fabSettings: {
    ...defaultSettings
  },
  promptHistory: [],
  assistants: [],
  openAIAPIKey: "",
  claudeAPIKey: "",
  geminiAPIKey: "",
  GPT4AllStreaming: false,
  //this setting determines whether or not fab is shown by default
  showFAB: false,
  defaultModel: ""
};
var LLMPlugin2 = class extends import_obsidian13.Plugin {
  async onload() {
    this.fileSystem = import_obsidian13.Platform.isDesktop ? new DesktopFileSystem() : new MobileFileSystem(this);
    this.os = import_obsidian13.Platform.isDesktop ? new DesktopOperatingSystem() : new MobileOperatingSystem();
    await this.loadSettings();
    await this.checkForAPIKeyBasedModel();
    this.registerRibbonIcons();
    this.registerCommands();
    this.messageStore = new MessageStore();
    this.settings.currentIndex = -1;
    this.messageStore.setMessages([]);
    this.saveSettings();
    this.registerView(TAB_VIEW_TYPE, (tab) => new WidgetView(tab, this));
    this.fab = new FAB(this);
    this.addSettingTab(new SettingsView(this.app, this, this.fab));
    if (this.settings.showFAB) {
      setTimeout(() => {
        this.fab.regenerateFAB();
      }, 500);
    }
    this.history = new History(this);
    this.assistants = new Assistants(this);
  }
  onunload() {
    this.fab.removeFab();
  }
  registerCommands() {
    this.addCommand({
      id: "open-llm-modal",
      name: "Open modal",
      callback: () => {
        new ChatModal2(this).open();
      }
    });
    this.addCommand({
      id: "open-LLM-widget-tab",
      name: "Open chat in tab",
      callback: () => {
        this.activateTab();
      }
    });
    this.addCommand({
      id: "toggle-LLM-fab",
      name: "Toggle FAB",
      callback: () => {
        const currentFABState = this.settings.showFAB;
        this.settings.showFAB = !currentFABState;
        this.saveSettings();
        this.settings.showFAB ? this.fab.regenerateFAB() : this.fab.removeFab();
      }
    });
  }
  registerRibbonIcons() {
    this.addRibbonIcon("bot", "Ask a question", (evt) => {
      new ChatModal2(this).open();
    });
  }
  async activateTab() {
    const { workspace } = this.app;
    let tab = null;
    const tabs = workspace.getLeavesOfType(TAB_VIEW_TYPE);
    if (tabs.length > 0) {
      tab = tabs[0];
    } else {
      tab = workspace.getLeaf("tab");
      await tab.setViewState({ type: TAB_VIEW_TYPE, active: true });
    }
    workspace.revealLeaf(tab);
  }
  async loadSettings() {
    const dataJSON = await this.loadData();
    if (dataJSON) {
      this.settings = Object.assign({}, dataJSON);
      this.settings.fabSettings.historyIndex = -1;
      this.settings.widgetSettings.historyIndex = -1;
    } else {
      this.settings = Object.assign(
        {},
        DEFAULT_SETTINGS,
        await this.loadData()
      );
    }
  }
  async saveSettings() {
    await this.saveData(this.settings);
  }
  async validateActiveModelsAPIKeys() {
    let activeClaudeModel, activeGeminiModel, activeOpenAIModel;
    const settingsObjects = [
      this.settings.modalSettings,
      this.settings.widgetSettings,
      this.settings.fabSettings
    ];
    settingsObjects.forEach((settings) => {
      const model = settings.model;
      switch (model) {
        case claudeSonnetJuneModel:
          activeClaudeModel = model === claudeSonnetJuneModel;
          break;
        case geminiModel:
          activeGeminiModel = model === geminiModel;
          break;
        case openAIModel:
          activeOpenAIModel = model === openAIModel;
          break;
      }
    });
    const providerKeyPairs = [
      {
        provider: openAI,
        key: this.settings.openAIAPIKey,
        isActive: activeOpenAIModel
      },
      {
        provider: claude,
        key: this.settings.claudeAPIKey,
        isActive: activeClaudeModel
      },
      {
        provider: gemini,
        key: this.settings.geminiAPIKey,
        isActive: activeGeminiModel
      }
    ];
    const filteredPairs = providerKeyPairs.filter(({ key, isActive }) => {
      if (!key)
        return;
      if (!isActive)
        return;
      return key;
    });
    const promises = filteredPairs.map(async (pair) => {
      const result = await getApiKeyValidity(pair);
      return result;
    });
    const results = await Promise.all(promises);
    const hasValidOpenAIAPIKey = results.some((result) => {
      if (result) {
        return result.valid && result.provider === openAI;
      }
    });
    if (hasValidOpenAIAPIKey)
      await generateAssistantsList(this.settings);
  }
  async checkForAPIKeyBasedModel() {
    const fabModelRequiresKey = this.settings.fabSettings.model === openAIModel || this.settings.fabSettings.model === claudeSonnetJuneModel || this.settings.fabSettings.model === geminiModel;
    const widgetModelRequresKey = this.settings.widgetSettings.model === openAIModel || this.settings.widgetSettings.model === claudeSonnetJuneModel || this.settings.widgetSettings.model === geminiModel;
    const modalModelRequresKey = this.settings.modalSettings.model === openAIModel || this.settings.modalSettings.model === claudeSonnetJuneModel || this.settings.modalSettings.model === geminiModel;
    const activeModelRequiresKey = fabModelRequiresKey || widgetModelRequresKey || modalModelRequresKey;
    if (activeModelRequiresKey)
      await this.validateActiveModelsAPIKeys();
  }
  // end refactor into utils section
};
/*! Bundled license information:

@google/generative-ai/dist/index.mjs:
  (**
   * @license
   * Copyright 2024 Google LLC
   *
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   *   http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   *)

@google/generative-ai/dist/index.mjs:
  (**
   * @license
   * Copyright 2024 Google LLC
   *
   * Licensed under the Apache License, Version 2.0 (the "License");
   * you may not use this file except in compliance with the License.
   * You may obtain a copy of the License at
   *
   *   http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing, software
   * distributed under the License is distributed on an "AS IS" BASIS,
   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   * See the License for the specific language governing permissions and
   * limitations under the License.
   *)
*/


/* nosourcemap */